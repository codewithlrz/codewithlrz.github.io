[{"title":"图深度学习笔记-图论基础","url":"/2021/11/24/book-mayao-02-md/","content":" 1 图的表示\n 1.1 图（Graph）\n 定义： \\textcolor{blue} {\\textbf { 定义： } } 定义： 一个图可以表示为G={V,ε}G = { \\{V,\\varepsilon \\}}G={V,ε}，其中VVV = {v1v_1v1​,⋯\\cdots⋯,vNv_NvN​}是大小为N=∣V∣N = | V |N=∣V∣ 的节点集合，ε\\varepsilonε={ e1e_1e1​,⋯\\cdots⋯,eMe_MeM​}是大小为MMM的边的集合。\n\n如上图（图1），为一个有五个节点和六条边的图。其中{ v1v_1v1​,⋯\\cdots⋯,v5v_5v5​}为五个节点，{ e1e_1e1​,⋯\\cdots⋯,e6e_6e6​}为六条边。\n在图中，如果有一条边eie_iei​连接两个节点vei1v_{e_i}^1vei​1​和vei2v_{e_i}^2vei​2​，那么这条边可以表示为（vei1v_{e_i}^1vei​1​,vei2v_{e_i}^2vei​2​），在有向图中表示边从起点vei1v_{e_i}^1vei​1​指向终点vei2v_{e_i}^2vei​2​。相反在无向图中由于没有顺序之分，则eie_iei​ = （vei1v_{e_i}^1vei​1​,vei2v_{e_i}^2vei​2​）= （vei2v_{e_i}^2vei​2​,vei1v_{e_i}^1vei​1​）。举个例子，如图一中的边e6e_6e6​连接节点v1v_1v1​与节点v5v_5v5​，那么由于图1为无向图，则e6e_6e6​也可以表示为（v1v_1v1​,v5v_5v5​）或（v5v_5v5​,v1v_1v1​）。\n 1.2 邻接矩阵（Adjacency Matrix）\n为了方便查看节点之间的连接关系，图G={V,ε}G = { \\{V,\\varepsilon \\}}G={V,ε}可以等价的表示为邻接矩阵的形式，更加直观的描述节点之间的关系。\n 定义： \\textcolor{blue} {\\textbf { 定义： } } 定义： 给定一个图G={V,ε}G = { \\{V,\\varepsilon \\}}G={V,ε},对应的邻接矩阵可以表示为A∈{0,1}N×NA∈{ \\{ 0,1 \\} ^ { N×N } }A∈{0,1}N×N。邻接矩阵AAA的第iii行第jjj列元素Ai,jA_{i,j}Ai,j​表示节点viv_ivi​和vjv_jvj​的连接关系。具体来讲，如果viv_ivi​与vjv_jvj​相邻，则Ai,j=1A_{i,j} = 1Ai,j​=1，否则Ai,j=0A_{i,j} = 0Ai,j​=0。\n在无向图中，由于不受节点顺序影响，则Ai,j=Aj,iA_{ i,j } = A_{ j,i }Ai,j​=Aj,i​，所以无向图的邻接矩阵一定是 关于主对角线对称 \\textbf { 关于主对角线对称 } 关于主对角线对称 的。以图1为例，该图的邻接矩阵可表示为：\nA=(0101110100010011000110110)A = \n\\begin {pmatrix}\n0 &amp; 1 &amp; 0 &amp; 1 &amp; 1 \\\\\n1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\\n0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 \\\\\n1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\\n1 &amp; 0 &amp; 1 &amp; 1 &amp; 0 \\\\\n\\end {pmatrix}\nA=⎝⎜⎜⎜⎜⎛​01011​10100​01001​10001​10110​⎠⎟⎟⎟⎟⎞​\n可以看到，主对角线值均为0，且都关于其对称。\n 2 图的性质\n 2.1 度（Degree）\n 定义： \\textcolor{blue} {\\textbf { 定义： } } 定义： 在图G={V,E}G = { \\{V,E \\}}G={V,E}中，节点vi∈Vv_i ∈ Vvi​∈V的度定义为图GGG中与节点viv_ivi​相关联的边的数目。\nd(vi)=∑vj∈V1ε({vi,vj})d(v_i) = \\sum_{ v_j ∈ V}1\\varepsilon( \\{ v_i,v_j \\} )\nd(vi​)=vj​∈V∑​1ε({vi​,vj​})\n其中，1ε(⋅)1\\varepsilon(\\cdot)1ε(⋅)为指示函数，简单来说就是满足条件就取1，不满足就取0。对于图来讲可表示为：\n1ε({vi,vj})={1,(vi,vj)∈ε0,(vi,vj)∉ε1\\varepsilon( \\{ v_i,v_j \\} ) = \n\\begin {cases}\n1, \\quad (v_i,v_j)∈\\varepsilon \\\\\n0, \\quad (v_i,v_j)\\notin\\varepsilon\n\\end {cases}\n1ε({vi​,vj​})={1,(vi​,vj​)∈ε0,(vi​,vj​)∈/​ε​\n即当存在(vi,vj)(v_i,v_j)(vi​,vj​)这条边时，d(vi)d(v_i)d(vi​)就加1，反之如果不存在，就加0。其实最直观的看法就是找从这个节点出发引出几条边。但这种方法会有一些麻烦，所以如果能够画出图的邻接矩阵的话，可以更简单的计算结果，利用邻接矩阵可以将节点的度表示为：\nd(vi)=∑j=1NAi,jd(v_i) = \\sum_{ j = 1 }^N A_{ i,j }\nd(vi​)=j=1∑N​Ai,j​\n简单来说就是计算该节点所在行所有值的和。任以图一为例，节点v5v_5v5​与v1,v3,v4v_1,v_3,v_4v1​,v3​,v4​相连，则节点v5v_5v5​的度为3。如果利用邻接矩阵计算，v5v_5v5​占矩阵第五行，直接将改行所有元素相加，即可得到其度为3。\n 2.2 邻域（Neighborhood）\n 定义： \\textcolor{blue} {\\textbf { 定义： } } 定义： 在图G={V,ε}G=\\{ V,\\varepsilon \\}G={V,ε}中，节点viv_ivi​的邻域N(vi)N(v_i)N(vi​)是所有和它相邻的节点的集合。\n依然以图1中v5v_5v5​节点为例，其邻域为N(v5)={v1,v3,v4}N(v_5) = \\{ v_1,v_3,v_4 \\}N(v5​)={v1​,v3​,v4​}，可以发现对于任意节点viv_ivi​,其 邻域中元素个数等于该节点的度 \\textbf{ 邻域中元素个数等于该节点的度 } 邻域中元素个数等于该节点的度 ，即d(vi)=∣N(vi)∣d(v_i) = |N(v_i)|d(vi​)=∣N(vi​)∣。\n 定理： \\textcolor{green} {\\textbf { 定理： } } 定理： 一个图G={V,ε}G=\\{ V,\\varepsilon \\}G={V,ε}中所有的节点的度之和是图中边数量的两倍：\n∑vi∈Vd(vi)=2 ⋅ ∣ε∣\\sum_{ v_i ∈ V}d(v_i) = 2\\ \\cdot \\ | \\varepsilon |\nvi​∈V∑​d(vi​)=2 ⋅ ∣ε∣\n这个定理很容易想通，举一个例子，如下图（图2），v1与v2v_1与v_2v1​与v2​的度都为1，度之和为2。但两个节点公用一条边e1e_1e1​，则是2倍的关系。\n\n同样的，如果将图写为邻接矩阵的话，每一个非零元素则代表两个节点存在连接，第iii行中非零元素的个数就是节点viv_ivi​的度，那么矩阵中所有非零元素的个数就是所有节点的度之和，那么可以有如下推论。\n 推论： \\textcolor{green} {\\textbf { 推论： } } 推论： 无向图邻接矩阵的非零元素的个数是边的数量的两倍。\n 2.3 连通度（Connectivity）\n 定义： \\textcolor{blue} {\\textbf { 定义： } } 定义： 途径（Walk）图的途径是节点和边的交替序列，从一个节点开始，以一个节点结束，其中每条边与紧邻的节点相关联。\n 定义： \\textcolor{blue} {\\textbf { 定义： } } 定义： 迹（Trail）是边各不同的途径。\n 定义： \\textcolor{blue} {\\textbf { 定义： } } 定义： 路（Path）是节点各不相同的途径，也称路径\n简而言之，只要不跳着走，那从一个节点到另一个节点就是一个途径，在途径这个大类中，如果序列中没有重复边就是迹，没有重复节点就是路。举个例子，图1中想从v1v_1v1​走到v2v_2v2​，假设绕远走，那么可以选择(v1,e4,v4,e5,v5,e6,v1,e1,v2)(v_1,e_4,v_4,e_5,v_5,e_6,v_1,e_1,v_2)(v1​,e4​,v4​,e5​,v5​,e6​,v1​,e1​,v2​)这条途径，那么这条途径也是一条迹但不是路，因为这条途径中没有重复的边，但存在重复的节点v1v_1v1​。如果选(v1,e1,v2)(v_1,e_1,v_2)(v1​,e1​,v2​)这条途径，那么这即可以成为是迹，也可以称为是路径。\n 定理： \\textcolor{green} {\\textbf { 定理： } } 定理： 对于图G={ε,V}G = { \\{ \\varepsilon,V \\} }G={ε,V}及其邻接矩阵AAA，用AnA^nAn表示该邻接矩阵的n次幂。那么AnA^nAn的第iii行第jjj列的元素等于长度为nnn的vi−vjv_i - v_jvi​−vj​的途径的个数。\n这条定理看似很复杂，但功能很简单，举例如图1，假设从v1−v4v_1-v_4v1​−v4​，如果有长度为6的途径的走法，问一共有几种。那么答案就是A1,46A^6_{ 1,4}A1,46​。\n书中的证明方法用的是数学归纳法，那么我们可以用线性代数的方式以及图论的方式来解释一下书中的证明过程。首先假设当n=kn = kn=k时，Ai,jkA^k_{i,j}Ai,jk​等于长度为kkk的vi−vjv_i-v_jvi​−vj​的途径的数量。那么我们只需要证明：\nAi,jk+1=∑h=1N Ai,hk ⋅ Ah,jA^{k+1}_{i,j} = \\sum^N_{h=1} \\ A^k_{i,h}\\  \\cdot \\ A_{h,j}\nAi,jk+1​=h=1∑N​ Ai,hk​ ⋅ Ah,j​\n对于式中的h代表的是节点，那么上面这个公式就是一个基础的线性代数的计算公式，那么如何用图论来解释呢？\n\n如上图（图3），假定要从v1v_1v1​分6步走v4v_4v4​，按照公式，第六步的方式要与前五步的走法相关，那么我们空出第6步，先只看前5步：\n\n如上图（图4），左侧部分为前5步从v1v_1v1​走到各个点的方式，也就是对应的Ai,hkA^k_{i,h}Ai,hk​，在这里就是A1,h5(h=1⋯4)A^5_{1,h} (h = 1 \\cdots4)A1,h5​(h=1⋯4)。那么最后一步就是从这四个点到v4v_4v4​的走法，分别对应的就是Ah,4A_{h,4}Ah,4​,将每一个节点的结果都加和，就是最终的结果，这个证明就同时结合了线代和图论的知识。\n 定义： \\textcolor{blue} {\\textbf { 定义： } } 定义： 子图（Subgraph），图G={V,ε}G=\\{ V,\\varepsilon \\}G={V,ε}的子图G′={V′,ε′}G&#x27;=\\{ V&#x27;,\\varepsilon&#x27; \\}G′={V′,ε′}由节点集合的子集V′⊂VV&#x27; \\subset VV′⊂V和边集的子集ε’⊂ε\\varepsilon ’ \\subset \\varepsilonε’⊂ε组成，此外，集合V′V&#x27;V′必须包含集合ε′\\varepsilon&#x27;ε′涉及的所有节点。\n举个例子，如图3中节点子集{v2,v3,v4}\\{ v_2,v_3,v_4 \\}{v2​,v3​,v4​}和边子集{e2,e3,e4}\\{ e_2,e_3,e_4 \\}{e2​,e3​,e4​}就构成原图的一个子图。\n","categories":["图深度学习教材笔记"],"tags":["笔记","GNN","教材"]},{"title":"paper-aGentleIntroductionToGNN.md","url":"/2021/11/21/paper-aGentleIntroductionToGNN-md/","content":"","categories":[],"tags":[]},{"title":"第一讲-图的结构","url":"/2021/10/19/MIT-01-md/","content":" 引言\n​\t图机器学习是我博士期间研究方向，这篇博客会从头到尾详细描述我对于该领域的学习过程。理论部分我会通过学习视频课程和看教材来完成，MIT-CS224W-图机器学习这个分类是我用所看的视频课程的名字命名的，该课程是斯坦福的Jure老师在2021年冬季的课程，该课程也是最好的图机器学习启蒙课程，因此，我会对Jure老师的每一集课程都做一期复现与详解，以帮助自己和读者更好的理解这门课程。本文所有的图片以及公式将大量引用自Jure老师的PPT，在此对老师表示感谢，话不多说，开始第一次课程。\n 一、Motivation for Graph ML\n​\t图是描述与分析包含 关联（relations）或  相互作用（interactions）的实体的通用语言。\n​\t在日常生活中，我们身边许多的数据形式都是图结构。如下图所示，从左到右依次为自然图（natural graph）、基础设施图（infrastructure）、社交网络（social network）、知识图谱（knowledge graph）等诸多领域。\n\n​\t不难看出，当前有两种大的数据类型可以表示为图，首先是网络（network），其次就是图（graph）\n​\tNetworks（Natural Graphs）：自然表示为图\n​\t\t社交网络：社会是70亿人口的集合，那么每个人之间的联系就可以构成一个社交网络。\n​\t\t通讯和交易：例如电子设备，电话通讯，金融交易等等。\n​\t\t生物医学：例如身体中基因或蛋白质之间的相互作用。\n​\t\t脑神经元链接：我们的思维就是由数以亿计的脑神经元链接并相互作用产生的。\n\n​\tGraphs：作为一种表示形式\n​\t\t信息或知识图谱：这些节点都是有组织有关联的。\n​\t\t软件：软件也可以表示为图。\n​\t\t相似性网络（similarity networks）：可以多次将数据点（datapoints）与相似的数据点相连构成相似性网络。\n​\t\t具有关联结构：例如分子间结构，场景图，3D模拟等等。\n\n​\t当我们了解图后，我们就要思考如何利用这些关联结构来进行预测呢？\n","categories":["MIT-CS224W-图机器学习"],"tags":["GNN","MIT","2019"]},{"title":"关于","url":"/about/index.html","content":"Hi！你好！我是来自天津大学的博士研究生小泽，首先十分欢迎你来到我的博客。\n","categories":[],"tags":[]}]