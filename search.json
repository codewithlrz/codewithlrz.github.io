[{"title":"Advances and Challenges in Conversational Recommender Systems--A Survey","url":"/2022/01/10/paper-CRS-servey/","content":" 摘要\n推荐系统利用用户的交互历史来估计用户的偏好，当前广泛使用的是静态推荐模型，但由于静态模型没有明确的指令以及它无法从用户处获得主动反馈，因此其很难解决两个重要问题：\n​\t（a）用户到底喜欢什么？\n​\t（b）为什么用户会喜欢这件物品？\n最近，会话推荐系统（CRSs）的兴起改变了上述的情况，用户与系统可以通过自然语言的交互进行动态通信。\n现有的CRS仍未成熟，作者从五个方向总结出开发CRSs的关键挑战：\n​\t（1）Question-based user preference elicitation（基于问题的用户偏好启发）\n​\t（2）Multi-turn conversational recommendation strategies（多轮会话推荐策略）\n​\t（3）Dialogue understanding and generation（对话的理解与生成）\n​\t（4）Exploitation-exploration trade-offs（探究与开发权衡）\n​\t（5）Evaluation and user simulation（评估与用户模拟）\n这些研究领域涉及到信息检索（IR）、自然语言处理（NLP）以及人机交互（HCI）等多个领域研究。\n 引言\n静态推荐模型（传统），主要通过分析用户过去的离线行为来预测偏好。例如用户的点击历史、访问日志、对物品的评价等。早期使用的方法，如协同滤波（CF）、逻辑回归（LR）、因素分解机（FM）和梯度增强决策树（GBDT），由于其有效性以及可解释性。已经在实际应用中广泛使用。当前，已经开发出更多复杂但更有效的神经网络，如Wide&amp;Deep、神经协同过滤（NCF）、深度兴趣网络（DIN）、基于树的深度模型（TDM）以及图卷积网络（GCNs）。\n 静态推荐模型的缺点\n静态推荐模型以用户历史行为为数据进行离线训练。然后用结果为用户提供在线服务。尽管其被广泛使用，但是依然未解决两个问题。\n 用户到底喜欢什么？\n静态模型的学习过程是基于用户历史数据，那么这些数据是会存在噪声，同时也可能是稀疏的。为什么会这样？\n（1）静态模型的一个基本假设就是，所有的历史交互都代表了用户的偏好。\n（2）用户可能做出过错误的决定。\n（3）用户偏好会随着时间推移（之前喜欢不代表现在喜欢）。\n（4）存在新用户或者活跃度低的用户，没有历史数据的支持。\n 用户为什么喜欢这件物品？\n在现实生活中影响用户决定的因素有很多，比如一个用户购买一件商品有可能是出于好奇或是受他人影响，也可能是深思熟虑过后才购买。不同的用户购买相同商品很可能动机不同，因此用相同标准看待不同的用户或者用相同标准看待同一用户的不同活动，是不适合推荐的。静态模型很难理清用户消费行为背后的不同原因。\n尽管人们已经做了很多努力来消除这些问题，但他们做出的假设是有限的。例如，一个常见的设置是利用大量的辅助数据（如社交网络、知识图）来更好地解释用户意图，然而这些附加的数据也可能是不完整和有噪声的。关键的困难来源于静态模型的固有的机制，其交互模式限制了用户的表达（根本没有交互），导致了用户与机器之间存在信息的不对等。\n CRSs介绍\n文中将会话推荐系统（CRSs）定义为： 一种能通过实时的多轮对话，探出用户的动态偏好，并根据用户当前的需求采取行动的推荐系统。\n从定义可以看到，其中一个属性是多轮交互，就是用户与系统之间以任意形式的交互，包括书面、自然语言甚至是手势等等方式。用过交互，推荐系统可以更加容易的推断出用户当前的偏好以及理解消费动机。如图一，使用户求助代理商音乐推荐。\n\n如图所示，代理商会先根据用户先前的偏好为其推荐，如果不满足用户需求就会根据用户的反馈进行更改。这就与先前的静态推荐系统不同，静态系统没有人机交互过程，而CRSs有交互过程。\n CRSs与交互推荐的联系\n在交互推荐的设置中，每一个推荐之后都会有一个反馈信号，表示用户是否或者有多么喜欢这个推荐。但是，交互式推荐的效率较低，由于物品种类太多，对于理解用户的意图是不明确的。因此，可以利用物品的属性信息，来缩小候选项范围。其中一种方案是基于评价的推荐系统，它旨在探究用户对某些属性的反馈而不是物品层面。举个例子，当用户购买手机时，可根据用户提供的“价格更便宜”或是“电池更耐用”等属性反馈，推荐更合适的机型。这种机制能够有助于快速缩小候选项范围。\n虽然上述 方式有效，但仍存在局限性。比如应该避免收到用户反馈就给出推荐，应当是系统觉得计算出的结果可信度高了再给出推荐。同时使用当前交互系统，用户只能通过预设的选项与系统交互，所以需要考虑设计更灵活的形式。\n CRSs与其他会话式人工智能系统的联系\n除了CRSs，当前市面上还有很多其他的会话式的AI系统，最常见的比如面向任务的会话系统、聊天机器人、问答系统等等。那么这些与CRSs有什么区别呢？\n\n\n中心任务的不同\n虽然都是基于会话，但其他系统的会话方式只基于自然语言文本。而CRSs可以基于任何形式，如标签，按钮甚至手势等。\n\n\n使用场景的不同\n在某些使用场景中会存在不同，如常见的问答系统只针对与用户的单个回合的问题进行回答，而CRSs是多回合推荐。\n\n\n 本次调查的焦点\n文中将CRSs以一种通用框架呈现，如下图。\n\n可以看到系统由三个组件组成：\n\n\n用户界面\n充当用户与机器之间的转换器，即从用户的原始表达中提取信息并转换为机器可理解的表示形式。\n\n\n会话策略模型\n相当于CRSs的大脑，首先是决定CRS的核心逻辑，如激发用户偏好，保持多回合对话，引导新主题等。同时也要协调其他两个组件。\n\n\n推荐引擎\n负责建模实体间的关系，学习和记录用户对项目和项目属性的偏好，检索所需信息。\n\n\n同时基于三个组件，文中也总结了五个主要挑战：\n\n\n基于问题的用户偏好引导\n该挑战主要有两个问题，如何询问？如何根据用户回应调整推荐？\n\n\n多轮会话式推荐策略\n在与用户交互过程中，可以让用户通过选择是继续提出要求还是直接获得推荐结果。虽然得到的问题越多结果越确定，但是系统应尽可能减少交互的回合数。\n\n\n自然语言的理解与生成\nCRSs应能够准确捕获用户的语义信息，同时也要生成具有可读性、流畅性的自然语言给用户。\n\n\nExploration and Exploitation (E&amp;E)之间的权衡\n对于用户来说，只会与数据集中的小部分项目存在交互，甚至是对于新用户毫无交互。因此CRSs需要对用户进行积极探索。\n\n\n评估与用户模拟\n由于评价CRSs需要大量的在线用户进行交互，这代价太高，因此需要开发用户模拟器。\n\n\n 基于问题的用户偏好引导\n即使用户知道自己想要什么，也是需要进行精准的关键词检索，并且检索结果也是一堆候选项。但是推荐系统推荐的是用户可能喜欢的物品，这与用户所熟悉的物品是不同的。传统的推荐系统只能利用历史记录作为输入，从而受限制。但CRSs系统可以通过实时交互，通过用户反馈的某些属性判断此时用户的需求与态度，并作出及时调整。\n问题驱动的方法关注于在对话中应该问什么的问题。一般有两种方法：（1）询问物品（2）询问物品的属性/主题/类别。\n 询问物品\n文中介绍三种方法，可以引导出用户对物品的态度，从而快速调整。\n 基于选择的方法\n其主要思想就是经常让用户从给定的选项中选择喜欢的项目或项目集，常见策略包括：\n\n从两个给定选项中选择一个项目\n从给定项目的列表中选择一个项目\n从两个给定的列表中先择一组项目\n\n在用户选择项目后，系统会根据反馈更改推荐。在选择过程中也应尽可能确保两个候选集是不同的或是可区分的。\n例如： Loepp等人 使用矩阵分解（MF）来初始化用户和商品的嵌入，然后从商品嵌入空间中选择两组商品作为候选集，然后让用户选择这两组商品之一 。 重要的是要确保两个候选集尽可能不同或可区分。作者采用了MF算法，并按解释的方差递减的顺序逐一获得了嵌入向量。因此，这些因素，即嵌入向量的不同维数，是按独特性排序的。然后，作者迭代选择只有一个因子值变化的两个项目集。例如，如果两个因素分别代表电影的幽默度和动作度，则两个候选集是具有较高幽默度的电影集和具有较低幽默度的另一组电影，而具有较高幽默度的电影集两套固定在平均水平。当用户选择一个商品集时，用户的偏好嵌入向量将设置为所选商品的嵌入向量的平均值。随着交互过程的继续，选择变得更加困难。用户可以选择忽略该问题，这意味着用户无法区分两个物品集之间的区别，或者他们不在乎。\n 贝叶斯偏好引导方法\n可定义函数u(xj,ui)u(x_j,u_i)u(xj​,ui​)代表用户 i 对商品 j 的偏好。在通常情况下，u(xj,ui)=xjTuiu(x_j,u_i) = x_j^Tu_iu(xj​,ui​)=xjT​ui​。\n在贝叶斯条件下，用户 i 的偏好将有之前的向量形式替换为概率分布形式，可先计算用户先验概率P(U(i))P(U ^ { (i)} )P(U(i))，然后单个物品 j 对于用户 i 的期望可计算为：\n\n那么其中的最大期望就可作为推荐项，即 argmaxjE[u(xj,ui)]argmax_j\\mathbb {E} [u(x_j,u_i)]argmaxj​E[u(xj​,ui​)] 。\n系统可选择一些要询问的物品，同时对于用户的状态认知分布（belief distribution）可通过用户的反馈进行更新。若给定一个用户对于问题 qqq 的回复 rir_iri​ ,用户的后验概率 P(ui∣q,ri)P(u_i|q,r_i)P(ui​∣q,ri​) 可写为：\n\n同时，询问策略也有两类：（1）两两比较，用户在两个项目（集）中选择更喜欢的，（2）用户从多个选项中选择。\n 交互式推荐\n交互式推荐模型主要是基于强化学习，首先是可以使用MAB算法，其优势是：\n\n可以自然的支持会话场景\n可以利用用户以前喜欢的物品，并探索用户可能喜欢但从未尝试过的物品。\n\n还有人提出元学习的方法，这些都会在后续具体展开来讲。\n由于候选项实在太大，如果只询问物品本身效率会很低，同时用户也会随着交互次数过多而感到无聊，所以以属性为中心的问题更实际，因此利用用户对属性的偏好成了一个关键的研究问题。\n 询问属性\n可以通过属性而更有效的排除候选项，其挑战在于如何在不确定用户需求的情况下，系统该询问哪一系列的属性。下面是一些主流方法。\n 历史交互拟合模式\n一个会话可以看作一个实体的序列，包括已经消费的物品和所提及的属性。其目标是为了学习预测下一个要询问的属性或是下一个要推荐的物品。因此为捕获用户行为模式的长期和短期的依赖，可以使用时序神经网络（RNN），如GRU、LSTM。\n其中一个范例就是Christakopoulou等人提出的Q&amp;R模型，它首先让用户在给定的列表中选择一个或多个主题。这个模型包含一个触发机制，这个机制可以是随机的，也可以是制定一些标准获取用户状态，甚至可以是用户自己发起。当用户在第 ttt 次选择时，那么用户是否选择下一个主题 qqq 的概率就基于他之前的观看历史 e1,…,eTe_1,\\dots,e_Te1​,…,eT​ ，即 P(q ∣ e1,…,eT)P(q\\ |\\ e_1,\\dots,e_T)P(q ∣ e1​,…,eT​) 。当选择主题 qqq 后，系统对推荐物品 rrr 的条件概率就是 P(r ∣ e1,…,eT,q)P(r\\ |\\ e_1,\\dots,e_T,q)P(r ∣ e1​,…,eT​,q) ，这种算法可以用在新用户上。Zhang等人还提出通过将用户的评论转为潜在向量，但这种方式时有局限性的，因为有的评论是负面的。\n系统生成的问题是使用预定义的语言模板，因此系统只是关注用户选择的方向与价值，毕竟其核心任务是推荐不是语言生成。\n上述方法有一个共同缺点，就是从用户的历史行为中学习是不能理解交互背后的逻辑的，这些模型只是在试图适合历史偏好，没有考虑过一旦用户拒绝了该怎么办。\n 减少不确定性\n一些研究试图简历一个简单的逻辑来缩小候选项的范围。\n 基于评论的方法\n上述的评论模型具备一种启发式策略，可以引导出用户对属性的偏好，那么基于这一点可以利用用户所评论的属性来移除不满足的属性，进而重构候选集。可以将用户的评论转为潜在向量Z⃗i,j\\vec {Z}_{i,j}Zi,j​，那么这个向量可用于生成评分r⃗i,j\\vec{r}_{i,j}ri,j​，以及解释的属性S⃗i,j\\vec{S}_{i,j}Si,j​，一旦用户不喜欢那个属性，就直接把属性向量中的属性归零，然后更新所有向量。\n 强化学习驱动方法\n强化学习可用于CRSs来选择适当的属性来询问，系统不仅可以选择属性，还可以选择何时改变当前对话的主题。\n 受图约束的候选项\n图是一种表示不同实体之间关系的普遍结构。利用图来筛选给定的一组属性的项是很自然的。例如，Lei等人在一个异构图上提出了一种交互式路径推理算法，它将用户、项目和属性表示为节点，一个边连接的两个节点表示两个节点之间的关系，例如，用户购买了一个项目，或一个项目对一个属性有一定的值。图的优势就是可以将一段对话转换成一张图上的路径，如下图二：\n\n 其他方法\nZou等人提出了一种基于扩展矩阵分解模型的问题驱动推荐系统，它只考虑用户评级数据，以结合用户的实时反馈。\n基本的假设是，如果用户喜欢一个物品，那么他一定喜欢这个物品的全部属性。基于这一点，如果只需要询问用户是否喜欢系统最不确定的属性就好，因为喜欢物品所共有的一些属性是一定喜欢的。\n 总结\n上述方法大多存在一些缺陷，系统会不断的提出问题，每个问题都会给出一个推荐，只有当用户满意了或者自己退出了才会终止，这就好像在审问用户一样。同时，在询问过程中，如果系统还没有理清用户的偏好时，也不应该将认可度低的推荐给用户。\n总之，需要一个多轮会话策略来控制如何在询问和推荐之间切换，这种策略应该在交互过程中动态变化。\n CRSs的多轮会话策略\n问题驱动的方法关注于“该问什么”,这一节则侧重于“何时问”或是“如何保持会话”，一个好的策略不仅可以在适当的时间（高度自信）提出推荐，灵活地适应用户的反馈，还可以保持对话话题，适应不同的场景，让用户在互动中感到舒适。\n 决定何时询问和推荐的会话策略\n好的策略可以是一个基于规则的策略，比如说可以是每提出 k 个问题就给出一次推荐。也可以是随机策略或者是基于模型的策略。\n在SAUR模型中，可以设置一个触发器来激活推荐模块，当系统认为可信度较高了就自动触发推荐，否则将继续询问。但是这种方式过于简单，它不能捕获丰富的语义信息，例如现在讨论的是什么话题，这个话题被讨论到什么程度了。因此，引入了强化学习（RL）。例如，Sun和Zhang[187]提出了一个被称为会话推荐模型(CRM)的模型，该模型使用了面向任务的对话系统的架构。在CRM模型中，用一个置信追踪器去追踪用户的输入，然后输出一个潜在向量，表示对话的当前状态和迄今为止已捕获的用户首选项。然后将状态向量输入到一个策略网络中，用以决定是推荐一个项目还是继续提问。比如说有 k+1 步动作，那么前 k 步是对某一方面的询问，第 k+1步就是给出推荐。\n在CRM问题中的潜在向量是捕获表面信息的，也不太适应动态环境。因此 Lei 等人提出 EAR 框架，让机器在正确的时间提出问题，在他们的定义中，正确的时间是（1）候选集足够小，（2）从信息的获取或是用户的耐心上来讲，所提出的额外问题意义不大了，（3）推荐引擎认为当前可信度会被用户接受。EAR 工作流程如下（ 图三）：\n\n系统必须根据得到的信息决定是否继续询问关于属性的问题，还是给出推荐。为了确定何时提出问题，他们构建了RL模型的状态，并考虑四个因素：\n\n当前候选项属性中每个属性的熵信息。询问熵大的属性有助于减少候选空间，从而有利于以更少的回合找到想要的项目。\n用户对每个属性的偏好。预测偏好高的属性很可能得到正反馈，这也有助于减少候选空间。\n历史用户反馈。如果系统已经询问了用户批准的一些属性，那么这可能是推荐的好时机。\n剩余候选商品的数量。 如果候选商品名单足够短，系统应该转向推荐，避免浪费更多的轮次。\n\n同时，不要把用户未选择的作为负样本，因为用户可能只是忽略了或是只是想尝试些新东西。\n此外，Lei 等人还通过整合用户、物品、属性的知识图谱，提出 CPR 模型来扩展 EAR 模型。原 EAR 模型中属性的选择都是不规则与不可预测的，CPR 在选择属性与推荐的过程中，都是严格遵循知识图谱上的路径，因此可以解释，同时知识的整合提高了推理能力，模型效率更高。\n 更广泛角度的会话策略\n大多数的CRS模型都是假设用户知道他们想要什么，而实际上，求助推荐的用户很可能并不知道他们想要什么，因此CRS不仅应该提出问题询问客户，也要承担起引导主题的责任。\n 会话中的多主题学习\nLiu 等人借鉴主动会话的思想，提出一个新的任务，它将会话推荐置于多主题对话的环境中，在这个模型中，系统可以主动的、自然的引导对话，并且可以在多个主题间切换。为了解决这一任务，他们提出了一个多目标驱动的会话生成（MGCG）框架。该框架由一个目标规划模块和一个目标导向的响应模块组成。目标规划模块可以进行对话管理来控制对话流程，对话流程以推荐为主要目标，完成自然主题转换为短期目标。\n同时文中也指出了一些人工构建的数据集。\n 特殊能力:建议、谈判和说服\n除了智能交互之外，还需要 CRS 有着在不同场景下的反应能力。比如 Rosset 等人建议可以推荐出用户下一步可能想问的问题。比如说如果用户查询“日产GTR价格”，那么系统就可以提供一些推荐例如“租赁日产GT-R需要多少钱？”、“日产GT-R的优点和缺点是什么？”亦或者是一些有趣的推荐例如“日产GT-R是最终的有轨电车吗？”等等，这样不仅丰富用户体验，还会获取更多用户偏好。\n 总结\n会话策略的主要焦点是通过提问来确定何时引导用户的偏好，以及何时给出推荐。作为推荐只有在系统认为可信度高的时候才应该给出，同时自适应策略比静态策略更有前途。除了这个核心功能外，文中还从更广泛的角度介绍了一些策略。\n CRSs中的对话理解与生成\nCRSs的一个重要方向是用自然语言与用户交谈，因此理解用户的意图和产生用户可理解的反应是至关重要的。当前大多数的CRSs只是通过从处理的数据结构中提取关键信息，并基于规则的模板响应显示结果。这需要耗费大量人力来构建规则与模板，用户体验也不好。因此可以通过引入自然语言处理（NLP）技术，帮助CRSs理解用户的意图和情绪。\n 对话理解\n理解用户的意图是一个CRS用户界面的关键需求，因为推荐很依赖这些信息。然而大多CRSs只关注核心推荐逻辑与多轮策略，避开了从用户原始语句和需要预处理的输入中提取用户意图。因此，有必要开发一种以显式或隐式的方式从用户的原始语言输入中提取语义信息的方法，文中介绍了对话系统如何使用NLP技术来解决这个问题。\n 槽填充\n在对话系统中，先预定义一些人们感兴趣的方面，然后从用户的输入中提取信息填写这些方面的值。Sun 和 Zhang 等人提出一个信念追踪器从用户的语句中捕获面值对（facet-value pairs），比如说（颜色，红色）。具体来说，就是在每一个时间点 t 都给定一个向量 ZtZ_tZt​ ，当用户输入一个语句 ete_tet​，即可提取出语义信息，即把ZtZ_tZt​对应位置赋值为 1 ，其余位置仍然为 0 。这样把所有时间段的向量传入LSTM模型中即可。\n在某些场景下，显式地将语义信息建模为面值对可能是一个限制，因为它很难，也没有必要这样做。同时，也不能精确的表达用户意图或感情信息等。\n 意图和情感学习\n神经网络以自动提取特征而闻名，因此可以用来提取CRSs中用户的意图和情绪。CRSs中的一个经典例子是Li等人提出的端到端（end-to-end）框架。它将用户的原始话语作为输入，并在交互中直接产生响应。他们建立了REDIAL数据集，找电影的人需要解释他们喜欢什么样的电影，然后向系统询问电影建议。系统则尝试了解用户的电影品味然后再推荐。所有的信息交互与推荐都使用自然语言，同时所有提及到的电影都要用“@”符号标记，表示这是一个实体，这样RESIAL数据中的对话就包含了所需要的语义信息。同时由于是监督学习，那么也要问三个问题以为数据打上标签：\n\n电影是否被用户提到，还是这是系统的推荐。\n用户是否看过这个电影（看过、没看过、没说）\n用户是否喜欢这部电影或是推荐（喜欢、不喜欢、没说）\n\n通过这种方式，虽然面值约束被删除，但包括上述项和属性、用户态度和用户兴趣都被保留并标记在原始话语中。基于深度神经网络的模型由四个部分组成：\n\n是一个层次循环编码器，用双向GRU实现，用以将原始话语转换为保留关键语义信息的潜在向量。\n在每次检测到一个电影实体时（使用“@“标记），将实例化一个RNN模型来分类搜索者关于该实体的情绪或意见。\n一个基于自动编码器的推荐模块，它将情绪预测作为输入，并产生一个推荐物品。\n生成响应的交换解码器，决定是否将推荐项推荐给用户，同时生成一个句子。\n\n但是深度神经网络经常被评论是不透明和难以解释的，所以现在还是不清楚深度语言模型如何了解用户的需求。因此有人提出使用BERT模型。\n 响应生成\n一个基于自然语言的CRS响应至少应该满足两个级别的标准。较低级别的标准要求生成的语言是正确的；较高级别的标准要求响应的推荐结果是有意义且有用的。\n 生成恰当的自然语言话语\n许多CRSs通过模板方法生成响应，然而模板方法会产生重复且不灵活的输出，此外模板响应会损害用户体验，因此在CRSs中自动生成适当和流畅的响应式很重要的。在文中介绍了两种方法。\n 基于检索的方法\n其基本思想是从大量的候选项中检索适当的响应，这个问题可以理解为是用户输入的查询与候选项之间的匹配问题。最直接的方式就是度量表示查询和响应的特征向量的内积。\n有两种策略，一种是用神经网络分别从用户的查询和候选响应中学习表示向量，然后定义匹配函数进行组合，输出匹配概率。另一种是先组合用户查询和候选响应的表示向量，然后在对组合后的表示对进行学习。第一种适合在线服务，第二种的匹配信息被充分的保存与挖掘。\n 基于生成的方法\n与基于检索的方法从模板响应数据库中选择现有的响应不同，基于生成的方法直接从模型中生成一个句子。基本的生成模型是一个循环的序列到序列的模型，以查询的每个单词作为输入，然后依次输出生成的单词。\n基于生成的方法存在一些挑战，首先是不能保证一定能够生成一个良好的自然语言句子，同时由于机器没有基本的常识与情感，让用户依然能够区分出人与机器。甚至有的机器为了说一个安全的答案，还会说”OK“、”我不知道你在说什么“，这很伤用户。因此，Ke等人提出要明确的控制生成的句子的功能，比如说机器可以用询问音来表示询问，命令音表示指示等等。其次是如何评估生成的响应，因为没有标准答案，这个在后文会讲。\n同时，一个句子的正确并不代表它完成了推荐任务，推荐的实体一定要在句子中被提及，同时也也要引入一些外部知识，使得推荐结果是有意义的。\n 合并面向推荐的信息\n利用端到端框架作为用户界面有一个主要的限制，那就是只有在训练语料库中提到的物品才能被推荐，这使其性能受人工推荐的训练数据影响。因此Chen等人提出引入知识图谱，（1）对话界面可以通过连接知识图谱中的相关实体来帮助推荐引擎，推荐模型基于R-GCN模型从知识图中提取信息，（2）推荐系统也可以帮助对话界面，通过采集高概率单词，对话可以将电影与一些有偏向的词汇连接起来，从而产生一致的、可解释的响应。\nZhou等人引入一种面向物品的知识图谱，但是系统存在不理解原始话语中的一些单词的情况，因此他们合并了两个知识图谱，一个是面向词的，一个是面向物的。同时还提出了一种基于自我注意的推荐模型，可以调整相应实体在图谱上的表示。\n 本章摘要\n一般来说，交互式会话、评论方式以及聚焦于多轮会话策略的CRSs更倾向于使用带有预处理的输入和基于规则或是基于模板的输出。对话系统与关注对话能力的CRSs系统才更有可能使用原始的自然语言。在未来，CRSs中的用户理解与响应仍然是一个关键的研究领域，只要它还是作为CRSs的用户界面并直接影响用户体验。\n Exploration-Exploitation Trade-offs\nCRSs面临的一个挑战是处理历史交互很少的冷启动用户。解决这一问题的办法就是用E&amp;E权衡。通过开发，系统可以利用已知的最佳选项。通过探究，虽然系统会承担一些收集未知选项信息的风险。但为了实现长期的优化，人们可以做出短期的牺牲。在E&amp;E的早期阶段，探究试验可能会失败，但它警告该模型在未来不要经常采取这种行动。\n在CRSs中引入了MAB算法来改进推荐值。\n 多臂老虎机（MAB）推荐\n MAB介绍\nMAB是一个经典问题，在一排老虎机面前，如果想要最大化预期收益，就要选择是继续玩当前的老虎机还是换下一台，每一台老虎机都是一个黑盒，同时每一台的属性（获胜概率）都只能通过之前的实验反馈来估计。\n同时，这问题可以理解为最小regret function，使最优预期积累与估计预期积累之间差异最小。\n\n其中 a∗a^*a∗ 是理论上在任何时候都能获得最大预期奖励的最佳手臂。\n常见MAB策略就是贪心，只利用当前估计最高的手臂。还有随机策略，只探究的策略，还有同时包括贪心与随机的策略。还有一些其他的如UCB与TS。\n 基于MAB的推荐方法\nMAB算法可以嵌入到在线推荐或者交互式推荐系统当中，在推荐最佳物品的过程中，每个手臂都对应一个物品。传统的老虎机思想只将物品视为独立的手臂，而忽视了它的属性，根据累积的奖励直接估计每个物品的概率，由于有大量物品，所以是低效的。\n在推荐中，用户和物品都有着一组丰富的特征，所以用户utu_tut​是否会选择物品ata_tat​可以通过它俩的特性进行预测。Li等人提出一个名为LinUCB的线性上下文的老虎机模型。这也是第一个在推荐系统中联系上下文（即用户/项目特性）的老虎机模型。\n也有研究指出，对于推荐的探究是重要的。比如推荐应该是多样化的，而不是收到类似物品的限制，例如Ding等人认为用户可能对物品的多样性有不同偏好，比如有特定兴趣爱好的用户可能更喜欢相关的物品集。而没有特定爱好的用户可能更喜欢不同的物品集来探究自己的兴趣爱好，因此他们提出了一种既考虑相关物品集又考虑多样物品集的框架，这是一种权衡准确性与多样性的方法。\n此外Yu等人认为，可以在视觉对话增强交互推荐系统中级联老虎机。其想法是，用户从第一个推荐一直看到最后一个，那么第一个选择的就是对他有吸引力的，那么之前看过的肯定是没有吸引力，因此可以作为可靠的负样本。\nMAB的主要优势就是在于它能够进行在线学习，能够使模型更好了解冷启动用户的偏好，并且可以在多次试验后快速调整策略的能力，从而能够追求全局最优。\n CRSs中的MAB\nCRSs与用户交互的能力使其能够直接使用基于MAB的方式帮助推荐。Chri等人基于MAB提出了一种经典的CRS。它使用几种简单的MAB方法来增强离线概率矩阵分解（PMF）模型。首先使用离线数据初始化模型参数，然后利用用户的实时反馈，通过常见的MAB模型来更新参数。一方面，由于在线更新，初始化模型的性能有所提高，另一方面，离线初始化有助于降低计算复杂度。\n原始的MAB方法忽略了项目的属性，Zhang等人提出一种ConUCB算法将LinUCB应用于CRS上下文。ConUCB不是询问项目，而是询问用户关于一个或多个属性。具体来说，他们假设用户对属性的偏好可以传播到物品，因此系统可以分析用户对查询属性的反馈，从而快速缩小候选项目的范围。此外，作者还使用了一个人工函数来确定询问属性或提出建议的时间，比如每 m 轮进行 k 次对话。\n但是上述策略是不灵活的，因为系统应该在认为可信度高的时候才给出推荐。因此，Li等人提出ConTS方法，来自动交替询问关于属性的问题，他们聚合同一个手臂中的所有物品与属性。因此从手臂聚合中选择的手臂既可以是推荐，也可以是属性问题。流程如下图：\n\n CRSs中的元学习\n除了MAB，还有一些工作在尝试保持E&amp;E的平衡。邹等人。[271]将交互式推荐表述为一个元学习问题，其目标是学习一个学习算法，它将用户的历史交互作为输入，并输出一个可以应用于新用户的模型（策略函数）。此外，Lee等人通过一个基于MAML模型的模型来解决推荐中的冷启动问题，学习到的推荐模型可以在微调阶段通过向冷用户询问一些关于某些物品的问题来快速适应冷用户偏好。 这项工作的一个缺点一个候选的证据只能被选择一次，并且询问过程只在冷用户刚开始使用时进行。 最好将此策略扩展到 CRS 设置并开发动态多轮询问策略以进一步增强推荐。\n 评估和用户模拟\n本节作者讨论如何评估CRSs，并尝试分为两类。\n\n轮级评估，它评估系统输出的单个回合，包括推荐任务和响应生成任务，这两者都是监督预测任务。\n对话级评价，评估多回合对话策略的性能，这是一个顺序决策任务。\n\n为了实现这一目标，用户模拟很重要的。\n 数据集和工具\n下表中列出了常用的CRS数据集的统计数据。一些研究通过要求真正的用户在特定规则下使用自然语言进行对话来收集人与人和人机对话数据。\n\n如前所述，许多关于CRS的研究都集中在交互策略和推荐策略上，而不是语言理解和生成。因此，所有这些研究所需要的都是多回合对话中的标记实体（包括用户、项目、属性等）。\n尽管在CRSs中似乎有许多数据集，但这些数据集没有能力开发可以在工业应用中可使用的CRSs。原因有两个方面：首先，这些数据集的规模不足以覆盖现实世界的实体和概念；其次，对话要么由非对话数据构建，要么在一定的严格约束下生成，因此很难推广到复杂而多样的真实世界对话。\nZhou等人已经实现了一个叫CRSLab的工具包用于构建和评估CRSs。将现有CRSs中的任务统一为三个子任务：推荐、对话和策略，这分别对应于图二中的三个组成模块：推荐引擎、用户界面和对话策略模块。\n 轮级评估\nCRSs的精准评估是对于每单轮的输出进行处理，包含两个任务：语言生成和推荐。\n 对语言生成能力的评估\n对于生成基于自然语言的响应以与用户交互的CRS模型，生成的响应的质量是至关重要的。因此，我们可以采用用于对话响应生成的指标来评估CRSs的输出。两个示例度量标准是BLEU和Rouge。然而，这些指标是否适用于评估语言生成还存在着广泛的争论。因为这些指标只对词汇变化敏感，所以它们不能适当地评估给定参考文献的语义或句法变化。因此，其他反映用户满意度的指标更适合用于评价，如流畅性、一致性、可读性等等。\n基于端到端的对话框架或深度语言模型的CRSs在实践中的可用性方面存在些局限性，当前面临三个关键问题。\n\n对于每个系统，大约三分之一的系统话语在给定的上下文中没有意义，并且可能会导致人工评估中的对话中断。\n不到三分之二的建议被认为在人工评估中有意义。\n这两个系统都没有“产生”话语，因为几乎所有的系统响应都已经出现在训练数据中。\n\n 推荐评估\n通过将预测结果与测试集中的记录进行比较，来评估推荐模型的性能。衡量推荐系统性能的指标有两种：\n\n\n基于评级的指标\n这些指标假设用户的反馈是一个明确的评分分数，例如，一个在1到5个范围内的整数。因此，我们可以测量模型的预测分数和测试集中用户给出的地面真实分数之间的差异。传统的基于评级的度量标准包括均方误差(MSE)和均方根误差(RMSE)，其中RMSE是MSE的平方根。\n\n\n基于排名的指标\n基于排名的指标要求预测项目的相对顺序应该与测试集中的项目的顺序相一致。因此，不需要从用户那里进行显式的评分，而隐式的交互（例如，点击、播放）可以用来评估模型。\n\n\n这些评估方法中最大的问题是，现实世界的用户交互非常少，很大一部分项目永远没有机会被用户使用。然而，这并不意味着用户不喜欢它们。\n 会话级评估\n会话级评估并不是一个有监督项目。因为每个发现都是一个连续过程的一部分，而且系统所做的每一个监视都可能影响未来的观测，因此交互过程不是i.i.d.的独立同分布。此外，会话在很大程度上依赖于用户的反馈。因此，对会话的评估需要进行在线用户测试或利用历史交互数据，这些数据可以通过非策略评估或使用用户模拟来进行。\n 在线用户测试\n通过利用真正的用户反馈来直接评估对话策略。为了进行评估，应设计适当的指标。\n平均回合（AT）是一种优化CRS的全局度量方式，因为模型应该引导用户意图并做出成功的建议，从而以尽可能少的完成对话。\n一个类似的指标是推荐成功率(SR@t)，它衡量的是第 t 回合成功推荐结束的多少对话。此外，尝试失败的比例，例如系统提出的多少问题被用户拒绝或忽略，也是衡量系统是否对用户满意的决策的可行方法。\n除了这些全局统计数据外，每个回合的累积性能也可以反映对话的整体质量。\n在线用户评估虽然有效，但仍存在严重问题：（1）人与CRS之间的在线互动速度较慢，通常需要数周的时间才能收集到足够的数据以使评估具有统计学意义。 （2）收集用户的反馈是昂贵的，并且可能会损害用户的体验，自然的解决方案是在模型开发阶段和评估阶段都拥有模拟用户。\n Off-Policy评估\n其目的是为了回答一个反事实的问题，即如果我们使用πθ\\pi_\\thetaπθ​代替πβ\\pi_\\betaπβ​，也就是说当我们像评估πθ\\pi_\\thetaπθ​的目标策略但只有πβ\\pi_\\betaπβ​的数据集，那仍然可以通过引入重要性抽样或者逆倾向评分来评估目标策略。类似于下面公式：\n\n另一种直观的方法是直接模拟用户行为。\n 用户模拟\n模拟用户的策略一般有四种类型。\n 直接使用用户的交互历史记录\n其基本思想类似于传统推荐系统的评估，其中保留了用户交互数据的一个子集作为测试集。如果CRS推荐的项目在用户的测试集中，那么这个建议将被认为是成功的。\n首先，Lei等人从oracle集中随机选择一个属性作为用户对会话的初始化。会话进入一个“模型行为-模拟器响应”过程的循环，在这个过程中，如果查询实体包含在Oracle集中，模拟用户将响应“是”，否则为“否”。因为其简单，大多数CRS研究采用这种模拟方法，然而，推荐系统中的稀疏性问题仍然存在：用户项矩阵中只有少数值是已知的，而大多数元素缺失，这阻碍了对这些项进行模拟。\n 估计用户在所有项目上的偏好\n使用直接的用户交互来模拟对话也具有与我们上面提到的相同的缺点，即大量没有被用户看到的项被视为不喜欢的项。为了克服评估过程中的这种偏差，一些研究建议提前获取用户对所有项目的偏好。给定一个项目及其辅助信息，模拟用户交互的关键是估计或综合对该项目的偏好。\n 从用户评论中提取信息\n除了用户行为历史外，许多电子商务平台都有文本审查数据。与消费历史不同，一个项目的审查数据通常明确地提到属性，这可以反映用户对该项目的个性化意见。\nZhang等人将Amazon数据集的每一部分的文本审查转换为问答序列，以模拟对话。例如，当一个用户在评论手机X时提到使用安卓系统的蓝色华为手机时，那么根据本文构建的对话序列是(类别：手机→系统：安卓→颜色：蓝色→推荐：X)。Zhang等然通过利用用户评论来构建模拟的交互。基于给定的用户档案及其历史观看记录，他们构建了一个主题线程，其中包括从这些已观看电影的评论中提取的主题（例如，“家庭”或“找工作”）。主题线程由一个规则组织起来，并最终导致目标影片。通过检索相应主题下最相关的评论，充实了合成的对话。\n一个值得注意的问题是，评论中提到的这些方面可能包含了产品的一些缺点，这并不有助于理解用户为什么选择了一个产品。因此，有必要对评审数据进行情绪分析，在选择项目时，只考虑具有积极情绪的属性。\n 模仿人类的会话语料库\n为了生成没有偏见的会话数据，一个可行的解决方案是使用真实世界的两方人的对话作为训练数据。通过使用这种类型的数据，CRS系统可以通过从大量真实的人与人之间的对话中学习来直接模拟人类的行为。\nLiu等人不仅通过记录人工交谈数据，还收集并构建一个知识图，并为每个寻求推荐的用户定义一个明确的配置文件。对话话题可以包含许多非推荐场景，如问答或社交聊天，这些在现实生活中更为常见。\n这种方法也有一些缺点。首先，在收集人类会话语料库时，两个工作者需要同时进入任务，这是一个严格的设置，因此限制了数据集的规模。其次，设计师通常有许多限制对话方向的要求。因此，生成的对话受到限制，不能完全覆盖真实世界的场景。\n最近，Zhang和Balog研究了使用用户模拟来评估CRSs。它们将模拟用户的动作序列组织为一个类似于堆栈的结构，称为用户议程。议程的动态更新被视为一系列的拉或推操作，其中对话行动被从顶部删除或添加到顶部。如下图所示：\n\n 未来的发展方向与机遇\n在描述了该领域在未来的关键进展和挑战之后，作者设想了一些有前途的未来方向。\n 共同优化三项任务\n","categories":["论文解读"],"tags":["论文","CRS"]},{"title":"图表示学习基础","url":"/2021/12/09/book-GRL-01/","content":" 引言（Introduction）\n 什么是图（What Is  A Graph）\n图的定义以及一些性质可参考图论基础。\n 多关系图（Multi-relational Graphs）\n一个图的类型可以根据其边的性质进行划分，如有向图，无向图，加权图等，那么图中含有不同类型的边的图，就可以是多关系图。\n举个例子，在吃药物的过程中，不同种的药物之间会有不同的反应，因此每一条边的类型都会有很大差异，如图1。\n\n那么，就要扩展一种新的边表示方法，添加边的类型τ\\tauτ，例如在简单图中表示一条边为(u,v)(u,v)(u,v),其中u,vu,vu,v为图中的节点。那么表示多关系图的边就可以为(u,τ,v)∈E\\boldsymbol{ (u,\\tau,v) \\in E}(u,τ,v)∈E。\n同时邻接矩阵也要进行改变，从原来的二维变为三维A∈R∣V∣×∣R∣×∣V∣\\boldsymbol{ A \\in \\mathbb{R}^{ |V| \\times |R| \\times |V|}}A∈R∣V∣×∣R∣×∣V∣，其中RRR是一组关系的集合。\n在多关系图中也有两个很重要的子集，异质图（Heterogeneous graph）和多维图（Multiplex graph）\n 异质图（Heterogeneous graph）\n在异质图中，可以将节点划分为多种类型，每一个节点都对应着一种类型。\n表示为 构图中的每一条边都可能会被特定的节点所约束，换句话说，特定的边只会连接特定的节点。举一个例子，假设在一张医学类型的图上有三类节点，分别是蛋白质，药物和疾病。那么在其中代表“治疗”的边只会出现在药物和疾病两类节点之间，类似的代表“多药副作用”的边只会出现在两个药物节点之间。\n有一种特殊的异质图–多部图（Multipartite graphs），其边只能连接不同类型的节点。\n表示为(u,τi,v)∈E→u∈Vj,v∈Vk∧j≠k(u,\\tau_i,v)\\in E \\rightarrow u \\in V_j,v \\in V_k \\wedge j \\neq k(u,τi​,v)∈E→u∈Vj​,v∈Vk​∧j​=k。\n图2为普通异质图与多部图的表现形式。\n\n 多重图（Multiplex graphs）\n在多重图中，可以想象将节点分为kkk层，每一层节点都是一样的，只是每一层节点间的连接方式不同。同时，不同层内的相同节点可以通过层间边相连。其节点集合与边集合与边集合分别表示为：\nV=V1∪V2∪⋯∪VkV = V_1 \\cup V_2 \\cup \\cdots \\cup V_kV=V1​∪V2​∪⋯∪Vk​\nE=E1∪E2∪⋯∪Ek∪Ek+1E = E_1 \\cup E_2 \\cup \\cdots \\cup E_k \\cup E_{ k+1}E=E1​∪E2​∪⋯∪Ek​∪Ek+1​，Ek+1E_{ k+1}Ek+1​为层间边集合。\n举一个例子，如图3所示。\n\n假设每一层中的每一个节点为一个城市，连接的边代表各种交通工具连接两个城市，层间边代表同一座城市可以进行换乘。现在如果在L1层中想从节点v1到达节点v9，由于L1中无相应路径，即可先到达节点v5，通过层间边到达L2层，从而到达v9。\n 特征信息（Feature Information）\n在多数情况下都会有与图对应的特征信息，多数情况下都是节点级别的，所以用一个实数矩阵（real-valued matrix） X∈R∣V∣×m\\textbf{ X} \\in \\mathbb{ R}^{ |V| \\times m} X∈R∣V∣×m表示。\n 图机器学习（Machine learning on graphs）\n图机器学习没有像传统机器学习对监督学习（给定分类标签）与无监督学习（聚类）有明确的界限划分，大多数情况更偏向于\n半监督学习方式。\n 节点分类（Node classification）\n在通常情况下，由于只有少部分节点存在标签信息，但是在训练的过程中仍然会使用无标签的数据，这与监督学习的只使用有标签数据不同，因此节点分类任务通常被归为是半监督学习。同时，节点不满足独立同分布(i.i.d.)。\n当前对于节点分类主要时利用节点之间的连接，可利用其同质性与结构等价性。\n同质性：认为相邻节点属性相似，即节点与邻居节点有共享属性趋势。\n结构等价性：认为具有相似布局结构的节点将具有相似标签。\n异质性：认为节点优先连接与自身不同类型节点（如社交网络中的性别属性）。\n 关系预测（Relation prediction）\n关系预测在不同领域中有有不同的名字，链接预测（Link Prediction）、图谱补全（Graph Completion）、和关系推断（Relational Inference）。与节点分类相似，节点分类是预测节点，关系预测是预测边，即在边的层次研究。方式也是半监督学习，给定一组节点VVV以及部分边的集合Etrain⊂EE_{ train} \\subset EEtrain​⊂E，利用这些信息推断缺失边。\n 聚类和社区发现（Clustering and community detection）\n希望将图表现出一种社区结构，节点能够更有可能与同一社区的节点形成边。\n如下图所示，根据网络中不同节点之间连接的紧密程度，我们可以将网络视为是由不同的“簇”所组成，其中“簇”内的节点之间连接更加的紧密，不同“簇”之间的的节点之间的链接比较稀疏。我们将这种“簇”称为是网络中的社区结构。odes\n\n社区发现的定义：通过输入一张图G=(V,E)G = (V,E)G=(V,E)推断出潜在的社区结构。可以看到是在子图层次进行研究。\n 图的分类、回归与聚类（Graph classifification, regression, and clustering）\n当研究完节点、边、子图后，最后的是整张图级别的研究。\n在图分类或图回归任务中，其数据集由多张不同的图构成，利用图机器学习算法对每张图进行独立预测（不是每张图的组成部分）。\n在图聚类任务中，目标是学习一个无监督的测量图与图之间的相似性策略。\n 背景与传统方法（Background and Traditional Approaches）\n(21条消息) Bag-of-words 词袋模型基本原理_Jaster_wisdom的专栏-CSDN博客_词袋模型的原理\n","categories":["图表示学习笔记"],"tags":["GNN","笔记","图表示学习"]},{"title":"图论基础","url":"/2021/11/24/book-mayao-02-md/","content":" 图的表示\n 图（Graph）\n 定义： \\textcolor{blue} {\\textbf { 定义： } } 定义： 一个图可以表示为G={V,ε}G = { \\{V,\\varepsilon \\}}G={V,ε}，其中VVV = {v1v_1v1​,⋯\\cdots⋯,vNv_NvN​}是大小为N=∣V∣N = | V |N=∣V∣ 的节点集合，ε\\varepsilonε={ e1e_1e1​,⋯\\cdots⋯,eMe_MeM​}是大小为MMM的边的集合。\n\n如上图（图1），为一个有五个节点和六条边的图。其中{ v1v_1v1​,⋯\\cdots⋯,v5v_5v5​}为五个节点，{ e1e_1e1​,⋯\\cdots⋯,e6e_6e6​}为六条边。\n在图中，如果有一条边eie_iei​连接两个节点vei1v_{e_i}^1vei​1​和vei2v_{e_i}^2vei​2​，那么这条边可以表示为（vei1v_{e_i}^1vei​1​,vei2v_{e_i}^2vei​2​），在有向图中表示边从起点vei1v_{e_i}^1vei​1​指向终点vei2v_{e_i}^2vei​2​。相反在无向图中由于没有顺序之分，则eie_iei​ = （vei1v_{e_i}^1vei​1​,vei2v_{e_i}^2vei​2​）= （vei2v_{e_i}^2vei​2​,vei1v_{e_i}^1vei​1​）。举个例子，如图一中的边e6e_6e6​连接节点v1v_1v1​与节点v5v_5v5​，那么由于图1为无向图，则e6e_6e6​也可以表示为（v1v_1v1​,v5v_5v5​）或（v5v_5v5​,v1v_1v1​）。\n 邻接矩阵（Adjacency Matrix）\n为了方便查看节点之间的连接关系，图G={V,ε}G = { \\{V,\\varepsilon \\}}G={V,ε}可以等价的表示为邻接矩阵的形式，更加直观的描述节点之间的关系。\n 定义： \\textcolor{blue} {\\textbf { 定义： } } 定义： 给定一个图G={V,ε}G = { \\{V,\\varepsilon \\}}G={V,ε},对应的邻接矩阵可以表示为A∈{0,1}N×NA∈{ \\{ 0,1 \\} ^ { N×N } }A∈{0,1}N×N。邻接矩阵AAA的第iii行第jjj列元素Ai,jA_{i,j}Ai,j​表示节点viv_ivi​和vjv_jvj​的连接关系。具体来讲，如果viv_ivi​与vjv_jvj​相邻，则Ai,j=1A_{i,j} = 1Ai,j​=1，否则Ai,j=0A_{i,j} = 0Ai,j​=0。\n在无向图中，由于不受节点顺序影响，则Ai,j=Aj,iA_{ i,j } = A_{ j,i }Ai,j​=Aj,i​，所以无向图的邻接矩阵一定是 关于主对角线对称 \\textbf { 关于主对角线对称 } 关于主对角线对称 的。以图1为例，该图的邻接矩阵可表示为：\nA=(0101110100010011000110110)A = \n\\begin {pmatrix}\n0 &amp; 1 &amp; 0 &amp; 1 &amp; 1 \\\\\n1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\\n0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 \\\\\n1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\\n1 &amp; 0 &amp; 1 &amp; 1 &amp; 0 \\\\\n\\end {pmatrix}\nA=⎝⎜⎜⎜⎜⎛​01011​10100​01001​10001​10110​⎠⎟⎟⎟⎟⎞​\n可以看到，主对角线值均为0，且都关于其对称。\n 图的性质\n 度（Degree）\n 定义： \\textcolor{blue} {\\textbf { 定义： } } 定义： 在图G={V,E}G = { \\{V,E \\}}G={V,E}中，节点vi∈Vv_i ∈ Vvi​∈V的度定义为图GGG中与节点viv_ivi​相关联的边的数目。\nd(vi)=∑vj∈V1ε({vi,vj})d(v_i) = \\sum_{ v_j ∈ V}1\\varepsilon( \\{ v_i,v_j \\} )\nd(vi​)=vj​∈V∑​1ε({vi​,vj​})\n其中，1ε(⋅)1\\varepsilon(\\cdot)1ε(⋅)为指示函数，简单来说就是满足条件就取1，不满足就取0。对于图来讲可表示为：\n1ε({vi,vj})={1,(vi,vj)∈ε0,(vi,vj)∉ε1\\varepsilon( \\{ v_i,v_j \\} ) = \n\\begin {cases}\n1, \\quad (v_i,v_j)∈\\varepsilon \\\\\n0, \\quad (v_i,v_j)\\notin\\varepsilon\n\\end {cases}\n1ε({vi​,vj​})={1,(vi​,vj​)∈ε0,(vi​,vj​)∈/​ε​\n即当存在(vi,vj)(v_i,v_j)(vi​,vj​)这条边时，d(vi)d(v_i)d(vi​)就加1，反之如果不存在，就加0。其实最直观的看法就是找从这个节点出发引出几条边。但这种方法会有一些麻烦，所以如果能够画出图的邻接矩阵的话，可以更简单的计算结果，利用邻接矩阵可以将节点的度表示为：\nd(vi)=∑j=1NAi,jd(v_i) = \\sum_{ j = 1 }^N A_{ i,j }\nd(vi​)=j=1∑N​Ai,j​\n简单来说就是计算该节点所在行所有值的和。任以图一为例，节点v5v_5v5​与v1,v3,v4v_1,v_3,v_4v1​,v3​,v4​相连，则节点v5v_5v5​的度为3。如果利用邻接矩阵计算，v5v_5v5​占矩阵第五行，直接将改行所有元素相加，即可得到其度为3。\n 邻域（Neighborhood）\n 定义： \\textcolor{blue} {\\textbf { 定义： } } 定义： 在图G={V,ε}G=\\{ V,\\varepsilon \\}G={V,ε}中，节点viv_ivi​的邻域N(vi)N(v_i)N(vi​)是所有和它相邻的节点的集合。\n依然以图1中v5v_5v5​节点为例，其邻域为N(v5)={v1,v3,v4}N(v_5) = \\{ v_1,v_3,v_4 \\}N(v5​)={v1​,v3​,v4​}，可以发现对于任意节点viv_ivi​,其 邻域中元素个数等于该节点的度 \\textbf{ 邻域中元素个数等于该节点的度 } 邻域中元素个数等于该节点的度 ，即d(vi)=∣N(vi)∣d(v_i) = |N(v_i)|d(vi​)=∣N(vi​)∣。\n 定理： \\textcolor{green} {\\textbf { 定理： } } 定理： 一个图G={V,ε}G=\\{ V,\\varepsilon \\}G={V,ε}中所有的节点的度之和是图中边数量的两倍：\n∑vi∈Vd(vi)=2 ⋅ ∣ε∣\\sum_{ v_i ∈ V}d(v_i) = 2\\ \\cdot \\ | \\varepsilon |\nvi​∈V∑​d(vi​)=2 ⋅ ∣ε∣\n这个定理很容易想通，举一个例子，如下图（图2），v1与v2v_1与v_2v1​与v2​的度都为1，度之和为2。但两个节点公用一条边e1e_1e1​，则是2倍的关系。\n\n同样的，如果将图写为邻接矩阵的话，每一个非零元素则代表两个节点存在连接，第iii行中非零元素的个数就是节点viv_ivi​的度，那么矩阵中所有非零元素的个数就是所有节点的度之和，那么可以有如下推论。\n 推论： \\textcolor{green} {\\textbf { 推论： } } 推论： 无向图邻接矩阵的非零元素的个数是边的数量的两倍。\n 连通度（Connectivity）\n 定义： \\textcolor{blue} {\\textbf { 定义： } } 定义： 途径（Walk）图的途径是节点和边的交替序列，从一个节点开始，以一个节点结束，其中每条边与紧邻的节点相关联。\n 定义： \\textcolor{blue} {\\textbf { 定义： } } 定义： 迹（Trail）是边各不同的途径。\n 定义： \\textcolor{blue} {\\textbf { 定义： } } 定义： 路（Path）是节点各不相同的途径，也称路径\n简而言之，只要不跳着走，那从一个节点到另一个节点就是一个途径，在途径这个大类中，如果序列中没有重复边就是迹，没有重复节点就是路。举个例子，图1中想从v1v_1v1​走到v2v_2v2​，假设绕远走，那么可以选择(v1,e4,v4,e5,v5,e6,v1,e1,v2)(v_1,e_4,v_4,e_5,v_5,e_6,v_1,e_1,v_2)(v1​,e4​,v4​,e5​,v5​,e6​,v1​,e1​,v2​)这条途径，那么这条途径也是一条迹但不是路，因为这条途径中没有重复的边，但存在重复的节点v1v_1v1​。如果选(v1,e1,v2)(v_1,e_1,v_2)(v1​,e1​,v2​)这条途径，那么这即可以成为是迹，也可以称为是路径。\n 定理： \\textcolor{green} {\\textbf { 定理： } } 定理： 对于图G={ε,V}G = { \\{ \\varepsilon,V \\} }G={ε,V}及其邻接矩阵AAA，用AnA^nAn表示该邻接矩阵的n次幂。那么AnA^nAn的第iii行第jjj列的元素等于长度为nnn的vi−vjv_i - v_jvi​−vj​的途径的个数。\n这条定理看似很复杂，但功能很简单，举例如图1，假设从v1−v4v_1-v_4v1​−v4​，如果有长度为6的途径的走法，问一共有几种。那么答案就是A1,46A^6_{ 1,4}A1,46​。\n书中的证明方法用的是数学归纳法，那么我们可以用线性代数的方式以及图论的方式来解释一下书中的证明过程。首先假设当n=kn = kn=k时，Ai,jkA^k_{i,j}Ai,jk​等于长度为kkk的vi−vjv_i-v_jvi​−vj​的途径的数量。那么我们只需要证明：\nAi,jk+1=∑h=1N Ai,hk ⋅ Ah,jA^{k+1}_{i,j} = \\sum^N_{h=1} \\ A^k_{i,h}\\  \\cdot \\ A_{h,j}\nAi,jk+1​=h=1∑N​ Ai,hk​ ⋅ Ah,j​\n对于式中的h代表的是节点，那么上面这个公式就是一个基础的线性代数的计算公式，那么如何用图论来解释呢？\n\n如上图（图3），假定要从v1v_1v1​分6步走v4v_4v4​，按照公式，第六步的方式要与前五步的走法相关，那么我们空出第6步，先只看前5步：\n\n如上图（图4），左侧部分为前5步从v1v_1v1​走到各个点的方式，也就是对应的Ai,hkA^k_{i,h}Ai,hk​，在这里就是A1,h5(h=1⋯4)A^5_{1,h} (h = 1 \\cdots4)A1,h5​(h=1⋯4)。那么最后一步就是从这四个点到v4v_4v4​的走法，分别对应的就是Ah,4A_{h,4}Ah,4​,将每一个节点的结果都加和，就是最终的结果，这个证明就同时结合了线代和图论的知识。\n 定义： \\textcolor{blue} {\\textbf { 定义： } } 定义： 子图（Subgraph），图G={V,ε}G=\\{ V,\\varepsilon \\}G={V,ε}的子图G′={V′,ε′}G&#x27;=\\{ V&#x27;,\\varepsilon&#x27; \\}G′={V′,ε′}由节点集合的子集V′⊂VV&#x27; \\subset VV′⊂V和边集的子集ε’⊂ε\\varepsilon ’ \\subset \\varepsilonε’⊂ε组成，此外，集合V′V&#x27;V′必须包含集合ε′\\varepsilon&#x27;ε′涉及的所有节点。\n举个例子，如图3中节点子集{v2,v3,v4}\\{ v_2,v_3,v_4 \\}{v2​,v3​,v4​}和边子集{e2,e3,e4}\\{ e_2,e_3,e_4 \\}{e2​,e3​,e4​}就构成原图的一个子图。\n 定义： \\textcolor{blue} {\\textbf { 定义： } } 定义： 连通分量（Connected Component），给定一个图G={V,ε}G = \\{ V,\\varepsilon \\}G={V,ε}，如果一个子图G′={V′,ε′}G&#x27;=\\{ V&#x27;,\\varepsilon&#x27; \\}G′={V′,ε′}中任意一对节点之间都至少存在一条路，且V′V&#x27;V′中的节点不与任何V/V′V/V&#x27;V/V′中的节点相连，那么G′G&#x27;G′就是一个连通分量。\n如下图（图5），就是一个含有两个连通分量的图，因为其左右两边没有连接。\n\n 定义： \\textcolor{blue} {\\textbf { 定义： } } 定义： 连通图（Connected Graph），如果一个图G={V,ε}G = \\{ V,\\varepsilon \\}G={V,ε}只有一个连通分量，那么GGG是一个连通图。\n如图1为连通图，图5就不是连通图。\n 定义： \\textcolor{blue} {\\textbf { 定义： } } 定义： 最短路（Shortest Path），两节点中距离最小的路（注意这里是路，即不允许有重复节点），任意两节点间的最短路可以有多条。图中节点vs−vtv_s - v_tvs​−vt​间的最短路可表示为PstP_{st}Pst​。\n 定义： \\textcolor{blue} {\\textbf { 定义： } } 定义： 直径（Diameter），图中所有节点间最短路长度的最大值就是该图的直径。\n 中心性\n 度中心性（Degree Centrality）\n如果很多节点都连接到某一个节点，那么这个节点就被认为是重要的。简单来说，相当于如果很多人都说你厉害，那么可以认为你很厉害。表示为：\ncd(vi)=d(vi)=∑j=1NAi,jc_d(v_i) = d(v_i) = \\sum^N_{j=1}A_{i,j}\ncd​(vi​)=d(vi​)=j=1∑N​Ai,j​\n如图1中，节点v1,v5v_1,v_5v1​,v5​的度中心性都是3，节点v2,v3,v4v_2,v_3,v_4v2​,v3​,v4​的度中心性为2。\n 特征向量中心性（Eigenvector Centrality）\n与度中心性想法不同，度中心性认为每一个相邻节点的贡献是相同的，但特征向量则认为每一个节点的重要性是不同的。就好像，5个菜鸟说你厉害，那你也不一定厉害。但如果是5个大佬都说你厉害，那你肯定是很厉害。可以表示为：\nce(vi)=1λ∑j=1NAi,j ⋅ ce(vj)c_e(v_i)= \\cfrac{1}{\\lambda}\\sum^N_{j=1}A_{i,j} \\ \\cdot \\ c_e(v_j)\nce​(vi​)=λ1​j=1∑N​Ai,j​ ⋅ ce​(vj​)\n这里特征向量与特征值需要用到一些线代的知识，其实就是要找到最大的特征值并且找到其对应的特征向量。以图1及其邻接矩阵为例，在计算方面，可以借助python中的numpy库进行计算，下面代码是我改进的计算代码，可供参考：\nimport numpy as np\nA = np.array([[0,1,0,1,1],[1,0,1,0,0],[0,1,0,0,1],[1,0,0,0,1],[1,0,1,1,0]])\na, b = np.linalg.eig(A)  #调用numpy函数，a为特征值矩阵，b为特征向量矩阵\nmaxindex = a.argmax()   #找到特征值最大值的位置\nb = b[:,maxindex]      #特征向量矩阵要按列读取，索引到最大特征值所对应的特征矩阵\nmaxindex_b = b[abs(b).argmax()] #为方便阅读，做化简，找到绝对值最大值的数\nb = b * (1/maxindex_b)  #对整个矩阵化简\nprint('最大特征值：\\n&#123;&#125;'.format(a[maxindex]))\nprint('对应特征向量：\\n&#123;&#125;'.format(b))\n运行后我们可以发现最大的特征值约为2.481，对应特征向量为[1,0.675,0.675,0.806,1]。从中会发现，虽然v2,v3,v4v_2,v_3,v_4v2​,v3​,v4​的度都为2，但在特征向量中心性的计算结果中v4v_4v4​要高于另外两个节点，因为它与两个更高的邻居v1,v5v_1,v_5v1​,v5​直接相连。\n katz中心性（Katz Centrality）\n但当出现在有向无环图时，其中节点 eigenvector centrality变成0，因此Katz提出了一个改进方法，即每个节点初始就有一个centrality值，表示为：\nce(vi)=α∑j=1NAi,j ⋅ ck(vj)+βc_e(v_i)= \\alpha\\sum^N_{j=1}A_{i,j} \\ \\cdot \\ c_k(v_j) + \\beta\nce​(vi​)=αj=1∑N​Ai,j​ ⋅ ck​(vj​)+β\n其中，α\\alphaα尽量取在(0,1λmax)(0,\\cfrac{1}{\\lambda_{max}})(0,λmax​1​)节点越重要β\\betaβ的值越大，可以理解为，不仅别人说你厉害，你自己本身越厉害，值也就越高。\n 介数中心性（Betweenness Centrality）\n当多个节点间的最短路都通过某一节点，则认为这个节点很重要。其实和交通枢纽很类似，可以想成，如果很多人都通过你和别人联系上，那你就很重要。可表示为：\ncb(vi)=∑vs≠vi≠vtσst(vi)σstc_b(v_i) = \\sum_{v_s \\neq v_i \\neq v_t} \\cfrac{\\sigma_{st}(v_i)} {\\sigma_{st}}\ncb​(vi​)=vs​​=vi​​=vt​∑​σst​σst​(vi​)​\n其中σst\\sigma_{st}σst​表示从节点vsv_svs​到节点vtv_tvt​的最短路的数目，σst(vi)\\sigma_{st}(v_i)σst​(vi​)表示这些路中经过节点viv_ivi​的路的数目。如图1，节点v1v_1v1​的介数中心性为32\\cfrac{3} {2}23​，计算过程如下为，首先排除节点v1v_1v1​，然后计算其余的节点间最短路中通过节点v1v_1v1​的个数。通过观察，可以发现节点v2v_2v2​通过节点v1v_1v1​到达节点v4v_4v4​，且节点v2v_2v2​到达节点v4v_4v4​只有一条最短路，则此时σ2,4(v1)σ2,4=1\\cfrac{\\sigma_{2,4}(v_1)} {\\sigma_{2,4}} = 1σ2,4​σ2,4​(v1​)​=1,同时节点v2v_2v2​也通过节点v1v_1v1​到达节点v5v_5v5​，但节点v2v_2v2​到达节点v5v_5v5​有两条最短路\n则此时σ2,5(v1)σ2,5=12\\cfrac{\\sigma_{2,5}(v_1)} {\\sigma_{2,5}} = \\cfrac{1} {2}σ2,5​σ2,5​(v1​)​=21​，然后就没有除节点v1v_1v1​的其他节点的最短路了，那么节点v1v_1v1​的介数中心性为1+12=321+\\cfrac{1} {2} = \\cfrac{3} {2}1+21​=23​。但是不同的图这个介数中心性的值没有比较的意义，因此要对它进行一个归一化（normalization）。公式为：\ncnb=2×∑vs≠vi≠vtσst(vi)σst(N−1)(N−2)c_{nb} = \\cfrac{2 × \\sum_{v_s \\neq v_i \\neq v_t} \\cfrac{\\sigma_{st}(v_i)} {\\sigma_{st}}} {(N-1)(N-2)}\ncnb​=(N−1)(N−2)2×∑vs​​=vi​​=vt​​σst​σst​(vi​)​​\n那么可以计算节点v1v_1v1​的归一化介数中心性为14\\cfrac{1} {4}41​。\n","categories":["马耀汤继良-笔记"],"tags":["GNN","笔记","教材"]},{"title":"CS224W-01-Intro","url":"/2021/10/19/MIT-01-md/","content":" 引言\n​\t图机器学习是我博士期间研究方向，这篇博客会从头到尾详细描述我对于该领域的学习过程。理论部分我会通过学习视频课程和看教材来完成，MIT-CS224W-图机器学习这个分类是我用所看的视频课程的名字命名的，该课程是斯坦福的Jure老师在2021年冬季的课程，该课程也是最好的图机器学习启蒙课程，因此，我会对Jure老师的每一集课程都做一期复现与详解，以帮助自己和读者更好的理解这门课程。本文所有的图片以及公式将大量引用自Jure老师的PPT，在此对老师表示感谢，话不多说，开始第一次课程。\n Why Graphs\n​\t图是描述与分析包含 关联（relations）或  相互作用（interactions）的实体的通用语言。\n 网络（Network）与图（Graph）\n​\t在日常生活中，我们身边许多的数据形式都是图结构。如下图所示，从左到右依次为自然图（natural graph）、基础设施图（infrastructure）、社交网络（social network）、知识图谱（knowledge graph）等诸多领域。\n\n​\t不难看出，当前有两种大的数据类型可以表示为图，首先是网络（network），其次就是图（graph）\n​\tNetworks（Natural Graphs）：自然表示为图\n​\t\t社交网络：社会是70亿人口的集合，那么每个人之间的联系就可以构成一个社交网络。\n​\t\t通讯和交易：例如电子设备，电话通讯，金融交易等等。\n​\t\t生物医学：例如身体中基因或蛋白质之间的相互作用。\n​\t\t脑神经元链接：我们的思维就是由数以亿计的脑神经元链接并相互作用产生的。\n\n​\tGraphs：作为一种表示形式\n​\t\t信息或知识图谱：这些节点都是有组织有关联的。\n​\t\t软件：软件也可以表示为图。\n​\t\t相似性网络（similarity networks）：可以多次将数据点（datapoints）与相似的数据点相连构成相似性网络。\n​\t\t具有关联结构：例如分子间结构，场景图，3D模拟等等。\n\n 图表示学习（Graph Representation Learning）\n​\t当我们了解图后，我们就要思考**如何利用这些关联结构来进行预测呢？**开始人们依旧想使用传统机器学习的方式，但发现图机器学习与传统机器学习的区别，如下图。\n\n​\t可以看到图结构与传统的固定的图像矩阵或者文本序列不同，它是一种任意大小任意形状的拓扑结构。因此对于每一个节点都没有参照点并且可以具备不同或多种特征。因此要设计一种新的网络，当输入一张图（网络）后，即可通过设计的神经网络输出预测的节点类型，关联关系甚至是生成一张图或子图。如下图。\n\n​\t传统的机器学习更倾向于对原始数据先进行特征处理，以便机器学习模型进行使用。而在图深度学习过程中，如下图，则删掉传统的特征工程。通过表示学习（representation learning）方法自动地对图数据的特征进行提取、学习。然后应用至下流预测任务（downstream prediction task）中去。\n\n​\t表示学习最主要的方式就是图嵌入（embeding），这个在后面会深入讲解，在这里可以简单的了解为将图中的节点通过一个映射函数转换成一个d维向量。其中图中相似的节点嵌入的位置更接近，如下图。\n\n Applications of Graph ML\n​\t对于一张图，可以有多种不同层次的预测，如下图所示，对于每一个组成部分甚至是整张图，都可以作预测。\n\n​\t节点分类：预测一个节点的属性。\n​\t链接预测：预测两个节点之间是否缺少链接。\n​\t聚类：检测节点是否形成了一个社区\n​\t图分类：分类不同的图形。\n​\t图生成、图进化\n","categories":["MIT-CS224W-图机器学习"],"tags":["GNN","MIT","2019"]},{"title":"关于","url":"/about/index.html","content":"Hi！你好！我是来自天津大学的博士研究生小泽，首先十分欢迎你来到我的博客。\n","categories":[],"tags":[]}]