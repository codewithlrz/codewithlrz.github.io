[{"title":"Advances and Challenges in Conversational Recommender Systems--A Survey","url":"/2022/01/10/paper-CRS-servey/","content":"摘要推荐系统利用用户的交互历史来估计用户的偏好，当前广泛使用的是静态推荐模型，但由于静态模型没有明确的指令以及它无法从用户处获得主动反馈，因此其很难解决两个重要问题：\n​    （a）用户到底喜欢什么？\n​    （b）为什么用户会喜欢这件物品？\n最近，会话推荐系统（CRSs）的兴起改变了上述的情况，用户与系统可以通过自然语言的交互进行动态通信。\n现有的CRS仍未成熟，作者从五个方向总结出开发CRSs的关键挑战：\n​    （1）Question-based user preference elicitation（基于问题的用户偏好启发）\n​    （2）Multi-turn conversational recommendation strategies（多轮会话推荐策略）\n​    （3）Dialogue understanding and generation（对话的理解与生成）\n​    （4）Exploitation-exploration trade-offs\n​    （5）Evaluation and user simulation（评估与用户模拟）\n这些研究领域涉及到信息检索（IR）、自然语言处理（NLP）以及人机交互（HCI）等多个领域研究。\n引言静态推荐模型（传统），主要通过分析用户过去的离线行为来预测偏好。例如用户的点击历史、访问日志、对物品的评价等。早期使用的方法，如协同滤波（CF）、逻辑回归（LR）、因素分解机（FM）和梯度增强决策树（GBDT），由于其有效性以及可解释性。已经在实际应用中广泛使用。当前，已经开发出更多复杂但更有效的神经网络，如Wide&amp;Deep、神经协同过滤（NCF）、深度兴趣网络（DIN）、基于树的深度模型（TDM）以及图卷积网络（GCNs）。\n静态推荐模型的缺点静态推荐模型以用户历史行为为数据进行离线训练。然后用结果为用户提供在线服务。尽管其被广泛使用，但是依然未解决两个问题。\n用户到底喜欢什么？静态模型的学习过程是基于用户历史数据，那么这些数据是会存在噪声，同时也可能是稀疏的。为什么会这样？\n（1）静态模型的一个基本假设就是，所有的历史交互都代表了用户的偏好。\n（2）用户可能做出过错误的决定。\n（3）用户偏好会随着时间推移（之前喜欢不代表现在喜欢）。\n（4）存在新用户或者活跃度低的用户，没有历史数据的支持。\n用户为什么喜欢这件物品？在现实生活中影响用户决定的因素有很多，比如一个用户购买一件商品有可能是出于好奇或是受他人影响，也可能是深思熟虑过后才购买。不同的用户购买相同商品很可能动机不同，因此用相同标准看待不同的用户或者用相同标准看待同一用户的不同活动，是不适合推荐的。静态模型很难理清用户消费行为背后的不同原因。\n尽管人们已经做了很多努力来消除这些问题，但他们做出的假设是有限的。例如，一个常见的设置是利用大量的辅助数据（如社交网络、知识图）来更好地解释用户意图，然而这些附加的数据也可能是不完整和有噪声的。关键的困难来源于静态模型的固有的机制，其交互模式限制了用户的表达（根本没有交互），导致了用户与机器之间存在信息的不对等。\nCRSs介绍文中将会话推荐系统（CRSs）定义为： 一种能通过实时的多轮对话，探出用户的动态偏好，并根据用户当前的需求采取行动的推荐系统。\n从定义可以看到，其中一个属性是多轮交互，就是用户与系统之间以任意形式的交互，包括书面、自然语言甚至是手势等等方式。用过交互，推荐系统可以更加容易的推断出用户当前的偏好以及理解消费动机。如图一，使用户求助代理商音乐推荐。\n\n如图所示，代理商会先根据用户先前的偏好为其推荐，如果不满足用户需求就会根据用户的反馈进行更改。这就与先前的静态推荐系统不同，静态系统没有人机交互过程，而CRSs有交互过程。\nCRSs与交互推荐的联系在交互推荐的设置中，每一个推荐之后都会有一个反馈信号，表示用户是否或者有多么喜欢这个推荐。但是，交互式推荐的效率较低，由于物品种类太多，对于理解用户的意图是不明确的。因此，可以利用物品的属性信息，来缩小候选项范围。其中一种方案是基于评价的推荐系统，它旨在探究用户对某些属性的反馈而不是物品层面。举个例子，当用户购买手机时，可根据用户提供的“价格更便宜”或是“电池更耐用”等属性反馈，推荐更合适的机型。这种机制能够有助于快速缩小候选项范围。\n虽然上述 方式有效，但仍存在局限性。比如应该避免收到用户反馈就给出推荐，应当是系统觉得计算出的结果可信度高了再给出推荐。同时使用当前交互系统，用户只能通过预设的选项与系统交互，所以需要考虑设计更灵活的形式。\nCRSs与其他会话式人工智能系统的联系除了CRSs，当前市面上还有很多其他的会话式的AI系统，最常见的比如面向任务的会话系统、聊天机器人、问答系统等等。那么这些与CRSs有什么区别呢？\n\n中心任务的不同\n 虽然都是基于会话，但其他系统的会话方式只基于自然语言文本。而CRSs可以基于任何形式，如标签，按钮甚至手势等。\n\n使用场景的不同\n 在某些使用场景中会存在不同，如常见的问答系统只针对与用户的单个回合的问题进行回答，而CRSs是多回合推荐。\n\n\n本次调查的焦点文中将CRSs以一种通用框架呈现，如下图。\n\n可以看到系统由三个组件组成：\n\n用户界面\n 充当用户与机器之间的转换器，即从用户的原始表达中提取信息并转换为机器可理解的表示形式。\n\n会话策略模型\n 相当于CRSs的大脑，首先是决定CRS的核心逻辑，如激发用户偏好，保持多回合对话，引导新主题等。同时也要协调其他两个组件。\n\n推荐引擎\n 负责建模实体间的关系，学习和记录用户对项目和项目属性的偏好，检索所需信息。\n\n\n同时基于三个组件，文中也总结了五个主要挑战：\n\n基于问题的用户偏好引导\n 该挑战主要有两个问题，如何询问？如何根据用户回应调整推荐？\n\n多轮会话式推荐策略\n 在与用户交互过程中，可以让用户通过选择是继续提出要求还是直接获得推荐结果。虽然得到的问题越多结果越确定，但是系统应尽可能减少交互的回合数。\n\n自然语言的理解与生成\n CRSs应能够准确捕获用户的语义信息，同时也要生成具有可读性、流畅性的自然语言给用户。\n\nExploration and Exploitation (E&amp;E)之间的权衡\n　对于用户来说，只会与数据集中的小部分项目存在交互，甚至是对于新用户毫无交互。因此CRSs需要对用户进行积极探索。\n\n评估与用户模拟\n 由于评价CRSs需要大量的在线用户进行交互，这代价太高，因此需要开发用户模拟器。\n\n\n基于问题的用户偏好引导即使用户知道自己想要什么，也是需要进行精准的关键词检索，并且检索结果也是一堆候选项。但是推荐系统推荐的是用户可能喜欢的物品，这与用户所熟悉的物品是不同的。传统的推荐系统只能利用历史记录作为输入，从而受限制。但CRSs系统可以通过实时交互，通过用户反馈的某些属性判断此时用户的需求与态度，并作出及时调整。\n问题驱动的方法关注于在对话中应该问什么的问题。一般有两种方法：（1）询问物品（2）询问物品的属性/主题/类别。\n询问物品文中介绍三种方法，可以引导出用户对物品的态度，从而快速调整。\n基于选择的方法其主要思想就是经常让用户从给定的选项中选择喜欢的项目或项目集，常见策略包括：\n\n从两个给定选项中选择一个项目\n从给定项目的列表中选择一个项目\n从两个给定的列表中先择一组项目\n\n在用户选择项目后，系统会根据反馈更改推荐。在选择过程中也应尽可能确保两个候选集是不同的或是可区分的。\n例如： Loepp等人 使用矩阵分解（MF）来初始化用户和商品的嵌入，然后从商品嵌入空间中选择两组商品作为候选集，然后让用户选择这两组商品之一 。 重要的是要确保两个候选集尽可能不同或可区分。作者采用了MF算法，并按解释的方差递减的顺序逐一获得了嵌入向量。因此，这些因素，即嵌入向量的不同维数，是按独特性排序的。然后，作者迭代选择只有一个因子值变化的两个项目集。例如，如果两个因素分别代表电影的幽默度和动作度，则两个候选集是具有较高幽默度的电影集和具有较低幽默度的另一组电影，而具有较高幽默度的电影集两套固定在平均水平。当用户选择一个商品集时，用户的偏好嵌入向量将设置为所选商品的嵌入向量的平均值。随着交互过程的继续，选择变得更加困难。用户可以选择忽略该问题，这意味着用户无法区分两个物品集之间的区别，或者他们不在乎。\n贝叶斯偏好引导方法可定义函数$u(x_j,u_i)$代表用户 i 对商品 j 的偏好。在通常情况下，$u(x_j,u_i) = x_j^Tu_i$。\n在贝叶斯条件下，用户 i 的偏好将有之前的向量形式替换为概率分布形式，可先计算用户先验概率$P(U ^ { (i)} )$，然后单个物品 j 对于用户 i 的期望可计算为：\n\n那么其中的最大期望就可作为推荐项，即 $argmax_j\\mathbb {E} [u(x_j,u_i)]$ 。\n系统可选择一些要询问的物品，同时对于用户的状态认知分布（belief distribution）可通过用户的反馈进行更新。若给定一个用户对于问题 $q$ 的回复 $r_i$ ,用户的后验概率 $P(u_i|q,r_i)$ 可写为：\n\n同时，询问策略也有两类：（1）两两比较，用户在两个项目（集）中选择更喜欢的，（2）用户从多个选项中选择。\n交互式推荐交互式推荐模型主要是基于强化学习，首先是可以使用MAB算法，其优势是：\n\n可以自然的支持会话场景\n可以利用用户以前喜欢的物品，并探索用户可能喜欢但从未尝试过的物品。\n\n还有人提出元学习的方法，这些都会在后续具体展开来讲。\n由于候选项实在太大，如果只询问物品本身效率会很低，同时用户也会随着交互次数过多而感到无聊，所以以属性为中心的问题更实际，因此利用用户对属性的偏好成了一个关键的研究问题。\n询问属性可以通过属性而更有效的排除候选项，其挑战在于如何在不确定用户需求的情况下，系统该询问哪一系列的属性。下面是一些主流方法。\n历史交互拟合模式一个会话可以看作一个实体的序列，包括已经消费的物品和所提及的属性。其目标是为了学习预测下一个要询问的属性或是下一个要推荐的物品。因此为捕获用户行为模式的长期和短期的依赖，可以使用时序神经网络（RNN），如GRU、LSTM。\n其中一个范例就是Christakopoulou等人提出的Q&amp;R模型，它首先让用户在给定的列表中选择一个或多个主题。这个模型包含一个触发机制，这个机制可以是随机的，也可以是制定一些标准获取用户状态，甚至可以是用户自己发起。当用户在第 $t$ 次选择时，那么用户是否选择下一个主题 $q$ 的概率就基于他之前的观看历史 $e_1,\\dots,e_T$ ，即 $P(q\\ |\\ e_1,\\dots,e_T)$ 。当选择主题 $q$ 后，系统对推荐物品 $r$ 的条件概率就是 $P(r\\ |\\ e_1,\\dots,e_T,q)$ ，这种算法可以用在新用户上。Zhang等人还提出通过将用户的评论转为潜在向量，但这种方式时有局限性的，因为有的评论是负面的。\n系统生成的问题是使用预定义的语言模板，因此系统只是关注用户选择的方向与价值，毕竟其核心任务是推荐不是语言生成。\n上述方法有一个共同缺点，就是从用户的历史行为中学习是不能理解交互背后的逻辑的，这些模型只是在试图适合历史偏好，没有考虑过一旦用户拒绝了该怎么办。\n减少不确定性一些研究试图简历一个简单的逻辑来缩小候选项的范围。\n基于评论的方法上述的评论模型具备一种启发式策略，可以引导出用户对属性的偏好，那么基于这一点可以利用用户所评论的属性来移除不满足的属性，进而重构候选集。可以将用户的评论转为潜在向量 $\\vec {Z}  { i,j} $，那么这个向量可用于生成评分$\\vec {r}  { i,j} $，以及解释的属性$\\vec {S} _ { i,j} $，一旦用户不喜欢那个属性，就直接把属性向量中的属性归零，然后更新所有向量。\n强化学习驱动方法强化学习可用于CRSs来选择适当的属性来询问，系统不仅可以选择属性，还可以选择何时改变当前对话的主题。\n受图约束的候选项图是一种表示不同实体之间关系的普遍结构。利用图来筛选给定的一组属性的项是很自然的。例如，Lei等人在一个异构图上提出了一种交互式路径推理算法，它将用户、项目和属性表示为节点，一个边连接的两个节点表示两个节点之间的关系，例如，用户购买了一个项目，或一个项目对一个属性有一定的值。图的优势就是可以将一段对话转换成一张图上的路径，如下图二：\n\n其他方法Zou等人提出了一种基于扩展矩阵分解模型的问题驱动推荐系统，它只考虑用户评级数据，以结合用户的实时反馈。\n基本的假设是，如果用户喜欢一个物品，那么他一定喜欢这个物品的全部属性。基于这一点，如果只需要询问用户是否喜欢系统最不确定的属性就好，因为喜欢物品所共有的一些属性是一定喜欢的。\n总结上述方法大多存在一些缺陷，系统会不断的提出问题，每个问题都会给出一个推荐，只有当用户满意了或者自己退出了才会终止，这就好像在审问用户一样。同时，在询问过程中，如果系统还没有理清用户的偏好时，也不应该将认可度低的推荐给用户。\n总之，需要一个多轮会话策略来控制如何在询问和推荐之间切换，这种策略应该在交互过程中动态变化。\nCRSs的多轮会话策略问题驱动的方法关注于“该问什么”,这一节则侧重于“何时问”或是“如何保持会话”，一个好的策略不仅可以在适当的时间（高度自信）提出推荐，灵活地适应用户的反馈，还可以保持对话话题，适应不同的场景，让用户在互动中感到舒适。\n决定何时询问和推荐的会话策略好的策略可以是一个基于规则的策略，比如说可以是每提出 k 个问题就给出一次推荐。也可以是随机策略或者是基于模型的策略。\n在SAUR模型中，可以设置一个触发器来激活推荐模块，当系统认为可信度较高了就自动触发推荐，否则将继续询问。但是这种方式过于简单，它不能捕获丰富的语义信息，例如现在讨论的是什么话题，这个话题被讨论到什么程度了。因此，引入了强化学习（RL）。例如，Sun和Zhang[187]提出了一个被称为会话推荐模型(CRM)的模型，该模型使用了面向任务的对话系统的架构。在CRM模型中，用一个置信追踪器去追踪用户的输入，然后输出一个潜在向量，表示对话的当前状态和迄今为止已捕获的用户首选项。然后将状态向量输入到一个策略网络中，用以决定是推荐一个项目还是继续提问。比如说有 k+1 步动作，那么前 k 步是对某一方面的询问，第 k+1步就是给出推荐。\n在CRM问题中的潜在向量是捕获表面信息的，也不太适应动态环境。因此 Lei 等人提出 EAR 框架，让机器在正确的时间提出问题，在他们的定义中，正确的时间是（1）候选集足够小，（2）从信息的获取或是用户的耐心上来讲，所提出的额外问题意义不大了，（3）推荐引擎认为当前可信度会被用户接受。EAR 工作流程如下（ 图三）：\n\n系统必须根据得到的信息决定是否继续询问关于属性的问题，还是给出推荐。为了确定何时提出问题，他们构建了RL模型的状态，并考虑四个因素：\n\n当前候选项属性中每个属性的熵信息。询问熵大的属性有助于减少候选空间，从而有利于以更少的回合找到想要的项目。\n用户对每个属性的偏好。预测偏好高的属性很可能得到正反馈，这也有助于减少候选空间。\n历史用户反馈。如果系统已经询问了用户批准的一些属性，那么这可能是推荐的好时机。\n剩余候选商品的数量。 如果候选商品名单足够短，系统应该转向推荐，避免浪费更多的轮次。\n\n同时，不要把用户未选择的作为负样本，因为用户可能只是忽略了或是只是想尝试些新东西。\n此外，Lei 等人还通过整合用户、物品、属性的知识图谱，提出 CPR 模型来扩展 EAR 模型。原 EAR 模型中属性的选择都是不规则与不可预测的，CPR 在选择属性与推荐的过程中，都是严格遵循知识图谱上的路径，因此可以解释，同时知识的整合提高了推理能力，模型效率更高。\n更广泛角度的会话策略大多数的CRS模型都是假设用户知道他们想要什么，而实际上，求助推荐的用户很可能并不知道他们想要什么，因此CRS不仅应该提出问题询问客户，也要承担起引导主题的责任。\n会话中的多主题学习Liu 等人借鉴主动会话的思想，提出一个新的任务，它将会话推荐置于多主题对话的环境中，在这个模型中，系统可以主动的、自然的引导对话，并且可以在多个主题间切换。为了解决这一任务，他们提出了一个多目标驱动的会话生成（MGCG）框架。该框架由一个目标规划模块和一个目标导向的响应模块组成。目标规划模块可以进行对话管理来控制对话流程，对话流程以推荐为主要目标，完成自然主题转换为短期目标。\n同时文中也指出了一些人工构建的数据集。\n特殊能力:建议、谈判和说服除了智能交互之外，还需要 CRS 有着在不同场景下的反应能力。比如 Rosset 等人建议可以推荐出用户下一步可能想问的问题。比如说如果用户查询“日产GTR价格”，那么系统就可以提供一些推荐例如“租赁日产GT-R需要多少钱？”、“日产GT-R的优点和缺点是什么？”亦或者是一些有趣的推荐例如“日产GT-R是最终的有轨电车吗？”等等，这样不仅丰富用户体验，还会获取更多用户偏好。\n总结会话策略的主要焦点是通过提问来确定何时引导用户的偏好，以及何时给出推荐。作为推荐只有在系统认为可信度高的时候才应该给出，同时自适应策略比静态策略更有前途。除了这个核心功能外，文中还从更广泛的角度介绍了一些策略。\n","categories":["论文解读"],"tags":["论文","CRS"]},{"title":"图表示学习基础","url":"/2021/12/09/book-GRL-01/","content":" 引言（Introduction）\n 什么是图（What Is  A Graph）\n图的定义以及一些性质可参考图论基础。\n 多关系图（Multi-relational Graphs）\n一个图的类型可以根据其边的性质进行划分，如有向图，无向图，加权图等，那么图中含有不同类型的边的图，就可以是多关系图。\n举个例子，在吃药物的过程中，不同种的药物之间会有不同的反应，因此每一条边的类型都会有很大差异，如图1。\n\n那么，就要扩展一种新的边表示方法，添加边的类型τ\\tauτ，例如在简单图中表示一条边为(u,v)(u,v)(u,v),其中u,vu,vu,v为图中的节点。那么表示多关系图的边就可以为(u,τ,v)∈E\\boldsymbol{ (u,\\tau,v) \\in E}(u,τ,v)∈E。\n同时邻接矩阵也要进行改变，从原来的二维变为三维A∈R∣V∣×∣R∣×∣V∣\\boldsymbol{ A \\in \\mathbb{R}^{ |V| \\times |R| \\times |V|}}A∈R∣V∣×∣R∣×∣V∣，其中RRR是一组关系的集合。\n在多关系图中也有两个很重要的子集，异质图（Heterogeneous graph）和多维图（Multiplex graph）\n 异质图（Heterogeneous graph）\n在异质图中，可以将节点划分为多种类型，每一个节点都对应着一种类型。\n表示为 构图中的每一条边都可能会被特定的节点所约束，换句话说，特定的边只会连接特定的节点。举一个例子，假设在一张医学类型的图上有三类节点，分别是蛋白质，药物和疾病。那么在其中代表“治疗”的边只会出现在药物和疾病两类节点之间，类似的代表“多药副作用”的边只会出现在两个药物节点之间。\n有一种特殊的异质图–多部图（Multipartite graphs），其边只能连接不同类型的节点。\n表示为(u,τi,v)∈E→u∈Vj,v∈Vk∧j≠k(u,\\tau_i,v)\\in E \\rightarrow u \\in V_j,v \\in V_k \\wedge j \\neq k(u,τi​,v)∈E→u∈Vj​,v∈Vk​∧j​=k。\n图2为普通异质图与多部图的表现形式。\n\n 多重图（Multiplex graphs）\n在多重图中，可以想象将节点分为kkk层，每一层节点都是一样的，只是每一层节点间的连接方式不同。同时，不同层内的相同节点可以通过层间边相连。其节点集合与边集合与边集合分别表示为：\nV=V1∪V2∪⋯∪VkV = V_1 \\cup V_2 \\cup \\cdots \\cup V_kV=V1​∪V2​∪⋯∪Vk​\nE=E1∪E2∪⋯∪Ek∪Ek+1E = E_1 \\cup E_2 \\cup \\cdots \\cup E_k \\cup E_{ k+1}E=E1​∪E2​∪⋯∪Ek​∪Ek+1​，Ek+1E_{ k+1}Ek+1​为层间边集合。\n举一个例子，如图3所示。\n\n假设每一层中的每一个节点为一个城市，连接的边代表各种交通工具连接两个城市，层间边代表同一座城市可以进行换乘。现在如果在L1层中想从节点v1到达节点v9，由于L1中无相应路径，即可先到达节点v5，通过层间边到达L2层，从而到达v9。\n 特征信息（Feature Information）\n在多数情况下都会有与图对应的特征信息，多数情况下都是节点级别的，所以用一个实数矩阵（real-valued matrix） X∈R∣V∣×m\\textbf{ X} \\in \\mathbb{ R}^{ |V| \\times m} X∈R∣V∣×m表示。\n 图机器学习（Machine learning on graphs）\n图机器学习没有像传统机器学习对监督学习（给定分类标签）与无监督学习（聚类）有明确的界限划分，大多数情况更偏向于\n半监督学习方式。\n 节点分类（Node classification）\n在通常情况下，由于只有少部分节点存在标签信息，但是在训练的过程中仍然会使用无标签的数据，这与监督学习的只使用有标签数据不同，因此节点分类任务通常被归为是半监督学习。同时，节点不满足独立同分布(i.i.d.)。\n当前对于节点分类主要时利用节点之间的连接，可利用其同质性与结构等价性。\n同质性：认为相邻节点属性相似，即节点与邻居节点有共享属性趋势。\n结构等价性：认为具有相似布局结构的节点将具有相似标签。\n异质性：认为节点优先连接与自身不同类型节点（如社交网络中的性别属性）。\n 关系预测（Relation prediction）\n关系预测在不同领域中有有不同的名字，链接预测（Link Prediction）、图谱补全（Graph Completion）、和关系推断（Relational Inference）。与节点分类相似，节点分类是预测节点，关系预测是预测边，即在边的层次研究。方式也是半监督学习，给定一组节点VVV以及部分边的集合Etrain⊂EE_{ train} \\subset EEtrain​⊂E，利用这些信息推断缺失边。\n 聚类和社区发现（Clustering and community detection）\n希望将图表现出一种社区结构，节点能够更有可能与同一社区的节点形成边。\n如下图所示，根据网络中不同节点之间连接的紧密程度，我们可以将网络视为是由不同的“簇”所组成，其中“簇”内的节点之间连接更加的紧密，不同“簇”之间的的节点之间的链接比较稀疏。我们将这种“簇”称为是网络中的社区结构。odes\n\n社区发现的定义：通过输入一张图G=(V,E)G = (V,E)G=(V,E)推断出潜在的社区结构。可以看到是在子图层次进行研究。\n 图的分类、回归与聚类（Graph classifification, regression, and clustering）\n当研究完节点、边、子图后，最后的是整张图级别的研究。\n在图分类或图回归任务中，其数据集由多张不同的图构成，利用图机器学习算法对每张图进行独立预测（不是每张图的组成部分）。\n在图聚类任务中，目标是学习一个无监督的测量图与图之间的相似性策略。\n 背景与传统方法（Background and Traditional Approaches）\n(21条消息) Bag-of-words 词袋模型基本原理_Jaster_wisdom的专栏-CSDN博客_词袋模型的原理\n","categories":["图表示学习笔记"],"tags":["GNN","笔记","图表示学习"]},{"title":"图论基础","url":"/2021/11/24/book-mayao-02-md/","content":" 图的表示\n 图（Graph）\n 定义： \\textcolor{blue} {\\textbf { 定义： } } 定义： 一个图可以表示为G={V,ε}G = { \\{V,\\varepsilon \\}}G={V,ε}，其中VVV = {v1v_1v1​,⋯\\cdots⋯,vNv_NvN​}是大小为N=∣V∣N = | V |N=∣V∣ 的节点集合，ε\\varepsilonε={ e1e_1e1​,⋯\\cdots⋯,eMe_MeM​}是大小为MMM的边的集合。\n\n如上图（图1），为一个有五个节点和六条边的图。其中{ v1v_1v1​,⋯\\cdots⋯,v5v_5v5​}为五个节点，{ e1e_1e1​,⋯\\cdots⋯,e6e_6e6​}为六条边。\n在图中，如果有一条边eie_iei​连接两个节点vei1v_{e_i}^1vei​1​和vei2v_{e_i}^2vei​2​，那么这条边可以表示为（vei1v_{e_i}^1vei​1​,vei2v_{e_i}^2vei​2​），在有向图中表示边从起点vei1v_{e_i}^1vei​1​指向终点vei2v_{e_i}^2vei​2​。相反在无向图中由于没有顺序之分，则eie_iei​ = （vei1v_{e_i}^1vei​1​,vei2v_{e_i}^2vei​2​）= （vei2v_{e_i}^2vei​2​,vei1v_{e_i}^1vei​1​）。举个例子，如图一中的边e6e_6e6​连接节点v1v_1v1​与节点v5v_5v5​，那么由于图1为无向图，则e6e_6e6​也可以表示为（v1v_1v1​,v5v_5v5​）或（v5v_5v5​,v1v_1v1​）。\n 邻接矩阵（Adjacency Matrix）\n为了方便查看节点之间的连接关系，图G={V,ε}G = { \\{V,\\varepsilon \\}}G={V,ε}可以等价的表示为邻接矩阵的形式，更加直观的描述节点之间的关系。\n 定义： \\textcolor{blue} {\\textbf { 定义： } } 定义： 给定一个图G={V,ε}G = { \\{V,\\varepsilon \\}}G={V,ε},对应的邻接矩阵可以表示为A∈{0,1}N×NA∈{ \\{ 0,1 \\} ^ { N×N } }A∈{0,1}N×N。邻接矩阵AAA的第iii行第jjj列元素Ai,jA_{i,j}Ai,j​表示节点viv_ivi​和vjv_jvj​的连接关系。具体来讲，如果viv_ivi​与vjv_jvj​相邻，则Ai,j=1A_{i,j} = 1Ai,j​=1，否则Ai,j=0A_{i,j} = 0Ai,j​=0。\n在无向图中，由于不受节点顺序影响，则Ai,j=Aj,iA_{ i,j } = A_{ j,i }Ai,j​=Aj,i​，所以无向图的邻接矩阵一定是 关于主对角线对称 \\textbf { 关于主对角线对称 } 关于主对角线对称 的。以图1为例，该图的邻接矩阵可表示为：\nA=(0101110100010011000110110)A = \n\\begin {pmatrix}\n0 &amp; 1 &amp; 0 &amp; 1 &amp; 1 \\\\\n1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\\n0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 \\\\\n1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\\n1 &amp; 0 &amp; 1 &amp; 1 &amp; 0 \\\\\n\\end {pmatrix}\nA=⎝⎜⎜⎜⎜⎛​01011​10100​01001​10001​10110​⎠⎟⎟⎟⎟⎞​\n可以看到，主对角线值均为0，且都关于其对称。\n 图的性质\n 度（Degree）\n 定义： \\textcolor{blue} {\\textbf { 定义： } } 定义： 在图G={V,E}G = { \\{V,E \\}}G={V,E}中，节点vi∈Vv_i ∈ Vvi​∈V的度定义为图GGG中与节点viv_ivi​相关联的边的数目。\nd(vi)=∑vj∈V1ε({vi,vj})d(v_i) = \\sum_{ v_j ∈ V}1\\varepsilon( \\{ v_i,v_j \\} )\nd(vi​)=vj​∈V∑​1ε({vi​,vj​})\n其中，1ε(⋅)1\\varepsilon(\\cdot)1ε(⋅)为指示函数，简单来说就是满足条件就取1，不满足就取0。对于图来讲可表示为：\n1ε({vi,vj})={1,(vi,vj)∈ε0,(vi,vj)∉ε1\\varepsilon( \\{ v_i,v_j \\} ) = \n\\begin {cases}\n1, \\quad (v_i,v_j)∈\\varepsilon \\\\\n0, \\quad (v_i,v_j)\\notin\\varepsilon\n\\end {cases}\n1ε({vi​,vj​})={1,(vi​,vj​)∈ε0,(vi​,vj​)∈/​ε​\n即当存在(vi,vj)(v_i,v_j)(vi​,vj​)这条边时，d(vi)d(v_i)d(vi​)就加1，反之如果不存在，就加0。其实最直观的看法就是找从这个节点出发引出几条边。但这种方法会有一些麻烦，所以如果能够画出图的邻接矩阵的话，可以更简单的计算结果，利用邻接矩阵可以将节点的度表示为：\nd(vi)=∑j=1NAi,jd(v_i) = \\sum_{ j = 1 }^N A_{ i,j }\nd(vi​)=j=1∑N​Ai,j​\n简单来说就是计算该节点所在行所有值的和。任以图一为例，节点v5v_5v5​与v1,v3,v4v_1,v_3,v_4v1​,v3​,v4​相连，则节点v5v_5v5​的度为3。如果利用邻接矩阵计算，v5v_5v5​占矩阵第五行，直接将改行所有元素相加，即可得到其度为3。\n 邻域（Neighborhood）\n 定义： \\textcolor{blue} {\\textbf { 定义： } } 定义： 在图G={V,ε}G=\\{ V,\\varepsilon \\}G={V,ε}中，节点viv_ivi​的邻域N(vi)N(v_i)N(vi​)是所有和它相邻的节点的集合。\n依然以图1中v5v_5v5​节点为例，其邻域为N(v5)={v1,v3,v4}N(v_5) = \\{ v_1,v_3,v_4 \\}N(v5​)={v1​,v3​,v4​}，可以发现对于任意节点viv_ivi​,其 邻域中元素个数等于该节点的度 \\textbf{ 邻域中元素个数等于该节点的度 } 邻域中元素个数等于该节点的度 ，即d(vi)=∣N(vi)∣d(v_i) = |N(v_i)|d(vi​)=∣N(vi​)∣。\n 定理： \\textcolor{green} {\\textbf { 定理： } } 定理： 一个图G={V,ε}G=\\{ V,\\varepsilon \\}G={V,ε}中所有的节点的度之和是图中边数量的两倍：\n∑vi∈Vd(vi)=2 ⋅ ∣ε∣\\sum_{ v_i ∈ V}d(v_i) = 2\\ \\cdot \\ | \\varepsilon |\nvi​∈V∑​d(vi​)=2 ⋅ ∣ε∣\n这个定理很容易想通，举一个例子，如下图（图2），v1与v2v_1与v_2v1​与v2​的度都为1，度之和为2。但两个节点公用一条边e1e_1e1​，则是2倍的关系。\n\n同样的，如果将图写为邻接矩阵的话，每一个非零元素则代表两个节点存在连接，第iii行中非零元素的个数就是节点viv_ivi​的度，那么矩阵中所有非零元素的个数就是所有节点的度之和，那么可以有如下推论。\n 推论： \\textcolor{green} {\\textbf { 推论： } } 推论： 无向图邻接矩阵的非零元素的个数是边的数量的两倍。\n 连通度（Connectivity）\n 定义： \\textcolor{blue} {\\textbf { 定义： } } 定义： 途径（Walk）图的途径是节点和边的交替序列，从一个节点开始，以一个节点结束，其中每条边与紧邻的节点相关联。\n 定义： \\textcolor{blue} {\\textbf { 定义： } } 定义： 迹（Trail）是边各不同的途径。\n 定义： \\textcolor{blue} {\\textbf { 定义： } } 定义： 路（Path）是节点各不相同的途径，也称路径\n简而言之，只要不跳着走，那从一个节点到另一个节点就是一个途径，在途径这个大类中，如果序列中没有重复边就是迹，没有重复节点就是路。举个例子，图1中想从v1v_1v1​走到v2v_2v2​，假设绕远走，那么可以选择(v1,e4,v4,e5,v5,e6,v1,e1,v2)(v_1,e_4,v_4,e_5,v_5,e_6,v_1,e_1,v_2)(v1​,e4​,v4​,e5​,v5​,e6​,v1​,e1​,v2​)这条途径，那么这条途径也是一条迹但不是路，因为这条途径中没有重复的边，但存在重复的节点v1v_1v1​。如果选(v1,e1,v2)(v_1,e_1,v_2)(v1​,e1​,v2​)这条途径，那么这即可以成为是迹，也可以称为是路径。\n 定理： \\textcolor{green} {\\textbf { 定理： } } 定理： 对于图G={ε,V}G = { \\{ \\varepsilon,V \\} }G={ε,V}及其邻接矩阵AAA，用AnA^nAn表示该邻接矩阵的n次幂。那么AnA^nAn的第iii行第jjj列的元素等于长度为nnn的vi−vjv_i - v_jvi​−vj​的途径的个数。\n这条定理看似很复杂，但功能很简单，举例如图1，假设从v1−v4v_1-v_4v1​−v4​，如果有长度为6的途径的走法，问一共有几种。那么答案就是A1,46A^6_{ 1,4}A1,46​。\n书中的证明方法用的是数学归纳法，那么我们可以用线性代数的方式以及图论的方式来解释一下书中的证明过程。首先假设当n=kn = kn=k时，Ai,jkA^k_{i,j}Ai,jk​等于长度为kkk的vi−vjv_i-v_jvi​−vj​的途径的数量。那么我们只需要证明：\nAi,jk+1=∑h=1N Ai,hk ⋅ Ah,jA^{k+1}_{i,j} = \\sum^N_{h=1} \\ A^k_{i,h}\\  \\cdot \\ A_{h,j}\nAi,jk+1​=h=1∑N​ Ai,hk​ ⋅ Ah,j​\n对于式中的h代表的是节点，那么上面这个公式就是一个基础的线性代数的计算公式，那么如何用图论来解释呢？\n\n如上图（图3），假定要从v1v_1v1​分6步走v4v_4v4​，按照公式，第六步的方式要与前五步的走法相关，那么我们空出第6步，先只看前5步：\n\n如上图（图4），左侧部分为前5步从v1v_1v1​走到各个点的方式，也就是对应的Ai,hkA^k_{i,h}Ai,hk​，在这里就是A1,h5(h=1⋯4)A^5_{1,h} (h = 1 \\cdots4)A1,h5​(h=1⋯4)。那么最后一步就是从这四个点到v4v_4v4​的走法，分别对应的就是Ah,4A_{h,4}Ah,4​,将每一个节点的结果都加和，就是最终的结果，这个证明就同时结合了线代和图论的知识。\n 定义： \\textcolor{blue} {\\textbf { 定义： } } 定义： 子图（Subgraph），图G={V,ε}G=\\{ V,\\varepsilon \\}G={V,ε}的子图G′={V′,ε′}G&#x27;=\\{ V&#x27;,\\varepsilon&#x27; \\}G′={V′,ε′}由节点集合的子集V′⊂VV&#x27; \\subset VV′⊂V和边集的子集ε’⊂ε\\varepsilon ’ \\subset \\varepsilonε’⊂ε组成，此外，集合V′V&#x27;V′必须包含集合ε′\\varepsilon&#x27;ε′涉及的所有节点。\n举个例子，如图3中节点子集{v2,v3,v4}\\{ v_2,v_3,v_4 \\}{v2​,v3​,v4​}和边子集{e2,e3,e4}\\{ e_2,e_3,e_4 \\}{e2​,e3​,e4​}就构成原图的一个子图。\n 定义： \\textcolor{blue} {\\textbf { 定义： } } 定义： 连通分量（Connected Component），给定一个图G={V,ε}G = \\{ V,\\varepsilon \\}G={V,ε}，如果一个子图G′={V′,ε′}G&#x27;=\\{ V&#x27;,\\varepsilon&#x27; \\}G′={V′,ε′}中任意一对节点之间都至少存在一条路，且V′V&#x27;V′中的节点不与任何V/V′V/V&#x27;V/V′中的节点相连，那么G′G&#x27;G′就是一个连通分量。\n如下图（图5），就是一个含有两个连通分量的图，因为其左右两边没有连接。\n\n 定义： \\textcolor{blue} {\\textbf { 定义： } } 定义： 连通图（Connected Graph），如果一个图G={V,ε}G = \\{ V,\\varepsilon \\}G={V,ε}只有一个连通分量，那么GGG是一个连通图。\n如图1为连通图，图5就不是连通图。\n 定义： \\textcolor{blue} {\\textbf { 定义： } } 定义： 最短路（Shortest Path），两节点中距离最小的路（注意这里是路，即不允许有重复节点），任意两节点间的最短路可以有多条。图中节点vs−vtv_s - v_tvs​−vt​间的最短路可表示为PstP_{st}Pst​。\n 定义： \\textcolor{blue} {\\textbf { 定义： } } 定义： 直径（Diameter），图中所有节点间最短路长度的最大值就是该图的直径。\n 中心性\n 度中心性（Degree Centrality）\n如果很多节点都连接到某一个节点，那么这个节点就被认为是重要的。简单来说，相当于如果很多人都说你厉害，那么可以认为你很厉害。表示为：\ncd(vi)=d(vi)=∑j=1NAi,jc_d(v_i) = d(v_i) = \\sum^N_{j=1}A_{i,j}\ncd​(vi​)=d(vi​)=j=1∑N​Ai,j​\n如图1中，节点v1,v5v_1,v_5v1​,v5​的度中心性都是3，节点v2,v3,v4v_2,v_3,v_4v2​,v3​,v4​的度中心性为2。\n 特征向量中心性（Eigenvector Centrality）\n与度中心性想法不同，度中心性认为每一个相邻节点的贡献是相同的，但特征向量则认为每一个节点的重要性是不同的。就好像，5个菜鸟说你厉害，那你也不一定厉害。但如果是5个大佬都说你厉害，那你肯定是很厉害。可以表示为：\nce(vi)=1λ∑j=1NAi,j ⋅ ce(vj)c_e(v_i)= \\cfrac{1}{\\lambda}\\sum^N_{j=1}A_{i,j} \\ \\cdot \\ c_e(v_j)\nce​(vi​)=λ1​j=1∑N​Ai,j​ ⋅ ce​(vj​)\n这里特征向量与特征值需要用到一些线代的知识，其实就是要找到最大的特征值并且找到其对应的特征向量。以图1及其邻接矩阵为例，在计算方面，可以借助python中的numpy库进行计算，下面代码是我改进的计算代码，可供参考：\nimport numpy as np\nA = np.array([[0,1,0,1,1],[1,0,1,0,0],[0,1,0,0,1],[1,0,0,0,1],[1,0,1,1,0]])\na, b = np.linalg.eig(A)  #调用numpy函数，a为特征值矩阵，b为特征向量矩阵\nmaxindex = a.argmax()   #找到特征值最大值的位置\nb = b[:,maxindex]      #特征向量矩阵要按列读取，索引到最大特征值所对应的特征矩阵\nmaxindex_b = b[abs(b).argmax()] #为方便阅读，做化简，找到绝对值最大值的数\nb = b * (1/maxindex_b)  #对整个矩阵化简\nprint('最大特征值：\\n&#123;&#125;'.format(a[maxindex]))\nprint('对应特征向量：\\n&#123;&#125;'.format(b))\n运行后我们可以发现最大的特征值约为2.481，对应特征向量为[1,0.675,0.675,0.806,1]。从中会发现，虽然v2,v3,v4v_2,v_3,v_4v2​,v3​,v4​的度都为2，但在特征向量中心性的计算结果中v4v_4v4​要高于另外两个节点，因为它与两个更高的邻居v1,v5v_1,v_5v1​,v5​直接相连。\n katz中心性（Katz Centrality）\n但当出现在有向无环图时，其中节点 eigenvector centrality变成0，因此Katz提出了一个改进方法，即每个节点初始就有一个centrality值，表示为：\nce(vi)=α∑j=1NAi,j ⋅ ck(vj)+βc_e(v_i)= \\alpha\\sum^N_{j=1}A_{i,j} \\ \\cdot \\ c_k(v_j) + \\beta\nce​(vi​)=αj=1∑N​Ai,j​ ⋅ ck​(vj​)+β\n其中，α\\alphaα尽量取在(0,1λmax)(0,\\cfrac{1}{\\lambda_{max}})(0,λmax​1​)节点越重要β\\betaβ的值越大，可以理解为，不仅别人说你厉害，你自己本身越厉害，值也就越高。\n 介数中心性（Betweenness Centrality）\n当多个节点间的最短路都通过某一节点，则认为这个节点很重要。其实和交通枢纽很类似，可以想成，如果很多人都通过你和别人联系上，那你就很重要。可表示为：\ncb(vi)=∑vs≠vi≠vtσst(vi)σstc_b(v_i) = \\sum_{v_s \\neq v_i \\neq v_t} \\cfrac{\\sigma_{st}(v_i)} {\\sigma_{st}}\ncb​(vi​)=vs​​=vi​​=vt​∑​σst​σst​(vi​)​\n其中σst\\sigma_{st}σst​表示从节点vsv_svs​到节点vtv_tvt​的最短路的数目，σst(vi)\\sigma_{st}(v_i)σst​(vi​)表示这些路中经过节点viv_ivi​的路的数目。如图1，节点v1v_1v1​的介数中心性为32\\cfrac{3} {2}23​，计算过程如下为，首先排除节点v1v_1v1​，然后计算其余的节点间最短路中通过节点v1v_1v1​的个数。通过观察，可以发现节点v2v_2v2​通过节点v1v_1v1​到达节点v4v_4v4​，且节点v2v_2v2​到达节点v4v_4v4​只有一条最短路，则此时σ2,4(v1)σ2,4=1\\cfrac{\\sigma_{2,4}(v_1)} {\\sigma_{2,4}} = 1σ2,4​σ2,4​(v1​)​=1,同时节点v2v_2v2​也通过节点v1v_1v1​到达节点v5v_5v5​，但节点v2v_2v2​到达节点v5v_5v5​有两条最短路\n则此时σ2,5(v1)σ2,5=12\\cfrac{\\sigma_{2,5}(v_1)} {\\sigma_{2,5}} = \\cfrac{1} {2}σ2,5​σ2,5​(v1​)​=21​，然后就没有除节点v1v_1v1​的其他节点的最短路了，那么节点v1v_1v1​的介数中心性为1+12=321+\\cfrac{1} {2} = \\cfrac{3} {2}1+21​=23​。但是不同的图这个介数中心性的值没有比较的意义，因此要对它进行一个归一化（normalization）。公式为：\ncnb=2×∑vs≠vi≠vtσst(vi)σst(N−1)(N−2)c_{nb} = \\cfrac{2 × \\sum_{v_s \\neq v_i \\neq v_t} \\cfrac{\\sigma_{st}(v_i)} {\\sigma_{st}}} {(N-1)(N-2)}\ncnb​=(N−1)(N−2)2×∑vs​​=vi​​=vt​​σst​σst​(vi​)​​\n那么可以计算节点v1v_1v1​的归一化介数中心性为14\\cfrac{1} {4}41​。\n","categories":["马耀汤继良-笔记"],"tags":["GNN","笔记","教材"]},{"title":"CS224W-01-Intro","url":"/2021/10/19/MIT-01-md/","content":" 引言\n​\t图机器学习是我博士期间研究方向，这篇博客会从头到尾详细描述我对于该领域的学习过程。理论部分我会通过学习视频课程和看教材来完成，MIT-CS224W-图机器学习这个分类是我用所看的视频课程的名字命名的，该课程是斯坦福的Jure老师在2021年冬季的课程，该课程也是最好的图机器学习启蒙课程，因此，我会对Jure老师的每一集课程都做一期复现与详解，以帮助自己和读者更好的理解这门课程。本文所有的图片以及公式将大量引用自Jure老师的PPT，在此对老师表示感谢，话不多说，开始第一次课程。\n Why Graphs\n​\t图是描述与分析包含 关联（relations）或  相互作用（interactions）的实体的通用语言。\n 网络（Network）与图（Graph）\n​\t在日常生活中，我们身边许多的数据形式都是图结构。如下图所示，从左到右依次为自然图（natural graph）、基础设施图（infrastructure）、社交网络（social network）、知识图谱（knowledge graph）等诸多领域。\n\n​\t不难看出，当前有两种大的数据类型可以表示为图，首先是网络（network），其次就是图（graph）\n​\tNetworks（Natural Graphs）：自然表示为图\n​\t\t社交网络：社会是70亿人口的集合，那么每个人之间的联系就可以构成一个社交网络。\n​\t\t通讯和交易：例如电子设备，电话通讯，金融交易等等。\n​\t\t生物医学：例如身体中基因或蛋白质之间的相互作用。\n​\t\t脑神经元链接：我们的思维就是由数以亿计的脑神经元链接并相互作用产生的。\n\n​\tGraphs：作为一种表示形式\n​\t\t信息或知识图谱：这些节点都是有组织有关联的。\n​\t\t软件：软件也可以表示为图。\n​\t\t相似性网络（similarity networks）：可以多次将数据点（datapoints）与相似的数据点相连构成相似性网络。\n​\t\t具有关联结构：例如分子间结构，场景图，3D模拟等等。\n\n 图表示学习（Graph Representation Learning）\n​\t当我们了解图后，我们就要思考**如何利用这些关联结构来进行预测呢？**开始人们依旧想使用传统机器学习的方式，但发现图机器学习与传统机器学习的区别，如下图。\n\n​\t可以看到图结构与传统的固定的图像矩阵或者文本序列不同，它是一种任意大小任意形状的拓扑结构。因此对于每一个节点都没有参照点并且可以具备不同或多种特征。因此要设计一种新的网络，当输入一张图（网络）后，即可通过设计的神经网络输出预测的节点类型，关联关系甚至是生成一张图或子图。如下图。\n\n​\t传统的机器学习更倾向于对原始数据先进行特征处理，以便机器学习模型进行使用。而在图深度学习过程中，如下图，则删掉传统的特征工程。通过表示学习（representation learning）方法自动地对图数据的特征进行提取、学习。然后应用至下流预测任务（downstream prediction task）中去。\n\n​\t表示学习最主要的方式就是图嵌入（embeding），这个在后面会深入讲解，在这里可以简单的了解为将图中的节点通过一个映射函数转换成一个d维向量。其中图中相似的节点嵌入的位置更接近，如下图。\n\n Applications of Graph ML\n​\t对于一张图，可以有多种不同层次的预测，如下图所示，对于每一个组成部分甚至是整张图，都可以作预测。\n\n​\t节点分类：预测一个节点的属性。\n​\t链接预测：预测两个节点之间是否缺少链接。\n​\t聚类：检测节点是否形成了一个社区\n​\t图分类：分类不同的图形。\n​\t图生成、图进化\n","categories":["MIT-CS224W-图机器学习"],"tags":["GNN","MIT","2019"]},{"title":"关于","url":"/about/index.html","content":"Hi！你好！我是来自天津大学的博士研究生小泽，首先十分欢迎你来到我的博客。\n","categories":[],"tags":[]}]