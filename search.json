[{"title":"图深度学习笔记-图论基础","url":"/2021/11/24/book-mayao-02-md/","content":" 1 图的表示\n 1.1 图（Graph）\n 定义：\\textbf{ 定义：} 定义：一个图可以表示为G={V,ε}G = { \\{V,\\varepsilon \\}}G={V,ε}，其中VVV = {v1v_1v1​,⋯\\cdots⋯,vNv_NvN​}是大小为N=∣V∣N = | V |N=∣V∣ 的节点集合，ε\\varepsilonε={ e1e_1e1​,⋯\\cdots⋯,eMe_MeM​}是大小为MMM的边的集合。\n\n如上图（图1），为一个有五个节点和六条边的图。其中{ v1v_1v1​,⋯\\cdots⋯,v5v_5v5​}为五个节点，{ e1e_1e1​,⋯\\cdots⋯,e6e_6e6​}为六条边。\n在图中，如果有一条边eie_iei​连接两个节点vei1v_{e_i}^1vei​1​和vei2v_{e_i}^2vei​2​，那么这条边可以表示为（vei1v_{e_i}^1vei​1​,vei2v_{e_i}^2vei​2​），在有向图中表示边从起点vei1v_{e_i}^1vei​1​指向终点vei2v_{e_i}^2vei​2​。相反在无向图中由于没有顺序之分，则eie_iei​ = （vei1v_{e_i}^1vei​1​,vei2v_{e_i}^2vei​2​）= （vei2v_{e_i}^2vei​2​,vei1v_{e_i}^1vei​1​）。举个例子，如图一中的边e6e_6e6​连接节点v1v_1v1​与节点v5v_5v5​，那么由于图1为无向图，则e6e_6e6​也可以表示为（v1v_1v1​,v5v_5v5​）或（v5v_5v5​,v1v_1v1​）。\n 1.2 邻接矩阵（Adjacency Matrix）\n为了方便查看节点之间的连接关系，图G={V,ε}G = { \\{V,\\varepsilon \\}}G={V,ε}可以等价的表示为邻接矩阵的形式，更加直观的描述节点之间的关系。\n 定义： \\textcolor{blue} {\\textbf { 定义： } } 定义： 给定一个图G={V,ε}G = { \\{V,\\varepsilon \\}}G={V,ε},对应的邻接矩阵可以表示为A∈{0,1}N×NA∈{ \\{ 0,1 \\} ^ { N×N } }A∈{0,1}N×N。邻接矩阵AAA的第iii行第jjj列元素Ai,jA_{i,j}Ai,j​表示节点viv_ivi​和vjv_jvj​的连接关系。具体来讲，如果viv_ivi​与vjv_jvj​相邻，则Ai,j=1A_{i,j} = 1Ai,j​=1，否则Ai,j=0A_{i,j} = 0Ai,j​=0。\n在无向图中，由于不受节点顺序影响，则Ai,j=Aj,iA_{ i,j } = A_{ j,i }Ai,j​=Aj,i​，所以无向图的邻接矩阵一定是 关于主对角线对称 \\textbf { 关于主对角线对称 } 关于主对角线对称 的。以图1为例，该图的邻接矩阵可表示为：\nA=(0101110100010011000110110)A = \n\\begin {pmatrix}\n0 &amp; 1 &amp; 0 &amp; 1 &amp; 1 \\\\\n1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\\n0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 \\\\\n1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\\n1 &amp; 0 &amp; 1 &amp; 1 &amp; 0 \\\\\n\\end {pmatrix}\nA=⎝⎜⎜⎜⎜⎛​01011​10100​01001​10001​10110​⎠⎟⎟⎟⎟⎞​\n可以看到，主对角线值均为0，且都关于其对称。\n 2 图的性质\n 2.1 度（Degree）\n 定义： \\textcolor{blue} {\\textbf { 定义： } } 定义： 在图G={V,E}G = { \\{V,E \\}}G={V,E}中，节点vi∈Vv_i ∈ Vvi​∈V的度定义为图GGG中与节点viv_ivi​相关联的边的数目。\nd(vi)=∑vj∈V1ε({vi,vj})d(v_i) = \\sum_{ v_j ∈ V}1\\varepsilon( \\{ v_i,v_j \\} )\nd(vi​)=vj​∈V∑​1ε({vi​,vj​})\n其中，1ε(⋅)1\\varepsilon(\\cdot)1ε(⋅)为指示函数，简单来说就是满足条件就取1，不满足就取0。对于图来讲可表示为：\n1ε({vi,vj})={1,(vi,vj)∈ε0,(vi,vj)∉ε1\\varepsilon( \\{ v_i,v_j \\} ) = \n\\begin {cases}\n1, \\quad (v_i,v_j)∈\\varepsilon \\\\\n0, \\quad (v_i,v_j)\\notin\\varepsilon\n\\end {cases}\n1ε({vi​,vj​})={1,(vi​,vj​)∈ε0,(vi​,vj​)∈/​ε​\n即当存在(vi,vj)(v_i,v_j)(vi​,vj​)这条边时，d(vi)d(v_i)d(vi​)就加1，反之如果不存在，就加0。其实最直观的看法就是找从这个节点出发引出几条边。但这种方法会有一些麻烦，所以如果能够画出图的邻接矩阵的话，可以更简单的计算结果，利用邻接矩阵可以将节点的度表示为：\nd(vi)=∑j=1NAi,jd(v_i) = \\sum_{ j = 1 }^N A_{ i,j }\nd(vi​)=j=1∑N​Ai,j​\n简单来说就是计算该节点所在行所有值的和。任以图一为例，节点v5v_5v5​与v1,v3,v4v_1,v_3,v_4v1​,v3​,v4​相连，则节点v5v_5v5​的度为3。如果利用邻接矩阵计算，v5v_5v5​占矩阵第五行，直接将改行所有元素相加，即可得到其度为3。\n","categories":["图深度学习教材笔记"],"tags":["笔记","GNN","教材"]},{"title":"paper-aGentleIntroductionToGNN.md","url":"/2021/11/21/paper-aGentleIntroductionToGNN-md/","content":"","categories":[],"tags":[]},{"title":"第一讲-图的结构","url":"/2021/10/19/MIT-01-md/","content":" 引言\n​\t图机器学习是我博士期间研究方向，这篇博客会从头到尾详细描述我对于该领域的学习过程。理论部分我会通过学习视频课程和看教材来完成，MIT-CS224W-图机器学习这个分类是我用所看的视频课程的名字命名的，该课程是斯坦福的Jure老师在2021年冬季的课程，该课程也是最好的图机器学习启蒙课程，因此，我会对Jure老师的每一集课程都做一期复现与详解，以帮助自己和读者更好的理解这门课程。本文所有的图片以及公式将大量引用自Jure老师的PPT，在此对老师表示感谢，话不多说，开始第一次课程。\n 一、Motivation for Graph ML\n​\t图是描述与分析包含 关联（relations）或  相互作用（interactions）的实体的通用语言。\n​\t在日常生活中，我们身边许多的数据形式都是图结构。如下图所示，从左到右依次为自然图（natural graph）、基础设施图（infrastructure）、社交网络（social network）、知识图谱（knowledge graph）等诸多领域。\n\n​\t不难看出，当前有两种大的数据类型可以表示为图，首先是网络（network），其次就是图（graph）\n​\tNetworks（Natural Graphs）：自然表示为图\n​\t\t社交网络：社会是70亿人口的集合，那么每个人之间的联系就可以构成一个社交网络。\n​\t\t通讯和交易：例如电子设备，电话通讯，金融交易等等。\n​\t\t生物医学：例如身体中基因或蛋白质之间的相互作用。\n​\t\t脑神经元链接：我们的思维就是由数以亿计的脑神经元链接并相互作用产生的。\n\n​\tGraphs：作为一种表示形式\n​\t\t信息或知识图谱：这些节点都是有组织有关联的。\n​\t\t软件：软件也可以表示为图。\n​\t\t相似性网络（similarity networks）：可以多次将数据点（datapoints）与相似的数据点相连构成相似性网络。\n​\t\t具有关联结构：例如分子间结构，场景图，3D模拟等等。\n\n​\t当我们了解图后，我们就要思考如何利用这些关联结构来进行预测呢？\n","categories":["MIT-CS224W-图机器学习"],"tags":["GNN","MIT","2019"]},{"title":"关于","url":"/about/index.html","content":"Hi！你好！我是来自天津大学的博士研究生小泽，首先十分欢迎你来到我的博客。\n","categories":[],"tags":[]}]