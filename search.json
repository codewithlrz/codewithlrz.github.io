[{"title":"Recommendation","url":"/2022/04/08/Multiple%20Choice%20Questions%20based%20Multi-Interest%20Policy%20Learning%20for%20Conversational%20Recommendation/","content":" 摘要\n会话推荐系统(CRS)能够基于交互式对话获得细粒度的动态用户偏好。但是之前的CRS假设用户有一个明确的目标项目，但都已经求助于CRS了，说明用户可能并不清楚自己需要什么，同时用户也可能对一个属性类型有多个接受项，因此，我们首先提出了一种更现实的会话推荐学习设置，即多兴趣多轮会话推荐(MIMCR)，其中用户可能对多个属性实例组合有兴趣，并接受具有部分重叠的吸引力组合的多个项。为了更有效地获得用户偏好，代理在特定的属性实例上生成多项选择题，而不是二进制的是/否问题。此外，我们提出了一种联合集策略来选择候选项目，而不是现有的交集集策略，以克服在对话过程中的过度过滤项目。最后，我们设计了一个多兴趣策略学习(MIPL)模块，它利用用户捕获的多个兴趣来决定下一个行动，或询问属性实例或推荐项目。\n 引言\n本文重点关注于多轮会话推荐(MCR)设置，这是迄今为止最现实的CRS设置。该系统关注于在每个回合中询问属性还是推荐项目，并通过用户反馈灵活调整动作，以更少的回合做出成功的推荐。\n\n如上图，（a）的方式太过于二分，例如询问了sports，如果回答yes就不会再询问该属性下的其他类别。（b）的方式则会出现过度过滤掉现象，因为每个属性间都会存在关联项，拒绝一个就会拒绝多个。\n 相关工作\n当前的MCR工作都忽略了一个更现实的场景，在这个场景中，用户可以接受多个具有部分重叠属性的项。因此，我们提出了一个名为MIMCR的新方案来填补这一空白。此外，我们开发了一个新的框架，即MCMIPL来应对现有的挑战。\n 定义与准备工作\n在这项工作中，我们假设用户在使用CRS时对项目的偏好是不完整的。具体来说，用户对于某些属性类型有明确的单一首选项，而对于其他属性类型，他的首选项可能是多样的或模糊的。在CRS的指导下，他可以接受相同类型的多个属性实例，这导致用户可能对不同属性实例组合下的多个项产生兴趣。因此，我们提出了一种新的方案，即多兴趣多回合对话推荐（MIMCR）。\n","categories":[],"tags":[]},{"title":"CS224W-01-Intro","url":"/2022/03/19/MIT-01-md/","content":"引言​    图机器学习是我博士期间研究方向，这篇博客会从头到尾详细描述我对于该领域的学习过程。理论部分我会通过学习视频课程和看教材来完成，MIT-CS224W-图机器学习这个分类是我用所看的视频课程的名字命名的，该课程是斯坦福的Jure老师在2021年冬季的课程，该课程也是最好的图机器学习启蒙课程，因此，我会对Jure老师的每一集课程都做一期复现与详解，以帮助自己和读者更好的理解这门课程。本文所有的图片以及公式将大量引用自Jure老师的PPT，在此对老师表示感谢，话不多说，开始第一次课程。\nWhy Graphs​    图是描述与分析包含 关联（relations）或  相互作用（interactions）的实体的通用语言。    \n网络（Network）与图（Graph）​    在日常生活中，我们身边许多的数据形式都是图结构。如下图所示，从左到右依次为自然图（natural graph）、基础设施图（infrastructure）、社交网络（social network）、知识图谱（knowledge graph）等诸多领域。\n\n​    不难看出，当前有两种大的数据类型可以表示为图，首先是网络（network），其次就是图（graph）\n​    Networks（Natural Graphs）：自然表示为图\n​        社交网络：社会是70亿人口的集合，那么每个人之间的联系就可以构成一个社交网络。\n​        通讯和交易：例如电子设备，电话通讯，金融交易等等。\n​        生物医学：例如身体中基因或蛋白质之间的相互作用。\n​        脑神经元链接：我们的思维就是由数以亿计的脑神经元链接并相互作用产生的。\n\n​    Graphs：作为一种表示形式\n​        信息或知识图谱：这些节点都是有组织有关联的。\n​        软件：软件也可以表示为图。\n​        相似性网络（similarity networks）：可以多次将数据点（datapoints）与相似的数据点相连构成相似性网络。\n​        具有关联结构：例如分子间结构，场景图，3D模拟等等。\n       \n图表示学习（Graph Representation Learning）​    当我们了解图后，我们就要思考如何利用这些关联结构来进行预测呢？开始人们依旧想使用传统机器学习的方式，但发现图机器学习与传统机器学习的区别，如下图。\n\n​    可以看到图结构与传统的固定的图像矩阵或者文本序列不同，它是一种任意大小任意形状的拓扑结构。因此对于每一个节点都没有参照点并且可以具备不同或多种特征。因此要设计一种新的网络，当输入一张图（网络）后，即可通过设计的神经网络输出预测的节点类型，关联关系甚至是生成一张图或子图。如下图。\n\n​    传统的机器学习更倾向于对原始数据先进行特征处理，以便机器学习模型进行使用。而在图深度学习过程中，如下图，则删掉传统的特征工程。通过表示学习（representation learning）方法自动地对图数据的特征进行提取、学习。然后应用至下流预测任务（downstream prediction task）中去。\n\n​    表示学习最主要的方式就是图嵌入（embeding），这个在后面会深入讲解，在这里可以简单的了解为将图中的节点通过一个映射函数转换成一个d维向量。其中图中相似的节点嵌入的位置更接近，如下图。\n\nApplications of Graph ML​    对于一张图，可以有多种不同层次的预测，如下图所示，对于每一个组成部分甚至是整张图，都可以作预测。\n\n​    节点分类：预测一个节点的属性。\n​    链接预测：预测两个节点之间是否缺少链接。\n​    聚类：检测节点是否形成了一个社区+\n​    图分类：分类不同的图形。\n​    图生成、图进化\nChoice of Graph Representation这个可以参考图论基础 \nTraditional Feature-based Method\nNode-level Tasks and Features**1. 节点度：** 节点的度是最常见的节点特征，但是在实验中，由于节点的度存在着相同的情况，所以不能明确的区分各个节点。\n\n\n\n例如，节点A与节点F度均为1；节点C与节点E度均为3。\n\n**2. 节点中心性：**由于节点度不能看出节点的重要性，因此节点中心性旨在设计捕获节点重要性。\n\n**(1) 特征向量中心性（Engienvector centrality）**\n\n特征向量则认为每一个节点的重要性是不同的。就好像，5个菜鸟说你厉害，那你也不一定厉害。但如果是5个大佬都说你厉害，那你肯定是很厉害。可以表示为：\n\n人们认为，计算出最大的特征值$\\lambda{max}$，那么它对应的特征向量$c{max}$就对应了每个节点的重要程度。\n**(2) 介数中心性（Betweenness centrality）**\n\n这里认为，一个节点重不重要，取决于通过它的最短路的个数，很好理解，如果这个节点充当着很多节点间最短路的一个中介，那它就很重要。\n\n这个我在介数中心性例子也写出了具体步骤。\n**(3) 紧密中心性（Closeness centrality）**\n\n这个方法就是找节点到其他节点的最短距离。\n\n老师也举了个例子\n**3. 聚类系数：**\n\n当然，我们也可以看一个节点的相邻节点是否有联系，就像是你的朋友之间是不是也是朋友。\n\n\n上图也是举了一个例子，找的方式也非常简单，只要数出包含中心节点的三角形就好，计算三角个数与节点对总数的比值。\n**4. Graphlets：**\n\n既然可以使用三角形表示，人们又开发出graphlets的方式，来计算小图数量。\n\n如上图是2-5个节点的graphlets表示，我们可以利用这个特性写出一个节点的Graphlet Degree Vector（GDV）。例如：\n\n以2-node graphlet和 3-node graphlet为例，节点v的GDV就可以这样表示。\nLink Prediction Tasks and Features边预测任务就是基于存在的边预测新的边，然后我们通过一个排序，前K个就是预测边。\n目前有两种方式描述边预测的任务：\n\n随机删除一些边，然后利用深度学习方法对其进行预测。\n随着时间变化，例如在t0时刻预测t1时刻的链接，然后观察在t1时刻哪些真的出现了。\n\n**1. 基于距离的特征：**\n\n\n然而，这并不能捕获邻近的重叠程度，比如(B,H)就是共享两个节点（C、D）,而距离同为2的(A,B)，(B,E)就只共享一个节点。\n**2. 本地邻域重叠：**\n\n这个方法就直接计算了两个节点之间的共享邻居节点。\n\n**3. 全局邻域重叠：**\n\n如果只考虑一跳关系显然是不够的，因此我们也要考虑2跳甚至是多跳节点，因此出现了Katz指标（Katz index）来计算一对节点间的路径总数。\n方法就是利用邻接矩阵，其中$A{uv}$表示节点u与v之间的长度为1的路径数，$A{uv}^2$来表示节点u、v之间长度为2的路径数，以此类推，$A_{uv}^l$表示长度为$l $的路径数。\n具体我在连通度的定理中用图论的方法以及线代方法证明过。\nGraph-Level Features and Graph Kernels在图级别，我们想要预测两个图的相似性。\n**1. 核方法（Kernel Method）：**\n\n这里类比词袋模型（Bag of Words），提出了一种Bag of node degrees，例如：\n\n**2. Graphlet Kernel Method：**\n\n通过引入Graphlet子图，计算每一个图的Graphlet个数，例如：\n\n那么只需要计算两个图graphlet kernel即可。\n\n当然，如果两个图的size不同，那么我们还要对其进行normalize：\n\n很明显，graphlet是一种很浪费空间的方法。因此引入下一种方式。\n**3. Weisfeiler-Lehman Kernel：**\n\n首先，先对两个图进行Color refinement，方法原理如下：\n\nPPT中也举了一个很详细的例子：\n\n\n以此类推，直到达到我们想要到的第K步，得到下图：\n\n接下来就使用Weisfeiler-Lehman Kernel对每一个颜色进行统计。\n\n最后计算乘积即可。\n\n","categories":["MIT-CS224W-图机器学习"],"tags":["GNN","MIT","2019"]},{"title":"图表示学习基础","url":"/2022/03/15/book-GRL-01/","content":"引言（Introduction）什么是图（What Is  A Graph）图的定义以及一些性质可参考图论基础。\n多关系图（Multi-relational Graphs）一个图的类型可以根据其边的性质进行划分，如有向图，无向图，加权图等，那么图中含有不同类型的边的图，就可以是多关系图。\n举个例子，在吃药物的过程中，不同种的药物之间会有不同的反应，因此每一条边的类型都会有很大差异，如图1。\n\n那么，就要扩展一种新的边表示方法，添加边的类型$\\tau$，例如在简单图中表示一条边为$(u,v)$,其中$u,v$为图中的节点。那么表示多关系图的边就可以为$\\boldsymbol{ (u,\\tau,v) \\in E}$。\n同时邻接矩阵也要进行改变，从原来的二维变为三维$\\boldsymbol{ A \\in \\mathbb{R}^{ |V| \\times |R| \\times |V|}}$，其中$R$是一组关系的集合。\n在多关系图中也有两个很重要的子集，异质图（Heterogeneous graph）和多维图（Multiplex graph）\n异质图（Heterogeneous graph）在异质图中，可以将节点划分为多种类型，每一个节点都对应着一种类型。\n表示为 构图中的每一条边都可能会被特定的节点所约束，换句话说，特定的边只会连接特定的节点。举一个例子，假设在一张医学类型的图上有三类节点，分别是蛋白质，药物和疾病。那么在其中代表“治疗”的边只会出现在药物和疾病两类节点之间，类似的代表“多药副作用”的边只会出现在两个药物节点之间。\n有一种特殊的异质图—多部图（Multipartite graphs），其边只能连接不同类型的节点。\n表示为$(u,\\tau_i,v)\\in E \\rightarrow u \\in V_j,v \\in V_k \\wedge j \\neq k$。\n图2为普通异质图与多部图的表现形式。\n\n多重图（Multiplex graphs）在多重图中，可以想象将节点分为$k$层，每一层节点都是一样的，只是每一层节点间的连接方式不同。同时，不同层内的相同节点可以通过层间边相连。其节点集合与边集合与边集合分别表示为：\n$V = V_1 \\cup V_2 \\cup \\cdots \\cup V_k$\n$E = E1 \\cup E_2 \\cup \\cdots \\cup E_k \\cup E{ k+1}$，$E_{ k+1}$为层间边集合。\n举一个例子，如图3所示。\n\n假设每一层中的每一个节点为一个城市，连接的边代表各种交通工具连接两个城市，层间边代表同一座城市可以进行换乘。现在如果在L1层中想从节点v1到达节点v9，由于L1中无相应路径，即可先到达节点v5，通过层间边到达L2层，从而到达v9。\n特征信息（Feature Information）在多数情况下都会有与图对应的特征信息，多数情况下都是节点级别的，所以用一个实数矩阵（real-valued matrix）$\\textbf{ X} \\in \\mathbb{ R}^{ |V| \\times m}$表示。\n图机器学习（Machine learning on graphs） 图机器学习没有像传统机器学习对监督学习（给定分类标签）与无监督学习（聚类）有明确的界限划分，大多数情况更偏向于\n半监督学习方式。\n节点分类（Node classification）在通常情况下，由于只有少部分节点存在标签信息，但是在训练的过程中仍然会使用无标签的数据，这与监督学习的只使用有标签数据不同，因此节点分类任务通常被归为是半监督学习。同时，节点不满足独立同分布(i.i.d.)。\n当前对于节点分类主要时利用节点之间的连接，可利用其同质性与结构等价性。\n同质性：认为相邻节点属性相似，即节点与邻居节点有共享属性趋势。\n结构等价性：认为具有相似布局结构的节点将具有相似标签。\n异质性：认为节点优先连接与自身不同类型节点（如社交网络中的性别属性）。\n关系预测（Relation prediction）关系预测在不同领域中有有不同的名字，链接预测（Link Prediction）、图谱补全（Graph Completion）、和关系推断（Relational Inference）。与节点分类相似，节点分类是预测节点，关系预测是预测边，即在边的层次研究。方式也是半监督学习，给定一组节点$V$以及部分边的集合$E_{ train} \\subset E$，利用这些信息推断缺失边。\n聚类和社区发现（Clustering and community detection）希望将图表现出一种社区结构，节点能够更有可能与同一社区的节点形成边。\n如下图所示，根据网络中不同节点之间连接的紧密程度，我们可以将网络视为是由不同的“簇”所组成，其中“簇”内的节点之间连接更加的紧密，不同“簇”之间的的节点之间的链接比较稀疏。我们将这种“簇”称为是网络中的社区结构。odes\n\n社区发现的定义：通过输入一张图$G = (V,E)$推断出潜在的社区结构。可以看到是在子图层次进行研究。\n图的分类、回归与聚类（Graph classifification, regression, and clustering）当研究完节点、边、子图后，最后的是整张图级别的研究。\n在图分类或图回归任务中，其数据集由多张不同的图构成，利用图机器学习算法对每张图进行独立预测（不是每张图的组成部分）。\n在图聚类任务中，目标是学习一个无监督的测量图与图之间的相似性策略。\n背景与传统方法（Background and Traditional Approaches）(21条消息) Bag-of-words 词袋模型基本原理Jaster_wisdom的专栏-CSDN博客词袋模型的原理\n","categories":["图表示学习笔记"],"tags":["GNN","笔记","图表示学习"]},{"title":"Unified Conversational Recommendation Policy Learning via Graph-based Reinforcement Learning","url":"/2022/03/15/Unified%20Conversational%20Recommendation%20Policy%20Learning%20via%20Graph-based%20Reinforcement%20Learning/","content":"摘要会话推荐系统(CRS)使传统的推荐系统能够通过交互式对话明确地获取用户对项目和属性的偏好。强化学习(RL)被广泛应用于学习会话推荐策略，以决定在每次对话回合中询问哪些属性，推荐哪些项目，以及何时询问或推荐。然而，现有的方法主要针对解决CRS中这三个决策问题中的一个或两个，这限制了CRS的可伸缩性和通用性，不能保持稳定的训练过程。针对这些挑战，我们建议将CRS中的这三个决策问题作为一个统一的策略学习任务。为了系统地集成会话和推荐组件，我们开发了一种基于动态加权图的RL方法来学习一个策略，以选择每个会话回合时的动作，或询问属性或推荐项目。进一步地，为了解决样本效率问题，我们提出了两种根据偏好信息和熵信息来减少候选行动空间的行动选择策略。在两个基准的CRS数据集和一个真实的电子商务应用上的实验结果表明，该方法不仅显著优于现有的方法，而且提高了CRS的可扩展性和稳定性。\n引言会话推荐系统(CRS)旨在通过交互式对话了解用户的偏好并提出建议。传统的推荐系统或交互推荐系统(IRS)主要关注解决推荐的问题，CRS通常存在另外两个核心研究问题，即问什么问题，何时问或推荐。最近的研究已经证明了在CRS中提出清晰的问题的互动的重要性。更重要的是，决定何时询问或推荐是协调对话和推荐，是开发一个有效的CRS的关键。\n在多轮会话推荐系统（MCR）场景中，CRS通常被表述为一个多步骤的决策过程，并通过策略学习的强化学习(RL)方法来解决。如图1(a)所示，基于RL的IRS只需要学习策略来决定推荐哪些项目。然而，CRS中的情况更为复杂，因为有两个组件需要一致地考虑，即对话和推荐组件。现有的方法CRM和  Estimation-Action-Reflection（EAR）采用策略梯度来改进何时和什么属性的策略，而推荐决策则由外部推荐模型做出。为了减少政策学习中的行动空间，另一种最先进的方法SCPR只考虑学习何时询问或推荐的策略，而两个孤立的组成部分负责决定该问什么和推荐哪一个。这两种CRS的策略学习框架如图1(b)和1(c)所示。 \n\n尽管这些方法很有效，但在现实应用中仍有一些问题需要解决：\n\n使用现有 CRS 方法训练的模型对不同领域或应用缺乏通用性，因为在 CRS 中需要考虑三个不同的决策过程，包括要询问哪些属性、要推荐哪些项目以及何时询问或推荐。 它需要额外的努力来训练离线推荐模型或使用合成对话历史预训练策略网络。\n策略学习很难收敛，因为会话和推荐模块是独立的，在训练过程中缺乏相互影响。\n\n为了解决这些问题，在本工作中，我们将CRS中的上述三个独立的决策过程制定为一个统一的政策学习问题，以利用CRS的最终目标，并在培训过程中填补建议和对话组件之间的差距。这种统一的会话推荐策略学习(UCRPL)旨在学习一个统一的策略来决定行动，要么在每次会话中询问一个属性，要么推荐一个项目，以最大化整个MCR过程中的累积效用。图2描述了CRS统一策略学习的概述。\n\n然而，UCRPL问题伴随着两个挑战：(i)如何系统地结合对话和推荐组件来实现统一的策略学习？(ii)如何处理样品效率问题？随着UCRPL中的动作空间变得非常大，包括所有可用的属性和项目，它需要大量的交互数据来学习最优策略。幸运的是，图结构捕获了不同类型的节点（即用户、项和属性）之间丰富的相关信息，使我们能够发现用户对属性和项的协作偏好。因此，我们可以利用图结构将推荐和会话组件作为一个有机整体集成，其中会话会话视为图中维护的节点序列，动态利用会话历史来预测下一个回合的行动。另一方面，虽然图的连通性也可以通过路径推理来消除无效的动作，但仍有大量的候选对象用于动作搜索。由于用户不太可能对所有的项目和属性都感兴趣，所以我们可以关注潜在的重要候选对象，以提高UCRPL的样本效率。\n为此，我们提出了一种新的、自适应的基于图的强化学习框架，即统一的对话重组器（UNIfied COnversational RecommeNder (UNICORN)）。具体来说，由于CRS的进化性质，我们利用一个动态加权图来建模对话过程中用户、项目和属性之间不断变化的相互关系，并考虑一个基于图的马尔可夫决策过程(MDP)环境来同时处理推荐和会话的决策制定。然后，我们集成了图增强的表示学习和序列对话建模，以捕获用户对项目和属性的动态偏好。此外，还设计了两种简单而有效的行动选择策略来处理样本效率问题。我们采用基于偏好的项选择和基于加权熵的属性选择策略来列举整个候选项和属性集，而是只考虑潜在的重要行为。\n综上所述，本文的贡献如下：\n\n在会话推荐系统中，我们制定了三个独立的决策过程，即何时询问或建议，问什么和建议什么，作为一个统一的会话推荐策略学习问题。\n为了解决UCRPL问题中的挑战，我们提出了一种新的自适应强化学习框架，即基于动态加权图的统一对话重组器（UNICORN）。为了解决样本效率问题，我们进一步设计了两种简单而有效的行动选择策略。\n实验结果表明，该方法在四个公共基准数据集和一个真实的电子商务应用程序上的性能显著优于最先进的CRS方法。\n\n相关工作会话推荐：根据问题的设置，目前的CRS研究可以分为四个方向。\n\nExploration-Exploitation Trade-offs for Cold-start Users。这些方法利用bandit方法来平衡在会话推荐场景中对冷启动用户的探索和开发权衡。\n问题驱动的方法。旨在向用户询问问题，以获得更多关于他们的偏好的信息，这通常被称为“询问澄清/澄清问题”。\n对话理解和生成。这些研究的重点是如何从用户的话语中理解用户的偏好和意图，并产生流畅的回应，从而提供自然和有效的对话行动。\n多轮对话推荐。在这个问题设置下，系统会多次询问关于用户偏好的问题或提出建议，目的是通过更少的对话次数来实现吸引人和成功的建议。在这些问题设置中，我们主要关注MCR问题。\n\n推荐中的强化学习（RL）：强化学习(RL)由于其考虑用户的长期反馈的优点，已被广泛地引入到推荐系统中。基于RL的推荐将推荐过程制定为用户与推荐代理之间交互的MDP，并使用RL算法来学习最优推荐策略。最近关于序列推荐和交互式推荐的工作采用RL来捕获用户的动态偏好，以便随着时间的推移生成准确的推荐。这些方法的目标通常是学习一个有效的策略来确定要推荐哪些项目。对于CRS，采用基于RL的方法来改进其他两个决策过程的策略，包括(i)询问有哪些属性，(ii)何时询问或推荐。为了简化MCR的整体框架，具有更好的可伸缩性和通用性，我们将CRS中的这三个核心决策过程制定为一个统一的策略学习问题。\n基于图的推荐：基于图的推荐研究主要利用图的结构用于两个目的。第一个方法是通过基于图形的表示学习来提高推荐性能，包括利用结构信息进行协同过滤，并采用知识图嵌入作为丰富的上下文信息。另一种研究将推荐模型作为一个路径推理问题来构建可解释的推荐系统。近年来，基于图的RL方法在推荐系统的不同场景中取得了许多成功的应用。\n问题定义多轮会话推荐（MCR），在这项工作中，我们关注多轮会话推荐(MCR)场景，这是迄今为止提出的最现实的会话推荐设置，其中CRS能够提出关于属性的问题或多次提出建议。\n具体来讲，在系统方面，CRS维护了一组要推荐的项目$V$,并且每个项目$v$都与一组属性$Pv$相关联。在每个中，由指定属性𝑝0的用户𝑢初始化会话。在每一个episode中，由指定属性$p_0$的用户$u$来初始化会话。然后，CRS可以自由地询问用户对从候选属性集$P {cand}$中选择的属性偏好，或者从候选项集$V_ {cand}$中推荐一定数量的项目。系统将根用户的响应来决定下一个操作。系统询问和用户响应过程重复进行，直到CRS达到目标或者达到最大轮数$T$。\n统一的对话推荐策略学习（UCRPL），上述的MCR任务可以表示为一个马尔可夫决策过程（MDP）。CRS的目标是学习一个策略𝜋，最大化观察到的MCR事件的预期累积回报。\n\n其中𝑠𝑡是从系统状态和对话历史中学习的状态表示，𝑎𝑡是代理在时间步𝑡采取的动作，𝑟(·)是中间回报，缩写为𝑟𝑡。\n方法下图是UNICORN方法的概述，主要有四个组成部分：基于图的MDP环境、图增强的状态表示学习、行动选择策略和Deep Q-Learning Network。\n\n基于图的MDP环境（Graph-based MDP Environment）MDP环境负责通知代理当前状态和可能要采取的操作，然后根据当前策略如何适合观察到的用户交互对代理进行奖励。形式上，MDP环境可以由一个元组(S，A，T，R)定义，其中S表示状态空间，A表示动作空间，T：S×A→S为状态转换函数，R：S×A→R为回报函数。\n状态（State）对于基于图的MDP环境，时间步𝑡的状态𝑠𝑡∈S应该包含会话推荐的所有给定信息，包括之前的会话历史和包含所有用户、项和属性的完整图G。给定一个用户𝑢，我们考虑两个主要元素：\n\n其中，$Hu^{ (t) } = [ P_u^{ (t) },P{ rej } ^{ (t) },V_{ rej }^{ (t) } ]$表示直到时间 t 之前的会话历史。$G_u^{ (t) }$表示用户 u 在时间步 t 时 $G$ 的动态子图（图的构造在后文）。\n$Pu$表示用户的偏好属性，$P{ rej } ^{ (t) },V_{ rej }^{ (t) }$分别表示被用户拒绝的属性与项目，初始状态𝑠0由用户指定的属性𝑝0初始化，例如，$s_0 = [ [ { p_0}, { } , { }],G_u^ { (0)}]$。\n行为（Action）根据状态$st$,代理可采取行为$a_t \\in A$，其中这个$a_t$可以是从候选项目集$V{ cand}^{ (t)}$中选择推荐项或者从候选属性集合$P_{ cand}^{ (t)}$中询问属性。遵循路径推理方法，我们有：\n\n其中$V{ p_u ^{ (t)}}$是直接连接所有$P_u^{ (t)}$（即所有满足首选属性的项目）的项目顶点的集合，$P{ V{ cand}^{ (t)}}$是直接连接到$V{ cand}^{ (t)}$之一的属性顶点集（即属性属于候选项中的至少一个）。\n转换（Transition）我们认为，当用户对行为$at$有响应时，当前状态$s_t$将转换到下一个状态$s{ t+1}$。具体而言，如果CRS询问一个属性$pt$并且用户接受了，那么下一个属性$s{ t+1}$将被$Pu^{ (t+1)} \\ = \\ P_u^{ (t)} \\cup p_t$更新。相反的，如果被拒绝了，那么下一个状态就会被$P{ rej}^{ (t+1)} \\  = \\ P{ rej}^{ (t)} \\cup a_t$或者$V{ rej}^{ (t+1)} \\  = \\ V{ rej}^{ (t)} \\cup a_t (a_t \\in P \\ or\\ a_t \\in V)$更新。因此下一个状态$s{ t+1}$将是$[H_u^{ (t+1)},G_u^{ (t+1)}]$。\n反馈（Reward）根据之前的MCR研究，我们的环境包含五种反馈，即：\n\n$r_{ rec_suc}$，当用户接受推荐项时的一个强正向反馈。\n$r_{rec_fail}$，当用户拒绝推荐项时的一个负反馈。\n$r_{ ask_suc}$，当用户接受所询问的属性时的一个弱正反馈。\n$r_{ask_fail}$，当用户拒绝所询问的属性时的一个负反馈。\n$r_{ quit}$，当达到最大回合数时，一个强负反馈。\n\n图增强状态表示（Graph-enhanced State Representation）由于我们将会话推荐定义为基于图的MDP环境上的统一策略学习问题，因此需要将会话和图的结构信息编码到潜在的分布式表示中。为了利用用户、项目和属性之间的相互关系，我们首先采用基于图的预训练方法来获得全图G中所有节点的节点嵌入。\n动态加权图的构造（Dynamic Weighted Graph Construction）我们基于图的MDP环境，将MDP环境的当前状态表示为一个动态加权图。形式上，我们将一个无向加权图表示为$G \\ = \\ (N,A)$，其中节点$ni \\in N$，邻接矩阵元素$A{i,j}$表示节点$n_i$和$n_j$之间的加权边。在我们的例子中，给定用户u,我们将在时间点t时的动态图表示为$G_u^{ (t)} \\ = \\ (N^{ (t)},A^{ (t)})$。\n\n其中，$w_v^{ (t)}$是一个标量，表示在当前状态下的项目$v$的推荐得分。为了纳入用户偏好以及被要求的属性和项目之间的相关性，这些权重$w_v^{ (t)}$被计算为:\n\n其中，$\\sigma( \\cdot )$表示sigmoid函数，$e_u,e_v,and \\ e_p$分别表示用户、项目、属性的嵌入。\n基于图表示学习（Graph-based Representation Learning）我们利用图的连通性相关用户、项目和属性之间的相关信息。利用图卷积网络（GCN），用结构和相关知识来改善节点表示。在第$(l+1)$层中的节点$n_i$可由如下公式计算：\n\n其中，$N_i$表示节点$n_i$的相邻索引，$W_l,B_l$表示从邻近节点和节点$n_i$本身的转换的可训练参数，$\\Lambda$是一个归一化的相邻矩阵。\n\n序列表示学习（Sequential Representation Learning）除了所涉及的用户、项和属性之间的相互关系外，CRS还需要对当前状态下的会话历史进行建模。与以往采用启发式特征进行对话历史建模的研究不同，我们使用Transformer编码器来获取会话历史的序列信息，并参与决定下一步行动的重要信息。每个Transformer层由三个组件组成：\n\n层归一化定义为 LayerNorm(·)。\nmulti-head attention定义为MultiHead(𝑸、𝑲、𝑽），其中𝑸、𝑲、𝑽分别为查询、键和值。\n将ReLU激活的前馈网络定义为FFN（·）。\n\n以第$l$层为例：\n\n其中，$X \\in R^{L\\times d}$表示嵌入，L是序列长度。在我们的例子中，输入序列$X^{ (0)}$是当前会话历史基于图表示学习${e_p^{ (L_g)}:p \\in P_u^{ (t)}}$所接受的属性$P_u^{ (t)}$，其中$L_g$是GCN中的网络层数。在使用𝐿𝑠Transformer层进行序列学习后，我们可以通过平均池化层聚合从图和会话历史中学习到的信息，得到$s_t$的状态表示。\n\n为简单起见，我们将𝑠𝑡的学习状态表示表示为𝑓𝜃𝑆（𝑠𝑡），其中𝜃𝑆是状态表示学习的所有网络参数的集合，包括GCN层和Transformer层。\n行为选择策略（Action Selection Strategy）较大的行为搜索空间会在很大程度上影响策略学习的性能。因此，它非常重视处理UCRPL中非常大的动作空间。为此，我们提出了两种简单的策略来提高候选行动选择的样本效率。\n基于偏好的项目选择。一般来说，对于要推荐的候选项目，我们只能考虑对少数最符合用户偏好的候选项目进行推荐的操作，因为用户不太可能对所有项目都感兴趣。为了实现这一点，我们在每个时间段t,从$V_{ camd}^{ (t)}$中选择$top-K_v$候选项到候选行为空间$A_t$中，排名是根据推荐评分$w_v^{ (t)}$排的。\n基于加权熵的属性选择。而要询问候选属性，期望属性不仅能够更好地消除候选项的不确定性，而且能够对用户偏好进行编码。受交互路径推理的启发，我们采用加权熵作为修剪候选属性的标准。\n\n其中$Vp$代表项目中含有属性$p$。类似于项目选择，我们也从$P{ cand}^{ (t)}$中选取$top-K_p$个候选属性到$A_t$中，基于加权熵分数$w_p^{ (t)}$。\nDeep Q-Learning Network在获得图增强状态表示和候选动作空间后，我们引入deep Q-Learning Network（DQN） 进行统一的会话推荐策略学习。我们进一步实施了一些技术来增强和稳定 DQN 的训练。统一的会话推荐 (Unified Conversational Recommender) 的训练过程见算法 1.\n\nDueling Q-Network根据标准假设，在每一个timestep中，延时的反馈都被一个$\\varUpsilon$中的因素给打折扣。我们定义Q-value $Q(st,a_t)$为基于状态$s_t$和行为$a_t$的预期反馈。如图3的最右部分，dueling Q-network是哟个两个深度神经网络分别计算value函数$f{ \\theta  V}( \\cdot )$和advantage函数$f{ \\theta_A}(\\cdot)$。\n然后Q-function可以被计算为：\n\n其中$f{ \\theta  V}( \\cdot )$和$f_{ \\theta_A}(\\cdot)$是两个独立的多层感知，参数分别为$\\theta_V$和$\\theta_A$，并让$\\theta_Q = { \\theta_V,\\theta_A }$。\n最优Q-function $Q^(s_t,a_t)$，具有最优策略$\\pi ^$可以达到最大期望回馈。它遵循贝尔曼方程：\n\nDouble Q-Learning with Prioritized Experience Replay在MCR过程中的每一个事件中，在每个时间步长𝑡中，CRS代理通过第4.2节中描述的图增强状态表示学习来获得当前状态表示$f_{\\theta_S}(s_t)$。然后通过第4.3节中描述的行为选择策略从候选行为空间$A_t$中选择一个行为$a_t$。本文采用$\\epsilon$-贪心算法去平衡行为取样中的exploration and exploitation（即选择一个贪心行为基于概率为1-$\\epsilon$最大Q-value,还有一个概率为$\\epsilon$的随机动作）。\n然后代理将从用户的反馈中得到一个反馈$rt$。根据这个反馈，当前状态$s_t$过渡到下一个状态$s{ t+1}$，候选行为空间$A_{ t+1}$也相应的做出更新。\n然后经验（experience）$(st,a_t,r_t,s{ t+1},A_{ t+1})$将被存入一个回放缓存（replay buffer）$D$ 中。为了训练DQN，我们从D中抽取小批经验，并将损失函数最小化：\n\n其中，$y_t$是基于当前最优$Q^*$的目标值。\n为了减轻传统DQN中的过高估偏差问题，我们采用了Double Q-learning，它使用了一个目标网络$Q’$作为在线网络的周期性副本，然后将在线网络的目标值更改为：\n\n其中，$\\theta_{Q’}$表示目标网络的参数，通过soft assignment更新为：\n\n其中$\\tau$表示更新频率。\n此外，传统的DQN是从回放缓存（replay buffer）中均匀的采样。为了更频繁的从那些有更多去学习的重要的过渡中取样，我们采用优先回放（replay）作为学习潜力的代理，它以相对于绝对 TD 误差的概率 𝛿 对转换进行采样。\n\n模型推理（Model Inference）对于学习到的UNICORN模型，给定一个用户和他/她的对话历史，我们遵循相同的过程来获得候选行为空间和当前状态表示，然后根据公式计算最大Q-value来决定下一个动作：\n\n如果所选的操作指向一个属性，系统将询问用户对该属性的偏好。否则，系统将向用户推荐Q-value最高的top-𝐾项。\n","categories":["论文解读"],"tags":["论文","CRS"]},{"title":"gnn4nlp-survey","url":"/2022/03/12/gnn4nlp-survey/","content":" 摘要\n深度学习已经成为NLP中处理各种任务的重要方法，尽管文本输入通常可以表示为一个token的序列，但大量的NLP问题可以用图结构更好的表示。在本项调查中，作者呈现了一个关于 GNN for NLP 的全面概述。对于 GNN for NLP，作者提出一个新的分类系统，从如下三个核心系统的组织了现有研究，即图构造、图表示学习、基于图的编解码器模型。文中进一步介绍了大量正在应用 GNNs 驱动 NLP 的应用，并总结了相应的基准数据集、评估指标和开源代码。最后，论文讨论了在NLP中充分利用GNNs的挑战以及未来的研究方向。\n 引言\n深度学习已经成为当今应对自然语言处理(NLP)中各种任务的主要方法，特别是在大规模文本语料库上操作时。传统来讲，文本序列会被认为是一袋 tokens，例如 NLP 任务中的 BoW（词袋） 和 TF-IDF算法。随着 Word Embeddings 技术的成功（Mikolov et al., 2013，以及Pennington et al., 2014）在NLP任务中，句子通常被表示为一个tokens序列。因此，流行的深度学习技术，如递归神经网络（Schuster and Paliwal, 1997）和卷积神经网络（Krizhevsky et al., 2012）已广泛应用于文本序列的建模。\n然而，有大量的NLP问题可以用图结构最好地表达。例如，文本序列中的句子结构信息（即句法解析树，如依赖树和选区解析树）可以通过合并特定于任务的知识来增强原始序列数据。类似地，序列数据中的语义信息（如语义解析图，如抽象意义表示图和信息提取图）也可以用来增强原始序列数据。因此，这些图结构数据可以为了学习更多信息的表示，编码实体 tokens 间的复杂关系。\n不幸的是，深度学习技术对于欧式数据（Euclidean data，如图像），或是序列数据（如文本）都是破坏性的。由于由于图数据的复杂性，如不规则的结构和节点邻居大小的变化，它们对于图结构的数据是不能直接适用的。因此这一隔阂推动了GNN的发展（Kipf and Welling, 2016，Defferrard et al., 2016，以及Hamilton et al., 2017a）。\n这种在图深度学习和NLP交叉上的研究浪潮影响了各种NLP任务。人们对应用和开发不同的GNNs变体的兴趣激增，并在许多NLP任务中取得了相当大的成功，从分类任务，如句子分类（Henaff et al., 2015， Huang and Carley, 2019） ，语义角色标记（Luo and Zhao (2020)；Gui et al. (2019)），关系提取（Qu et al., 2020；Sahu et al., 2019），来生成诸如机器翻译等任务（Qu et al., 2020；Beck et al., 2018），问题生成（Pan et al., 2020；Sachan et al., 2020）以及摘要生成（Fernandes et al., 2019；Yasunaga et al., 2017）。尽管现有的研究取得了成功，但NLP的图深度学习仍然面临着许多挑战，即：\n\n将原始文本序列数据自动转化为图结构的数据。大多数NLP任务使用文本序列作为原始输入，从文本序列中自动构建图数据以利用底层结构信息是利用图神经网络处理NLP问题的关键步骤。\n恰当的图表示学习技术。需要使用专门设计的GNN来学习不同图结构数据的独特特征，如无向、有向、多关系和异质图。\n有效地建模复杂数据。因为许多NLP任务涉及学习基于图的输入和高度结构化的输出数据之间的映射，如序列、树，以及具有多种类型节点和边的图数据。\n\n在本文中，作者将首次提出一个用于自然语言处理的图神经网络的全面概述，这项调查对机器学习和NLP社区都是及时的，它涵盖了相关和感兴趣的主题，包括NLP图的自动构建，图表示学习，基于GNN的编码器-解码器模型，以及各种NLP任务中的GNN的应用。\n\n作者强调其主要贡献如下：\n\n提出新的 GNN4NLP 的分类方法，系统地根据三个核心组织了现有的图构建、图表示学习和基于图的编码-解码器模型的研究。\n提供了针对各种NLP任务的最先进的基于GNN的方法的最全面的概述，作者基于领域知识和语义空间，对各种图的构造方法，针对各类图结构数据的图表示学习方法，基于GNN的编码-解码模型给定输入和输出数据类型的不同组合进行了详细的描述和必要的比较。\n介绍了大量的运用GNNs的NLP应用，包括它们如何通过三个关键组件（即图形构建、图形表示学习和嵌入初始化）处理这些NLP任务，以及提供相应的基准数据集、评估度量和开源代码。\n概述了在NLP中充分利用GNN所面临的各种突出挑战，并为富有成效和未探索的研究方向提供了讨论和建议。\n\n 基于图的自然语言处理算法\n在本节中，作者将首先从图的角度回顾NLP问题，然后简要介绍一些具有代表性的传统的基于图的方法来解决NLP问题的问题\n NLP：从图的视角\n一般来说，有三种不同的方式来表达自然语言。最简单的方法是将自然语言表示为一袋tokens。这种自然语言的观点完全忽略了在文本中出现的token的特定位置，而只考虑了一个token在文本中出现的次数。如果一个人随机打乱一个给定的文本，从这个角度来看，文本的意义根本不会改变。采用这种观点的最具代表性的NLP技术是主题建模（Blei et al., 2003），它旨在将每个输入文本建模为主题的混合，其中每个主题可以进一步建模为单词的混合。\n一种更自然的方法是将自然语言表示为一个tokens序列。这就是人类通常说和书写自然语言的方式。与上面的bag的视角相比，这种自然语言的视角能够捕获更丰富的文本信息，比如两个tokens是否连续，或是一个单词对在一个局部上下文中同时出现。采用这一观点的最具代表性的NLP技术包括线性链CRF（Lafferty et al., 2001），它实现了预测中的顺序依赖性。还有word2vec（Mikolov et al., 2013），它通过预测一个目标单词的上下文单词来学习单词嵌入。\n第三种方法是将自然语言表示为一个图。图在NLP中普遍存在，虽然将文本视为序列数据可能是最明显的，但在NLP社区中，将文本表示为各种图已经有很长的历史了。文本或世界知识的常见图表示包括依赖图、选区图、AMR图、IE图、词汇网络和知识图。此外，还可以构造一个包含文档、段落、句子、文字等多个元素层次结构的文本图。与上述两个视角相比，这种自然语言的视角能够捕捉到文本元素之间更丰富的关系。正如将在下一节中介绍的，许多传统的基于图的方法（如随机游走、标签传播）已经成功地应用于具有挑战性的NLP问题，包括词义消歧、名称消歧、指代消歧、情绪分析和文本聚类。\n 基于图的自然语言处理方法\n在上一小节中，作者已经讨论了许多NLP问题可以自然地转化为基于图的问题。在本小节中，作者将介绍各种已成功应用于这些方法的基于图的经典算法。具体地说，作者将首先简要说明一些具有代表性的基于图的算法及其在NLP领域的应用。然后进一步讨论了它们与GNN的联系。\n 随机行走算法\n**方法：**随机游走是一类基于图的算法，它可以在图中产生随机路径。为了进行随机行走，可以从图中的任何节点开始，并根据特定的转移概率反复选择访问一个随机的相邻节点。所有随机行走的访问节点然后形成一个随机路径。随机游走收敛后，可以获得一个平稳分布在所有的节点，可以用来选择最突出的节点图高结构的重要性排名概率分数或测量两个图的关系通过计算两个随机游走分布之间的相似性。\n**应用：**随机游走算法已被应用于各种NLP应用中，包括文本语义相似性的度量（Ramage et al., 2009）以及语义网络上的语义距离（Hughes and Ramage, 2007），词义消歧（Tarau et al., 2005），名称消歧（Minkov et al., 2006），查询扩展（Collins-Thompson and Callan, 2005），关键词提取（Mihalcea and Tarau, 2004），以及跨语言的信息检索（Monz and Dorr, 2005）。例如，给定一个语义网络和一个单词对，Hughes and Ramage (2007)等人使用随机游走算法计算了一个特定单词的平稳分布，并将两个单词之间的距离作为图上随机游走分布之间的相似度来测量，对给定单词对中的每个输入单词都有偏差。为解决电子邮件数据上的名称消歧任务，Minkov et al. (2006)从大量邮件中建立了一个特定于电子邮件的项目的图（例如发送方、接收方、主题），并提出了一种“lazy”主题敏感的随机游走算法，这就引入了随机游走在给定节点上停止的概率。给定一个电子邮件图和一个在输入电子邮件中出现的模糊名称，就会对给定电子邮件的文本进行随机游走，通过选择收敛后在平稳分布中得分最高的人节点，将名称解析为正确的引用。要解决关键字提取任务，Mihalcea and Tarau (2004)提出了对单词的共同出现图执行随机游走，并根据单词在平稳分布中的概率分数对文本中单词的重要性进行排序。\n 图聚类算法\n**方法：**常见的图聚类算法包括谱聚类、随机游走聚类和min-cut聚类。谱聚类算法利用图的拉普拉斯矩阵的频谱（特征值），在使用K-means等现有算法进行聚类前进行降维。随机漫步聚类算法通过在图上进行t步随机漫步来操作，因此，每个节点被表示为一个概率向量，表示图中所有其它节点的t步生成概率。基于图聚类的目的，小的t值是更可取的，因为想捕捉局部结构信息而不是全局结构信息。min-cut算法也可用于将图划分为多个簇。\n**应用：**图聚类算法已成功地应用于解决文本聚类任务。例如，Erkan (2006)提出利用有向生成图(包含n个文档节点)上的t步随机游走得到的n-dim概率分布作为语料库中每个文档的向量表示。然后，图形聚类算法可以使用这些文档表示来生成文档集群。\n 图匹配算法\n**方法：**图匹配算法的目的是计算两个图之间的相似度。Graph Edit Distance 是最常用的测量两个图的不相似性的方法。它将距离计算为将一个图转化为另一个图所需的变化（即添加、删除、替换）的数量。\n**应用：**图匹配算法在文本推断任务中有所应用，该任务旨在决定一个给定的句子是否可以从文本中推断出来。例如，Haghighi et al. (2005)假设当假设图与文本图之间的匹配成本较低时，从文本中提取一个假设，从而应用图匹配算法来解决该问题。\n 标签传播算法\n**方法：**标签传播算法（LPAs）是一类基于图的半监督算法，它将标签从有标签的数据点传播到之前没有标签的数据点。LPA是通过在图上迭代传播和聚集标签来操作的。在每次迭代中，每个节点都会根据其邻居节点拥有的标签改变其标签。因此，标签信息会在图中扩散。\n**应用：**LPA已被广泛应用于网络科学文献中，用于发现复杂网络中的社区结构。在NLP的文献中，LPA已成功地应用于词义消除歧义（Niu et al., 2005）以及情感分析（Goldberg and Zhu, 2006）。这些应用通常集中在标记数据稀缺的半监督学习设置上，并利用LPA算法将标签从有限的带标记的例子传播到大量相似的未带标签的例子中，并假设相似的例子应该有相似的标签。\n 与GNN的局限性与联系\n**局限性：**首先，他们的表达能力有限。它们主要关注于捕获图的结构信息，但不考虑节点和边缘特征，这对许多NLP应用程序也非常重要。其次，传统的基于图的算法还没有一个统一的学习框架。不同的基于图的算法具有非常不同的属性和设置，并且只适用于某些特定的用例。\n**联系：**传统基于图的算法的上述局限性要求建立一个统一的基于图的学习框架，该框架对图结构和节点/边属性的建模具有强大的表达能力。GNN作为一类特殊的神经网络获得了越来越多的关注，它可以为任意的图结构数据建模。大多数GNN的变体可以被视为一个基于消息传递的学习框架。与传统的基于消息传递的算法（如LPA）不同的是，GNN的操作方式是通过几个神经层对节点/边特征进行转换、传播和聚合，从而学习更好的图表示。作为一个通用的基于图的学习框架，GNNs可以应用于各种与图相关的任务，如节点分类、链接预测和图分类。\n 图神经网络\n本章直接将详细阐述GNN的基础于方法。\n 基础\nGNN就是要学习每个节点的embedding，然后将节点嵌入聚合成图嵌入。节点嵌入可表示为：\n\n其中A是邻接矩阵，H(l−1)={h1(l−1)…hn(l−1)}∈Rn×dH^{(l-1)} = \\{ h_1^{(l-1)} \\dots h_n^{(l-1)}\\} \\in R^{n \\times d}H(l−1)={h1(l−1)​…hn(l−1)​}∈Rn×d表示GNN第l−1l-1l−1层的embedding，HlH^lHl就是更新后的节点嵌入，d是h维度，在上述公式中，ffilterf_{filter}ffilter​是图滤波器。每个具体模型的不同就是图滤波器的选择以及参数不同。图滤波器不会改变图结构，只是改进了节点嵌入。图滤波器层也被堆叠出L层，产生最终的节点嵌入。\n由于图滤波器不能改变图结构，因此受到CNN启发，引入了池化操作来汇聚节点嵌入。其目的就是把一个图及其节点嵌入作为输入，然后生成一个更小的图以及新图的节点嵌入。可表示为：\n\n\n 方法\n 图滤波器\n主要有四种图滤波器，spectral-based graph fifilters（谱图滤波器）, spatial-based graph fifilters（空间图滤波器）, attention-based graph fifilters（注意力图滤波器）and recurrent-based graph fifilters（循环图滤波器）。\n其中谱图滤波器是基于谱图理论，空间图滤波器则是利用图上的空间邻近节点嵌入，一些谱滤波器还可以转换成空间滤波器。注意力图滤波器是受到自我注意机制启发给相邻节点一个权重。循环图滤波器则引入了门控机制，模型的参数跨不同的GNN层共享。\n**spectral-based graph fifilters（谱图滤波器）：**经过发展，最终表达式如下\n\n\nspatial-based graph fifilters（空间图滤波器）：类似于CNN的卷积操作，一方面基于节点的空间关系操作图卷积，即空间图滤波器将目标节点与邻居节点的进行卷积，推导出目标节点的更新表示。另一方面，基于空间的图滤波器也有信息传递的idea，其本质就是沿着边传递节点信息。这里介绍了两种方法：MPNN（Gilmer et al., 2017）、GraphSage（Hamilton et al., 2017a）。\n**MPNN：**这里定义了由fUf_UfU​和fMf_MfM​组成的复合函数，将图卷积视为一个消息传递过程，如运行K步消息传递迭代，将信息传播到K-hop邻近节点。\n\n其中，hi0=x0h_i^{0} = \\rm{x}_0hi0​=x0​，fUf_UfU​和fMf_MfM​分别是具有可学习参数的更新以及消息聚合函数，在推导出每个节点的隐藏表示之后，hi(L)h_i^{(L)}hi(L)​(L是卷积层数)可以传递到输出层进行节点级别的预测任务，或者读出函数执行图级别的预测任务。\n**Graphsage：**由于一个节点的邻域可多可少，所以在一个包括千万多节点的图中计算一个节点的所有邻居是低效的，因此Graphsage采用采样的方式来获得每个节点固定数量的邻居。\n\n其中，N(vi)是节点vi的相邻节点的一个随机样本。聚合函数可以是任何对节点排序的排列不变的函数，如均值、和或最大运算。\nattention-based graph fifilters（注意力图滤波器）：原始版本的GNN边连接时固定的，在图学习过程中也不动态调整连接信息。所以受到Transformer中multi-head注意力机制的启发，提出了Graph Attention Network（GAT），其通过在GNN体系结构中引入多头注意机制，该机制能够在执行消息传递时动态地学习边缘上的权重（即注意分数）。更具体地说，对图中每个目标节点对相邻节点的嵌入进行聚合时，多头注意机制考虑目标节点与每个相邻节点之间的语义相似度，在进行邻域聚合时，对重要的相邻节点分配较高的注意分数。对于第lll层，GAT因此使用了以下注意机制的公式：\n\n其中，u⃗(l)\\vec{u}^{(l)}u(l)和W⃗(l)\\vec{W}^{(l)}W(l)分别为第lll层的权值向量和权值矩阵，||为向量串联运算。请注意，N(vi)是vi的1跳邻域，包括它本身。在获得每对节点vi和vj的注意力得分αij后，更新后的节点嵌入可以计算为输入节点特征的线性组合，然后是一些非线性σ，表示为：\n\n后来又采用了多种独立的自我注意机制，将其输出连接起来，产生以下节点嵌入：\n\n而最后的GAT层(即带有L层的GNN的第L层)使用平均而不是连接来组合多头注意力输出。\n\nrecurrent-based graph fifilters（循环图滤波器）：基于循环的图滤波器的一个典型例子是门控图神经网络(GGNN)滤波器。从典型的GNN到GGNN，最大的修改是使用门控循环单元(GRU），GGNN-滤波器还考虑了边类型和边方向。基于循环的滤波器在GGNN中的传播过程可以总结如下：\n\n其中A∈Rdn×2dnA \\in \\mathbb{R}^{dn \\times 2dn}A∈Rdn×2dn是一个决定图中节点如何相互通信的矩阵，n是节点个数。Ai∈Rd×2dA_i \\in \\mathbb{R}^{d \\times 2d}Ai​∈Rd×2d是A中对应节点viv_ivi​的两列模块。第一个公式初始节点特征xi被填充了额外的零，以使输入的大小等于隐藏的大小。\n 图池化\n图池化的目的就是生成针对图的下游任务生成图级别的表示，如果是以节点为中心的任务那么生成节点embedding就够了，可是如果是以图为中心的任务，那么就需要对图进行表示。图池化层可分为两类平面图池化和分层图池化，平面图池化是一个步骤直接由节点嵌入生成图级别的表示。层次图池化包含几个池化层，每个池化层都有一个图滤波器的堆栈。\nFlat Graph Pooling（平面图池化）：理想情况下，聚合函数对其输入的排列是不变的，同时也要保证其表达能力。图池化操作一般就是最大池化或者平均池化，另一种比较流行的方法是遵循全连接层的转换实现最大或平均池化的变体，可以表示为：\n\n其中i是节点嵌入的第i个通道，H:,i∈Rn×1H_{:,i} \\in R^{n \\times 1}H:,i​∈Rn×1是一个向量，W是一个表示FCmax池化层的可训练参数矩阵，RiR_iRi​是一个标量，最终的图嵌入R=[r1,r2⋯ ,rn]TR = [ r_1,r_2 \\cdots ,r_n]^TR=[r1​,r2​⋯,rn​]T。一个强大但不常见的池化操作是BiLSTM聚合函数。\nHierarchical Graph Pooling（层次图池化）：该方法逐步压缩图，以学习图级别的嵌入，大致可分为两类。一类是分层池化层通过将最重要的节点作为粗化图（coarsen graph）的节点进行子采样。另一种是将节点组合成supernodes，然后将supernode作为粗化图的节点。可以概括为：（1）为粗化图生成图结构；（2）为粗化图生成节点特征。\n\n其中A是输入的邻接矩阵，A’是粗化后的邻接矩阵。\n NLP中的图构造方法\n上文介绍的是输入是图的时候的GNN方法，但是NLP输入的基本不会是图，所以将介绍静态图构造和动态图构造。\n 静态图构造\n静态图构造就是在对原始输入的文本序列进行预处理时构造图结构。静态图包含了隐藏在原始文本中的不同领域/外部知识，用丰富的结构化信息增强了原始文本。文中将构造静态图的方法分了11类，假设一个文档由n个paragraphs组成，即doc={para1⋯paran}doc = \\{para_1 \\cdots para_n  \\}doc={para1​⋯paran​}，然后一个paragraph又由m个sentences组成，一个sentence又由lll个words组成。\n 静态图构造方法\n Dependency Graph Construction（依赖关系图构造）\n\n依赖关系：给定特定段落中的句子，获得每个句子的依赖关系解析树（dependency parsing tree），这里可以通过使用NLP解析工具（例如Stanford CoreNLP）。依赖关系表示为(wi,reli,j,wj)(w_i,rel_{i,j},w_j)(wi​,reli,j​,wj​)，其中wi，wj是有一个边类型reli,jrel_{i,j}reli,j​连接的两个word节点。然后定义一个关系集RdepR_{dep}Rdep​。\nSequential Relations（顺序关系）：顺序关系对原始段落中的相邻关系进行编码，即对于依赖图的构造，定义了顺序关系集Rseq⊆V×VR_{seq} \\subseteq V \\times VRseq​⊆V×V，其中，V是基本元素（单词）集合。那么对于每个序列关系(wi,wi+1)∈Rseq(w_i,w_{i+1}) \\in R_{seq}(wi​,wi+1​)∈Rseq​，就意味着wiw_iwi​在给定的段落中与wi+1w_{i+1}wi+1​是相邻的。\n依赖图：给定一个片段para，有它的RdepR_{dep}Rdep​和RseqR_{seq}Rseq​，那么就可以构建图，假设vi,vjv_i,v_jvi​,vj​分别对应wi,wjw_i,w_jwi​,wj​，然后可根据RdepR_{dep}Rdep​构建两个节点间边的类型，然后也要根据RseqR_{seq}Rseq​构建两个节点之间是否有sequential类型的边。\n\n Constituency Graph Construction（选区图构造）\n依赖图关注的是单词与单词之间的关系，而选区图则是短语级别的。\n\n选区关系：选取关系遵循短语结构以及语法的关系，一般来说，选取关系来源于主语（名词短语NP）-谓语（动词短语VP）关系。同时，选取解析树还能区分终端节点和非终端节点。\n选区图：与依赖关系图类似，该图有非终端节点VntV_{nt}Vnt​以及终端节点VwordsV_{words}Vwords​，然后也给定选区边和顺序边。\n\n AMR Graph Construction（AMR图构造）\nAMR图是有根的、标记的、有向的、无环图，被广泛用于表示非结构化和具体自然文本的抽象概念之间的高级语义关系。与句法特性不同，AMR是一种高级的语义抽象。更具体地说，语义相似的不同句子可能共享相同的AMR解析结果，与之前引入的依赖树和选区树类似，AMR图是从AMR解析树派生出来的。\n\nAMR关系，这里有两类节点，一种是名称节点，它是节点样例的具体值；另一种概念要么是英语单词比如说（“boy”），要么是PropBank框架（例如“want-01”），要么是特殊关键词。名称节点是唯一的标识，而概念节点则由不同的实例所共享，节点间的边关系记作(ni,ri,j,nj)∈Ramr(n_i,r_{i,j},n_j) \\in R_{amr}(ni​,ri,j​,nj​)∈Ramr​.\nAMR图，与上一个类似，对于每一个(ni,ri,j,nj)∈Ramr(n_i,r_{i,j},n_j) \\in R_{amr}(ni​,ri,j​,nj​)∈Ramr​，先添加节点viv_ivi​对应nin_ini​，vjv_jvj​对应njn_jnj​。然后添加一条边类型ri,jr_{i,j}ri,j​。\n\n Information Extraction Graph Construction（信息提取图构造）\n信息提取图(IEGraph)旨在提取结构信息来表示自然句子之间的高级信息，如基于文本的文档。\n\n\n共指消解（coreference resolution），目的是找到在文本序列中引用相同实体的表示（expressions），例如下图中，名称“Pual”、名词术语“He”和“ a renowned computer scientist”可以指同一个物体（人）。\n \n许多NLP工具，如OpenIE提供了共同参考分辨率功能来实现这一目标。我们将共引用簇C表示为引用同一对象的一组短语。给定一段，可以得到从非结构化数据中提取的共引用集C={C1、C2、…，Cn}。\n\n\nIE关系，首先用信息提取系统提取三元组（如OpenIE），每个三元组（主语、谓语、宾语）为一个关系。它表示为(ni,ri,j,nj)∈Rie(n_i,r_{i,j},n_j) \\in R_{ie}(ni​,ri,j​,nj​)∈Rie​，如果说两个三月组只有一个参数不一样，其他参数相同，那么只保留最长的那个。\n\n\nIE图构造，与上述构造方法大致相同，这里边是谓词类型的边。同时，对于每一个共指簇Ci∈CC_i \\in CCi​∈C，就可以把所有共指短语变成一个节点，这将大大减少节点数量，而且只保留一个节点来消除歧义。\n\n\n Discourse Graph Construction（语篇图构建）\n当候选文档太长时，许多NLP任务都会遭受长时间的依赖挑战。语篇图描述了两个句子是如何在逻辑上相互联系的，它被证明可以有效地应对这一挑战。\n\n语篇关系，语篇关系来源于语篇分析，目的是识别一组句子上的句子排序约束，给定两句句子senti,sentjsent_i,sent_jsenti​,sentj​，可以定义语篇关系(senti,sentj)(sent_i,sent_j)(senti​,sentj​)，表示句子sentjsent_jsentj​可以放在sentisent_isenti​之后。所以，对于给定的一个文档docdocdoc，可以先将文档分割为句子集V={sent1,sent2,…,sentm}V = \\{ sent_1,sent_2,\\dots,sent_m\\}V={sent1​,sent2​,…,sentm​}，然后应用语篇关系得到语篇关系集Rsep⊆V×VR_{sep} \\subseteq V \\times VRsep​⊆V×V.\n语篇图，同理，只不过这里是将句子当作节点，然后根据关系添加有向边。\n\n Knowledge Graph Construction（知识图构造）\n捕获实体和关系的知识图(KG)可以极大地促进许多NLP应用程序中的学习和推理。一般来说，kg根据它们的图构造方法可以分为两大类。1）将KG视为是非结构化数据（如文档）的小型的（compact）、可解释的中间表示，类似于IE图；2）合并现有知识库，以进一步提升下游任务性能。本文主要介绍第二种。\n形式上，也将三元组(e1,rel,e2)(e_1,rel,e_2)(e1​,rel,e2​)定义为知识库的基本元素。其中e1e_1e1​是源实体，e2e_2e2​是目标实体，relrelrel是关系类型。同样也是在KG图中添加节点v1,v2v_1,v_2v1​,v2​对应e1,e2e_1,e_2e1​,e2​，然后添加类型为relrelrel的一条有向边。\n\nKG图总是被视为输入的一部分，虽然它可以作为一种数据增强的方式，但并不适合整张图进行输入。相比之下，如上图人们通常从给定的查询构建子图。\n在构造时，举一个具有代表性的方法，首先时在给定的查询中获取terms实例，然后通过一些匹配算法，将其与KG图中的概念连接起来，下一步就是获取KG节点中的一跳邻居，当然也可以利用一些模型如PageRank等先计算一些节点与邻居节点的相关性等，然后去一些无用边。\n Coreference Graph Construction（共指图构造）\n在语言学中，如果一个给定的段落中的两个或多个术语引用同一对象，就会出现共同引用，这样有助于更好的理解语料库的复杂结构和逻辑，消除歧义。接下来将重点讨论由m个句子组成的段落对段的共引用图构造。虽然它类似于IE图的第一步，但共引用图将显式地通过图来建模共引用关系，而不是折叠成一个节点。\n\n共指关系，和IE图的一样，建立共指簇C。\n共指图建立在共指关系集RcorefR_{coref}Rcoref​上，根据节点类型可分为两类，1）短语；2)单词。每一类都是根据节点连接一条无向边，区别是如果节点是短语的话，连接的是第一个单词。\n\n Similarity Graph Construction（相似图构造）\n相似度图构造旨在量化节点之间的相似度，构建是在预处理的过程中进行的。\n\n相似度图，给定一个语料库C，图的节点可以被定义为不同的级别，比如实体，句子和文档等等。然后根据各种机制来计算节点特征，然后也可以通过余弦相似度等指标来计算节点之间的相似度，并用于表示节点对的边权值。\n稀疏机制，初始的相似度图很密集，很多权值非常小甚至会被认为是噪声，所以可以使用稀疏方法，要么保留K个最大的权值边，要么就是删掉小于特定阈值的边。\n\n Co-occurrence Graph Construction（共现图构造）\n共现图旨在捕捉文本中单词之间的共现关系,共现关系是指在一个固定大小的上下文窗口中，两个单词同时出现的频率，它是捕捉语料库中单词之间语义关系的一个重要特征。\n\n共现关系，由给定的语料库C的共现矩阵定义，对于一个特定的段落para由m个句子组成，共现矩阵描述了单词是如何同时出现的。可以将矩阵共现表示为M∈R∣V∣×∣V∣M \\in \\mathbb R ^{ |V| \\times |V|}M∈R∣V∣×∣V∣，其中∣V∣|V|∣V∣是C的词汇表大小。Mwi,wjM_{w_i,w_j}Mwi​,wj​​描述了在单词wi,wjw_i,w_jwi​,wj​在语料库C中的固定窗口滑动时，共同出现的次数。获得共现矩阵之后可以有两种方法计算单词之间的权重，1）共现频率；2）PMI。\n共现图，已知节点和边，构造方法同上。\n\n\n Topic Graph Construction（主题图构造）\n主题图建立在多个文档之上，旨在建模不同主题之间的高级语义关系。\n\nApp-driven Graph Construction（应用驱动图构造）\n应用程序驱动的图指的是专门为特定的NLP任务而设计的图，在一些NLP任务中，通常使用特定于应用程序的方法通过结构化形成来表示非结构化数据。例如SQL语言可以用SQL解析树来表示，进而转换成SQL图。因此将在后面将讨论图构造方法如何应用于流行的NLP任务。\n","categories":[],"tags":[]},{"title":"图论基础","url":"/2022/03/10/book-mayao-02-md/","content":"图的表示图（Graph）$\\textcolor{blue} {\\textbf { 定义： } }$一个图可以表示为$G = { {V,\\varepsilon }}$，其中$V$ = {$v_1$,$\\cdots$,$v_N$}是大小为$N = | V |$ 的节点集合，$\\varepsilon$={ $e_1$,$\\cdots$,$e_M$}是大小为$M$的边的集合。\n\n如上图（图1），为一个有五个节点和六条边的图。其中{ $v_1$,$\\cdots$,$v_5$}为五个节点，{ $e_1$,$\\cdots$,$e_6$}为六条边。\n在图中，如果有一条边$ei$连接两个节点$v{ei}^1$和$v{ei}^2$，那么这条边可以表示为（$v{ei}^1$,$v{ei}^2$），在有向图中表示边从起点$v{ei}^1$指向终点$v{ei}^2$。相反在无向图中由于没有顺序之分，则$e_i$ = （$v{ei}^1$,$v{ei}^2$）= （$v{ei}^2$,$v{e_i}^1$）。举个例子，如图一中的边$e_6$连接节点$v_1$与节点$v_5$，那么由于图1为无向图，则$e_6$也可以表示为（$v_1$,$v_5$）或（$v_5$,$v_1$）。\n邻接矩阵（Adjacency Matrix）为了方便查看节点之间的连接关系，图$G = { {V,\\varepsilon }}$可以等价的表示为邻接矩阵的形式，更加直观的描述节点之间的关系。\n$\\textcolor{blue} {\\textbf { 定义： } }$给定一个图$G = { {V,\\varepsilon }}$,对应的邻接矩阵可以表示为$A∈{ { 0,1 } ^ { N×N } }$。邻接矩阵$A$的第$i$行第$j$列元素$A{i,j}$表示节点$v_i$和$v_j$的连接关系。具体来讲，如果$v_i$与$v_j$相邻，则$A{i,j} = 1$，否则$A_{i,j} = 0$。\n在无向图中，由于不受节点顺序影响，则$A{ i,j } = A{ j,i }$，所以无向图的邻接矩阵一定是$\\textbf { 关于主对角线对称 }$的。以图1为例，该图的邻接矩阵可表示为：\n\nA = \n\\begin {pmatrix}\n0 & 1 & 0 & 1 & 1 \\\\\n1 & 0 & 1 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 1 \\\\\n1 & 0 & 0 & 0 & 1 \\\\\n1 & 0 & 1 & 1 & 0 \\\\\n\\end {pmatrix}可以看到，主对角线值均为0，且都关于其对称。\n图的性质度（Degree）$\\textcolor{blue} {\\textbf { 定义： } }$在图$G = { {V,E }}$中，节点$v_i ∈ V$的度定义为图$G$中与节点$v_i$相关联的边的数目。\n\nd(v_i) = \\sum_{ v_j ∈ V}1\\varepsilon( \\{ v_i,v_j \\} )其中，$1\\varepsilon(\\cdot)$为指示函数，简单来说就是满足条件就取1，不满足就取0。对于图来讲可表示为：\n\n1\\varepsilon( \\{ v_i,v_j \\} ) = \n\\begin {cases}\n1, \\quad (v_i,v_j)∈\\varepsilon \\\\\n0, \\quad (v_i,v_j)\\notin\\varepsilon\n\\end {cases}即当存在$(v_i,v_j)$这条边时，$d(v_i)$就加1，反之如果不存在，就加0。其实最直观的看法就是找从这个节点出发引出几条边。但这种方法会有一些麻烦，所以如果能够画出图的邻接矩阵的话，可以更简单的计算结果，利用邻接矩阵可以将节点的度表示为：\n\nd(v_i) = \\sum_{ j = 1 }^N A_{ i,j }简单来说就是计算该节点所在行所有值的和。任以图一为例，节点$v_5$与$v_1,v_3,v_4$相连，则节点$v_5$的度为3。如果利用邻接矩阵计算，$v_5$占矩阵第五行，直接将改行所有元素相加，即可得到其度为3。\n邻域（Neighborhood）$\\textcolor{blue} {\\textbf { 定义： } }$在图$G={ V,\\varepsilon }$中，节点$v_i$的邻域$N(v_i)$是所有和它相邻的节点的集合。\n依然以图1中$v_5$节点为例，其邻域为$N(v_5) = { v_1,v_3,v_4 }$，可以发现对于任意节点$v_i$,其$\\textbf{ 邻域中元素个数等于该节点的度 }$，即$d(v_i) = |N(v_i)|$。\n$\\textcolor{green} {\\textbf { 定理： } }$一个图$G={ V,\\varepsilon }$中所有的节点的度之和是图中边数量的两倍：\n\n\\sum_{ v_i ∈ V}d(v_i) = 2\\ \\cdot \\ | \\varepsilon |这个定理很容易想通，举一个例子，如下图（图2），$v_1与v_2$的度都为1，度之和为2。但两个节点公用一条边$e_1$，则是2倍的关系。\n\n同样的，如果将图写为邻接矩阵的话，每一个非零元素则代表两个节点存在连接，第$i$行中非零元素的个数就是节点$v_i$的度，那么矩阵中所有非零元素的个数就是所有节点的度之和，那么可以有如下推论。\n$\\textcolor{green} {\\textbf { 推论： } }$无向图邻接矩阵的非零元素的个数是边的数量的两倍。\n连通度（Connectivity） $\\textcolor{blue} {\\textbf { 定义： } }$途径（Walk）图的途径是节点和边的交替序列，从一个节点开始，以一个节点结束，其中每条边与紧邻的节点相关联。\n $\\textcolor{blue} {\\textbf { 定义： } }$迹（Trail）是边各不同的途径。\n $\\textcolor{blue} {\\textbf { 定义： } }$路（Path）是节点各不相同的途径，也称路径\n简而言之，只要不跳着走，那从一个节点到另一个节点就是一个途径，在途径这个大类中，如果序列中没有重复边就是迹，没有重复节点就是路。举个例子，图1中想从$v_1$走到$v_2$，假设绕远走，那么可以选择$(v_1,e_4,v_4,e_5,v_5,e_6,v_1,e_1,v_2)$这条途径，那么这条途径也是一条迹但不是路，因为这条途径中没有重复的边，但存在重复的节点$v_1$。如果选$(v_1,e_1,v_2)$这条途径，那么这即可以成为是迹，也可以称为是路径。\n$\\textcolor{green} {\\textbf { 定理： } }$对于图$G = { { \\varepsilon,V } }$及其邻接矩阵$A$，用$A^n$表示该邻接矩阵的n次幂。那么$A^n$的第$i$行第$j$列的元素等于长度为$n$的$v_i - v_j$的途径的个数。\n这条定理看似很复杂，但功能很简单，举例如图1，假设从$v1-v_4$，如果有长度为6的途径的走法，问一共有几种。那么答案就是$A^6{ 1,4}$。\n书中的证明方法用的是数学归纳法，那么我们可以用线性代数的方式以及图论的方式来解释一下书中的证明过程。首先假设当$n = k$时，$A^k_{i,j}$等于长度为$k$的$v_i-v_j$的途径的数量。那么我们只需要证明：\n\nA^{k+1}_{i,j} = \\sum^N_{h=1} \\ A^k_{i,h}\\  \\cdot \\ A_{h,j}对于式中的h代表的是节点，那么上面这个公式就是一个基础的线性代数的计算公式，那么如何用图论来解释呢？\n\n如上图（图3），假定要从$v_1$分6步走$v_4$，按照公式，第六步的方式要与前五步的走法相关，那么我们空出第6步，先只看前5步：\n\n如上图（图4），左侧部分为前5步从$v1$走到各个点的方式，也就是对应的$A^k{i,h}$，在这里就是$A^5{1,h} (h = 1 \\cdots4)$。那么最后一步就是从这四个点到$v_4$的走法，分别对应的就是$A{h,4}$,将每一个节点的结果都加和，就是最终的结果，这个证明就同时结合了线代和图论的知识。\n $\\textcolor{blue} {\\textbf { 定义： } }$子图（Subgraph），图$G={ V,\\varepsilon }$的子图$G’={ V’,\\varepsilon’ }$由节点集合的子集$V’ \\subset V$和边集的子集$\\varepsilon ’ \\subset \\varepsilon$组成，此外，集合$V’$必须包含集合$\\varepsilon’$涉及的所有节点。\n举个例子，如图3中节点子集${ v_2,v_3,v_4 }$和边子集${ e_2,e_3,e_4 }$就构成原图的一个子图。\n $\\textcolor{blue} {\\textbf { 定义： } }$连通分量（Connected Component），给定一个图$G = { V,\\varepsilon }$，如果一个子图$G’={ V’,\\varepsilon’ }$中任意一对节点之间都至少存在一条路，且$V’$中的节点不与任何$V/V’$中的节点相连，那么$G’$就是一个连通分量。\n如下图（图5），就是一个含有两个连通分量的图，因为其左右两边没有连接。\n\n$\\textcolor{blue} {\\textbf { 定义： } }$连通图（Connected Graph），如果一个图$G = { V,\\varepsilon }$只有一个连通分量，那么$G$是一个连通图。\n如图1为连通图，图5就不是连通图。\n$\\textcolor{blue} {\\textbf { 定义： } }$最短路（Shortest Path），两节点中距离最小的路（注意这里是路，即不允许有重复节点），任意两节点间的最短路可以有多条。图中节点$vs - v_t$间的最短路可表示为$P{st}$。\n$\\textcolor{blue} {\\textbf { 定义： } }$直径（Diameter），图中所有节点间最短路长度的最大值就是该图的直径。\n中心性度中心性（Degree Centrality）如果很多节点都连接到某一个节点，那么这个节点就被认为是重要的。简单来说，相当于如果很多人都说你厉害，那么可以认为你很厉害。表示为：\n\nc_d(v_i) = d(v_i) = \\sum^N_{j=1}A_{i,j}如图1中，节点$v_1,v_5$的度中心性都是3，节点$v_2,v_3,v_4$的度中心性为2。\n特征向量中心性（Eigenvector Centrality）与度中心性想法不同，度中心性认为每一个相邻节点的贡献是相同的，但特征向量则认为每一个节点的重要性是不同的。就好像，5个菜鸟说你厉害，那你也不一定厉害。但如果是5个大佬都说你厉害，那你肯定是很厉害。可以表示为：\n\nc_e(v_i)= \\cfrac{1}{\\lambda}\\sum^N_{j=1}A_{i,j} \\ \\cdot \\ c_e(v_j)这里特征向量与特征值需要用到一些线代的知识，其实就是要找到最大的特征值并且找到其对应的特征向量。以图1及其邻接矩阵为例，在计算方面，借助python中的networkx库进行计算，下面代码可供参考：\nimport numpy as np\nimport pandas as pd\nimport networkx as nx\nimport matplotlib.pyplot as plt\nedges = pd.DataFrame()\nedges['sources'] = [1, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 5]\nedges['targets'] = [2, 4, 5, 1, 3, 2, 5, 1, 5, 1, 3, 4]\nedges['weights'] = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\nG = nx.from_pandas_edgelist(edges, source='sources', target='targets', edge_attr='weights')\nprint('特征向量中心性: ', nx.eigenvector_centrality(G))  # 特征向量中心性\nnx.draw(G, with_labels=edges['sources'].tolist())\nplt.show()\n计算后会发现，虽然$v_2,v_3,v_4$的度都为2，但在特征向量中心性的计算结果中$v_4$要高于另外两个节点，因为它与两个更高的邻居$v_1,v_5$直接相连。\nkatz中心性（Katz Centrality）但当出现在有向无环图时，其中节点 eigenvector centrality变成0，因此Katz提出了一个改进方法，即每个节点初始就有一个centrality值，表示为：\n\nc_e(v_i)= \\alpha\\sum^N_{j=1}A_{i,j} \\ \\cdot \\ c_k(v_j) + \\beta其中，$\\alpha$尽量取在$(0,\\cfrac{1}{\\lambda_{max}})$节点越重要$\\beta$的值越大，可以理解为，不仅别人说你厉害，你自己本身越厉害，值也就越高。\n介数中心性（Betweenness Centrality）当多个节点间的最短路都通过某一节点，则认为这个节点很重要。其实和交通枢纽很类似，可以想成，如果很多人都通过你和别人联系上，那你就很重要。可表示为：\n\nc_b(v_i) = \\sum_{v_s \\neq v_i \\neq v_t} \\cfrac{\\sigma_{st}(v_i)} {\\sigma_{st}}其中$\\sigma{st}$表示从节点$v_s$到节点$v_t$的最短路的数目，$\\sigma{st}(vi)$表示这些路中经过节点$v_i$的路的数目。如图1，节点$v_1$的介数中心性为$\\cfrac{3} {2}$，计算过程如下为，首先排除节点$v_1$，然后计算其余的节点间最短路中通过节点$v_1$的个数。通过观察，可以发现节点$v_2$通过节点$v_1$到达节点$v_4$，且节点$v_2$到达节点$v_4$只有一条最短路，则此时$\\cfrac{\\sigma{2,4}(v1)} {\\sigma{2,4}} = 1$,同时节点$v_2$也通过节点$v_1$到达节点$v_5$，但节点$v_2$到达节点$v_5$有两条最短路\n则此时$\\cfrac{\\sigma{2,5}(v_1)} {\\sigma{2,5}} = \\cfrac{1} {2}$，然后就没有除节点$v_1$的其他节点的最短路了，那么节点$v_1$的介数中心性为$1+\\cfrac{1} {2} = \\cfrac{3} {2}$。但是不同的图这个介数中心性的值没有比较的意义，因此要对它进行一个归一化（normalization）。公式为：\n\nc_{nb} = \\cfrac{2 × \\sum_{v_s \\neq v_i \\neq v_t} \\cfrac{\\sigma_{st}(v_i)} {\\sigma_{st}}} {(N-1)(N-2)}那么可以计算节点$v_1$的归一化介数中心性为$\\cfrac{1} {4}$。\n","categories":["马耀汤继良-笔记"],"tags":["GNN","笔记","教材"]},{"title":"Interactive Path Reasoning on Graph for Conversational Recommendation","url":"/2022/03/10/CRS-CPRonGraph/","content":"摘要传统推荐系统评估用户偏好是利用用户的历史交互，因此受获取的细粒度以及动态的用户偏好所限制。CRS 利用直接询问用户在项目上偏好的属性，然而现有的这些CRS方法并不能充分利用其优势，他们只用一些非常隐式的方法来使用用户的属性反馈。本文中作者提出一种会话路径推理（CPR）的框架，将会话推荐建模为一个图上的交互路径推理问题。它可以遵循用户的反馈，用一种显式的方式利用用户首选的属性，遍历属性节点。通过利用图结构，CPR能够删除许多不相关的侯选属性，从而有更好的机会命中用户的偏好属性。\n作者通过实验得出，属性越多他们方法的优势越大。\n引言传统方法虽然被证明是成功的，但在推荐过程中被动获取用户反馈的内在局限性。这种信息不对称使得很难获得动态和细粒度的用户偏好，使得系统无法提供准确、可解释的推荐服务。本文认为 CRS 由两个组件分别是一个会话系统和一个推荐系统。由于其使用自然语言与用户交互，并且还可以主动询问用户是否喜欢项目的属性，因此其信息是动态且可解释的。然而，现有的工作只是通过将属性映射到一个潜在的空间来隐式地利用属性反馈，文章认为这并没有充分利用属性反馈的优势，所以作者所提出的 CPR 框架就是希望结果是更准确，且可解释。\n受到图推荐的启发，作者将会话推荐建模为用户-项目-属性图中交互寻路的过程，如下图。\n\nCPR将对话表现为图表中的行走。它从用户顶点开始，在图中移动，其目标是到达用户喜欢的作为目标的一个或多个项目顶点。但要注意的是图中每一步都是按照用户在对话中的指示走。如果用户确认了对被询问属性有偏好，则系统将传输到该属性的顶点。但是，如果用户拒绝该属性，或拒绝推荐，系统将保持在同一个顶点，并向用户咨询其他属性。会话将重复多次这样的循环，直到推荐的项目被用户接受为止。\n作者认为所提出的CPR框架作为一种进行对话推荐的新角度，在概念上为CRS的发展带来了一些优点：\n\n它是透明可解释的，它将会话推荐建模为图上的一个交互式路径推理问题，每一步都由用户确认，生成的路径是推荐的正确原因。\n通过引入图的结构，它促进了对大量丰富的信息的利用。同时通过限制候选节点的询问作为当前节点的相邻属性，候选空间就会大大减少。\n在图上行走的路径为对话系统提供了一种自然的对话追踪，同时也能够直接征求用户反馈，以尽快删除无用分支。\n\n作者认为，他们的研究贡献有如下两个：\n\n提出CPR框架将会话推荐作为异构图上的路径推理问题，为构建CRS提供了一个新的角度。\n简单化了一个实例SCPR，属性空间越大，模型能达到的改良越多。\n\n相关工作推荐系统的成功取决于准确和及时地提供用户感兴趣的相关项目。最初，推荐系统主要建立在协同过滤假设（collaborative filtering hypothesis）之上，以推断用户配置文件的分布式表示，有代表性的模型包括矩阵分解（matrix factorization）和分解机（factorization machines）。然而，从本质上讲，这些方法存在两个内在的问题。第一个问题是由于用户的兴趣并不是长期静止的，因此不能长期用静态假设用户的动态偏好。第二个问题是较弱的可解释性，因为用户偏好表示只是一个连续的向量。\n近年来随着图推荐方法的研究，有一些人开始探索隐式属性，比如来自全局协作信号，有些专注于合并潜在的网络嵌入来产生更好的用户/项目表示。还有一批人利用了图的可解释性，将推荐建模为图上的路径推理问题，这样就能找到推荐原因。然而还是存在了问题：\n\n仍然是静态模型，不能获取动态偏好。\n建模的复杂程度很高，剪枝也就成了一个关键步骤。\n\n随着 CRS 的引入，通过动态获取用户的显式反馈，成为了解决动态偏好和若可解释性的方案。\n作者认为如何动态地提出属性问题并对属性答案给出推荐是当前阶段会话推荐的关键。因此，作者考虑系统的用户偏好属性和基于这些属性提出建议。如第1节中所讨论的，这一行的主要工作不显式地使用属性。我们认为，更明确地利用该属性将更好地推进会话推荐的优势。\n多轮会话推荐场景本文遵循多轮会话推荐(MCR)场景，因为它是迄今为止研究中最现实的场景，一轮就是一次推荐实验。这与所采用的单轮会话推荐形成对比，即系统多次询问属性，然后只提出一次建议，之后无论推荐是否成功，会话都会结束。当CRS有更多自由去做那些让策略空间更复杂的动作时，多轮设置会比单论设置更具备挑战。\n每一个项目 $v$ 都会有其一组属性 $P_v$ ，只要这个属性能描述这个物品，那就可以是这个项目的属性，因此其属性是广泛的。在对话过程中，CRS通过询问用户是否喜欢特定的属性来获得用户的细粒度偏好。基于这样的对话，CRS旨在在最短的对话回合中提供准确的推荐。\n 那么具体流程，就是首先系统先指定一些属性给用户初始化属性 $p0$,然后自由的从候选属性集 $p { cand } $中询问或是在候选项 $V  { cand }$ 中给出推荐。如果用户接受了属性或是物品，那么进行更新，首先将同意的属性加入到用户的偏好属性集合 $ p_u $ 中，并且将该属性从 $p{cand}$中删除，最后将包含该属性的项目集$Vp$加入到候选项目集合$V{cand}$中；如果用户拒绝，那么在候选项目集合中删除$V_p$。更新所有集合后，系统会根据需求重复上述过程，直到达到最大轮数 T 或是用户满意。具体算法如下：\n\n上述的MCR场景做出了两个假设\n\n假定用户毫无保留的将自己的偏好都交代了，因此只考虑首选项的项目集就足够了，这是一个合理的简化。\n\n\n提出方法作者首先提出了会话路径推理(CPR)，这是一个基于图的会话推荐的通用解决方案框架。然后，引入一个简单而有效的实例化SCPR来演示它是如何工作的。\nCPR框架","categories":["论文解读"],"tags":["论文","CRS"]},{"title":"Advances and Challenges in Conversational Recommender Systems--A Survey","url":"/2022/01/10/paper-CRS-servey/","content":" 摘要\n推荐系统利用用户的交互历史来估计用户的偏好，当前广泛使用的是静态推荐模型，但由于静态模型没有明确的指令以及它无法从用户处获得主动反馈，因此其很难解决两个重要问题：\n​\t（a）用户到底喜欢什么？\n​\t（b）为什么用户会喜欢这件物品？\n最近，会话推荐系统（CRSs）的兴起改变了上述的情况，用户与系统可以通过自然语言的交互进行动态通信。\n现有的CRS仍未成熟，作者从五个方向总结出开发CRSs的关键挑战：\n​\t（1）Question-based user preference elicitation（基于问题的用户偏好启发）\n​\t（2）Multi-turn conversational recommendation strategies（多轮会话推荐策略）\n​\t（3）Dialogue understanding and generation（对话的理解与生成）\n​\t（4）Exploitation-exploration trade-offs（探究与开发权衡）\n​\t（5）Evaluation and user simulation（评估与用户模拟）\n这些研究领域涉及到信息检索（IR）、自然语言处理（NLP）以及人机交互（HCI）等多个领域研究。\n 引言\n静态推荐模型（传统），主要通过分析用户过去的离线行为来预测偏好。例如用户的点击历史、访问日志、对物品的评价等。早期使用的方法，如协同滤波（CF）、逻辑回归（LR）、因素分解机（FM）和梯度增强决策树（GBDT），由于其有效性以及可解释性。已经在实际应用中广泛使用。当前，已经开发出更多复杂但更有效的神经网络，如Wide&amp;Deep、神经协同过滤（NCF）、深度兴趣网络（DIN）、基于树的深度模型（TDM）以及图卷积网络（GCNs）。\n 静态推荐模型的缺点\n静态推荐模型以用户历史行为为数据进行离线训练。然后用结果为用户提供在线服务。尽管其被广泛使用，但是依然未解决两个问题。\n 用户到底喜欢什么？\n静态模型的学习过程是基于用户历史数据，那么这些数据是会存在噪声，同时也可能是稀疏的。为什么会这样？\n（1）静态模型的一个基本假设就是，所有的历史交互都代表了用户的偏好。\n（2）用户可能做出过错误的决定。\n（3）用户偏好会随着时间推移（之前喜欢不代表现在喜欢）。\n（4）存在新用户或者活跃度低的用户，没有历史数据的支持。\n 用户为什么喜欢这件物品？\n在现实生活中影响用户决定的因素有很多，比如一个用户购买一件商品有可能是出于好奇或是受他人影响，也可能是深思熟虑过后才购买。不同的用户购买相同商品很可能动机不同，因此用相同标准看待不同的用户或者用相同标准看待同一用户的不同活动，是不适合推荐的。静态模型很难理清用户消费行为背后的不同原因。\n尽管人们已经做了很多努力来消除这些问题，但他们做出的假设是有限的。例如，一个常见的设置是利用大量的辅助数据（如社交网络、知识图）来更好地解释用户意图，然而这些附加的数据也可能是不完整和有噪声的。关键的困难来源于静态模型的固有的机制，其交互模式限制了用户的表达（根本没有交互），导致了用户与机器之间存在信息的不对等。\n CRSs介绍\n文中将会话推荐系统（CRSs）定义为： 一种能通过实时的多轮对话，探出用户的动态偏好，并根据用户当前的需求采取行动的推荐系统。\n从定义可以看到，其中一个属性是多轮交互，就是用户与系统之间以任意形式的交互，包括书面、自然语言甚至是手势等等方式。用过交互，推荐系统可以更加容易的推断出用户当前的偏好以及理解消费动机。如图一，使用户求助代理商音乐推荐。\n\n如图所示，代理商会先根据用户先前的偏好为其推荐，如果不满足用户需求就会根据用户的反馈进行更改。这就与先前的静态推荐系统不同，静态系统没有人机交互过程，而CRSs有交互过程。\n CRSs与交互推荐的联系\n在交互推荐的设置中，每一个推荐之后都会有一个反馈信号，表示用户是否或者有多么喜欢这个推荐。但是，交互式推荐的效率较低，由于物品种类太多，对于理解用户的意图是不明确的。因此，可以利用物品的属性信息，来缩小候选项范围。其中一种方案是基于评价的推荐系统，它旨在探究用户对某些属性的反馈而不是物品层面。举个例子，当用户购买手机时，可根据用户提供的“价格更便宜”或是“电池更耐用”等属性反馈，推荐更合适的机型。这种机制能够有助于快速缩小候选项范围。\n虽然上述 方式有效，但仍存在局限性。比如应该避免收到用户反馈就给出推荐，应当是系统觉得计算出的结果可信度高了再给出推荐。同时使用当前交互系统，用户只能通过预设的选项与系统交互，所以需要考虑设计更灵活的形式。\n CRSs与其他会话式人工智能系统的联系\n除了CRSs，当前市面上还有很多其他的会话式的AI系统，最常见的比如面向任务的会话系统、聊天机器人、问答系统等等。那么这些与CRSs有什么区别呢？\n\n\n中心任务的不同\n虽然都是基于会话，但其他系统的会话方式只基于自然语言文本。而CRSs可以基于任何形式，如标签，按钮甚至手势等。\n\n\n使用场景的不同\n在某些使用场景中会存在不同，如常见的问答系统只针对与用户的单个回合的问题进行回答，而CRSs是多回合推荐。\n\n\n 本次调查的焦点\n文中将CRSs以一种通用框架呈现，如下图。\n\n可以看到系统由三个组件组成：\n\n\n用户界面\n充当用户与机器之间的转换器，即从用户的原始表达中提取信息并转换为机器可理解的表示形式。\n\n\n会话策略模型\n相当于CRSs的大脑，首先是决定CRS的核心逻辑，如激发用户偏好，保持多回合对话，引导新主题等。同时也要协调其他两个组件。\n\n\n推荐引擎\n负责建模实体间的关系，学习和记录用户对项目和项目属性的偏好，检索所需信息。\n\n\n同时基于三个组件，文中也总结了五个主要挑战：\n\n\n基于问题的用户偏好引导\n该挑战主要有两个问题，如何询问？如何根据用户回应调整推荐？\n\n\n多轮会话式推荐策略\n在与用户交互过程中，可以让用户通过选择是继续提出要求还是直接获得推荐结果。虽然得到的问题越多结果越确定，但是系统应尽可能减少交互的回合数。\n\n\n自然语言的理解与生成\nCRSs应能够准确捕获用户的语义信息，同时也要生成具有可读性、流畅性的自然语言给用户。\n\n\nExploration and Exploitation (E&amp;E)之间的权衡\n对于用户来说，只会与数据集中的小部分项目存在交互，甚至是对于新用户毫无交互。因此CRSs需要对用户进行积极探索。\n\n\n评估与用户模拟\n由于评价CRSs需要大量的在线用户进行交互，这代价太高，因此需要开发用户模拟器。\n\n\n 基于问题的用户偏好引导\n即使用户知道自己想要什么，也是需要进行精准的关键词检索，并且检索结果也是一堆候选项。但是推荐系统推荐的是用户可能喜欢的物品，这与用户所熟悉的物品是不同的。传统的推荐系统只能利用历史记录作为输入，从而受限制。但CRSs系统可以通过实时交互，通过用户反馈的某些属性判断此时用户的需求与态度，并作出及时调整。\n问题驱动的方法关注于在对话中应该问什么的问题。一般有两种方法：（1）询问物品（2）询问物品的属性/主题/类别。\n 询问物品\n文中介绍三种方法，可以引导出用户对物品的态度，从而快速调整。\n 基于选择的方法\n其主要思想就是经常让用户从给定的选项中选择喜欢的项目或项目集，常见策略包括：\n\n从两个给定选项中选择一个项目\n从给定项目的列表中选择一个项目\n从两个给定的列表中先择一组项目\n\n在用户选择项目后，系统会根据反馈更改推荐。在选择过程中也应尽可能确保两个候选集是不同的或是可区分的。\n例如： Loepp等人 使用矩阵分解（MF）来初始化用户和商品的嵌入，然后从商品嵌入空间中选择两组商品作为候选集，然后让用户选择这两组商品之一 。 重要的是要确保两个候选集尽可能不同或可区分。作者采用了MF算法，并按解释的方差递减的顺序逐一获得了嵌入向量。因此，这些因素，即嵌入向量的不同维数，是按独特性排序的。然后，作者迭代选择只有一个因子值变化的两个项目集。例如，如果两个因素分别代表电影的幽默度和动作度，则两个候选集是具有较高幽默度的电影集和具有较低幽默度的另一组电影，而具有较高幽默度的电影集两套固定在平均水平。当用户选择一个商品集时，用户的偏好嵌入向量将设置为所选商品的嵌入向量的平均值。随着交互过程的继续，选择变得更加困难。用户可以选择忽略该问题，这意味着用户无法区分两个物品集之间的区别，或者他们不在乎。\n 贝叶斯偏好引导方法\n可定义函数u(xj,ui)u(x_j,u_i)u(xj​,ui​)代表用户 i 对商品 j 的偏好。在通常情况下，u(xj,ui)=xjTuiu(x_j,u_i) = x_j^Tu_iu(xj​,ui​)=xjT​ui​。\n在贝叶斯条件下，用户 i 的偏好将有之前的向量形式替换为概率分布形式，可先计算用户先验概率P(U(i))P(U ^ { (i)} )P(U(i))，然后单个物品 j 对于用户 i 的期望可计算为：\n\n那么其中的最大期望就可作为推荐项，即 argmaxjE[u(xj,ui)]argmax_j\\mathbb {E} [u(x_j,u_i)]argmaxj​E[u(xj​,ui​)] 。\n系统可选择一些要询问的物品，同时对于用户的状态认知分布（belief distribution）可通过用户的反馈进行更新。若给定一个用户对于问题 qqq 的回复 rir_iri​ ,用户的后验概率 P(ui∣q,ri)P(u_i|q,r_i)P(ui​∣q,ri​) 可写为：\n\n同时，询问策略也有两类：（1）两两比较，用户在两个项目（集）中选择更喜欢的，（2）用户从多个选项中选择。\n 交互式推荐\n交互式推荐模型主要是基于强化学习，首先是可以使用MAB算法，其优势是：\n\n可以自然的支持会话场景\n可以利用用户以前喜欢的物品，并探索用户可能喜欢但从未尝试过的物品。\n\n还有人提出元学习的方法，这些都会在后续具体展开来讲。\n由于候选项实在太大，如果只询问物品本身效率会很低，同时用户也会随着交互次数过多而感到无聊，所以以属性为中心的问题更实际，因此利用用户对属性的偏好成了一个关键的研究问题。\n 询问属性\n可以通过属性而更有效的排除候选项，其挑战在于如何在不确定用户需求的情况下，系统该询问哪一系列的属性。下面是一些主流方法。\n 历史交互拟合模式\n一个会话可以看作一个实体的序列，包括已经消费的物品和所提及的属性。其目标是为了学习预测下一个要询问的属性或是下一个要推荐的物品。因此为捕获用户行为模式的长期和短期的依赖，可以使用时序神经网络（RNN），如GRU、LSTM。\n其中一个范例就是Christakopoulou等人提出的Q&amp;R模型，它首先让用户在给定的列表中选择一个或多个主题。这个模型包含一个触发机制，这个机制可以是随机的，也可以是制定一些标准获取用户状态，甚至可以是用户自己发起。当用户在第 ttt 次选择时，那么用户是否选择下一个主题 qqq 的概率就基于他之前的观看历史 e1,…,eTe_1,\\dots,e_Te1​,…,eT​ ，即 P(q ∣ e1,…,eT)P(q\\ |\\ e_1,\\dots,e_T)P(q ∣ e1​,…,eT​) 。当选择主题 qqq 后，系统对推荐物品 rrr 的条件概率就是 P(r ∣ e1,…,eT,q)P(r\\ |\\ e_1,\\dots,e_T,q)P(r ∣ e1​,…,eT​,q) ，这种算法可以用在新用户上。Zhang等人还提出通过将用户的评论转为潜在向量，但这种方式时有局限性的，因为有的评论是负面的。\n系统生成的问题是使用预定义的语言模板，因此系统只是关注用户选择的方向与价值，毕竟其核心任务是推荐不是语言生成。\n上述方法有一个共同缺点，就是从用户的历史行为中学习是不能理解交互背后的逻辑的，这些模型只是在试图适合历史偏好，没有考虑过一旦用户拒绝了该怎么办。\n 减少不确定性\n一些研究试图简历一个简单的逻辑来缩小候选项的范围。\n 基于评论的方法\n上述的评论模型具备一种启发式策略，可以引导出用户对属性的偏好，那么基于这一点可以利用用户所评论的属性来移除不满足的属性，进而重构候选集。可以将用户的评论转为潜在向量Z⃗i,j\\vec {Z}_{i,j}Zi,j​，那么这个向量可用于生成评分r⃗i,j\\vec{r}_{i,j}ri,j​，以及解释的属性S⃗i,j\\vec{S}_{i,j}Si,j​，一旦用户不喜欢那个属性，就直接把属性向量中的属性归零，然后更新所有向量。\n 强化学习驱动方法\n强化学习可用于CRSs来选择适当的属性来询问，系统不仅可以选择属性，还可以选择何时改变当前对话的主题。\n 受图约束的候选项\n图是一种表示不同实体之间关系的普遍结构。利用图来筛选给定的一组属性的项是很自然的。例如，Lei等人在一个异构图上提出了一种交互式路径推理算法，它将用户、项目和属性表示为节点，一个边连接的两个节点表示两个节点之间的关系，例如，用户购买了一个项目，或一个项目对一个属性有一定的值。图的优势就是可以将一段对话转换成一张图上的路径，如下图二：\n\n 其他方法\nZou等人提出了一种基于扩展矩阵分解模型的问题驱动推荐系统，它只考虑用户评级数据，以结合用户的实时反馈。\n基本的假设是，如果用户喜欢一个物品，那么他一定喜欢这个物品的全部属性。基于这一点，如果只需要询问用户是否喜欢系统最不确定的属性就好，因为喜欢物品所共有的一些属性是一定喜欢的。\n 总结\n上述方法大多存在一些缺陷，系统会不断的提出问题，每个问题都会给出一个推荐，只有当用户满意了或者自己退出了才会终止，这就好像在审问用户一样。同时，在询问过程中，如果系统还没有理清用户的偏好时，也不应该将认可度低的推荐给用户。\n总之，需要一个多轮会话策略来控制如何在询问和推荐之间切换，这种策略应该在交互过程中动态变化。\n CRSs的多轮会话策略\n问题驱动的方法关注于“该问什么”,这一节则侧重于“何时问”或是“如何保持会话”，一个好的策略不仅可以在适当的时间（高度自信）提出推荐，灵活地适应用户的反馈，还可以保持对话话题，适应不同的场景，让用户在互动中感到舒适。\n 决定何时询问和推荐的会话策略\n好的策略可以是一个基于规则的策略，比如说可以是每提出 k 个问题就给出一次推荐。也可以是随机策略或者是基于模型的策略。\n在SAUR模型中，可以设置一个触发器来激活推荐模块，当系统认为可信度较高了就自动触发推荐，否则将继续询问。但是这种方式过于简单，它不能捕获丰富的语义信息，例如现在讨论的是什么话题，这个话题被讨论到什么程度了。因此，引入了强化学习（RL）。例如，Sun和Zhang[187]提出了一个被称为会话推荐模型(CRM)的模型，该模型使用了面向任务的对话系统的架构。在CRM模型中，用一个置信追踪器去追踪用户的输入，然后输出一个潜在向量，表示对话的当前状态和迄今为止已捕获的用户首选项。然后将状态向量输入到一个策略网络中，用以决定是推荐一个项目还是继续提问。比如说有 k+1 步动作，那么前 k 步是对某一方面的询问，第 k+1步就是给出推荐。\n在CRM问题中的潜在向量是捕获表面信息的，也不太适应动态环境。因此 Lei 等人提出 EAR 框架，让机器在正确的时间提出问题，在他们的定义中，正确的时间是（1）候选集足够小，（2）从信息的获取或是用户的耐心上来讲，所提出的额外问题意义不大了，（3）推荐引擎认为当前可信度会被用户接受。EAR 工作流程如下（ 图三）：\n\n系统必须根据得到的信息决定是否继续询问关于属性的问题，还是给出推荐。为了确定何时提出问题，他们构建了RL模型的状态，并考虑四个因素：\n\n当前候选项属性中每个属性的熵信息。询问熵大的属性有助于减少候选空间，从而有利于以更少的回合找到想要的项目。\n用户对每个属性的偏好。预测偏好高的属性很可能得到正反馈，这也有助于减少候选空间。\n历史用户反馈。如果系统已经询问了用户批准的一些属性，那么这可能是推荐的好时机。\n剩余候选商品的数量。 如果候选商品名单足够短，系统应该转向推荐，避免浪费更多的轮次。\n\n同时，不要把用户未选择的作为负样本，因为用户可能只是忽略了或是只是想尝试些新东西。\n此外，Lei 等人还通过整合用户、物品、属性的知识图谱，提出 CPR 模型来扩展 EAR 模型。原 EAR 模型中属性的选择都是不规则与不可预测的，CPR 在选择属性与推荐的过程中，都是严格遵循知识图谱上的路径，因此可以解释，同时知识的整合提高了推理能力，模型效率更高。\n 更广泛角度的会话策略\n大多数的CRS模型都是假设用户知道他们想要什么，而实际上，求助推荐的用户很可能并不知道他们想要什么，因此CRS不仅应该提出问题询问客户，也要承担起引导主题的责任。\n 会话中的多主题学习\nLiu 等人借鉴主动会话的思想，提出一个新的任务，它将会话推荐置于多主题对话的环境中，在这个模型中，系统可以主动的、自然的引导对话，并且可以在多个主题间切换。为了解决这一任务，他们提出了一个多目标驱动的会话生成（MGCG）框架。该框架由一个目标规划模块和一个目标导向的响应模块组成。目标规划模块可以进行对话管理来控制对话流程，对话流程以推荐为主要目标，完成自然主题转换为短期目标。\n同时文中也指出了一些人工构建的数据集。\n 特殊能力:建议、谈判和说服\n除了智能交互之外，还需要 CRS 有着在不同场景下的反应能力。比如 Rosset 等人建议可以推荐出用户下一步可能想问的问题。比如说如果用户查询“日产GTR价格”，那么系统就可以提供一些推荐例如“租赁日产GT-R需要多少钱？”、“日产GT-R的优点和缺点是什么？”亦或者是一些有趣的推荐例如“日产GT-R是最终的有轨电车吗？”等等，这样不仅丰富用户体验，还会获取更多用户偏好。\n 总结\n会话策略的主要焦点是通过提问来确定何时引导用户的偏好，以及何时给出推荐。作为推荐只有在系统认为可信度高的时候才应该给出，同时自适应策略比静态策略更有前途。除了这个核心功能外，文中还从更广泛的角度介绍了一些策略。\n CRSs中的对话理解与生成\nCRSs的一个重要方向是用自然语言与用户交谈，因此理解用户的意图和产生用户可理解的反应是至关重要的。当前大多数的CRSs只是通过从处理的数据结构中提取关键信息，并基于规则的模板响应显示结果。这需要耗费大量人力来构建规则与模板，用户体验也不好。因此可以通过引入自然语言处理（NLP）技术，帮助CRSs理解用户的意图和情绪。\n 对话理解\n理解用户的意图是一个CRS用户界面的关键需求，因为推荐很依赖这些信息。然而大多CRSs只关注核心推荐逻辑与多轮策略，避开了从用户原始语句和需要预处理的输入中提取用户意图。因此，有必要开发一种以显式或隐式的方式从用户的原始语言输入中提取语义信息的方法，文中介绍了对话系统如何使用NLP技术来解决这个问题。\n 槽填充\n在对话系统中，先预定义一些人们感兴趣的方面，然后从用户的输入中提取信息填写这些方面的值。Sun 和 Zhang 等人提出一个信念追踪器从用户的语句中捕获面值对（facet-value pairs），比如说（颜色，红色）。具体来说，就是在每一个时间点 t 都给定一个向量 ZtZ_tZt​ ，当用户输入一个语句 ete_tet​，即可提取出语义信息，即把ZtZ_tZt​对应位置赋值为 1 ，其余位置仍然为 0 。这样把所有时间段的向量传入LSTM模型中即可。\n在某些场景下，显式地将语义信息建模为面值对可能是一个限制，因为它很难，也没有必要这样做。同时，也不能精确的表达用户意图或感情信息等。\n 意图和情感学习\n神经网络以自动提取特征而闻名，因此可以用来提取CRSs中用户的意图和情绪。CRSs中的一个经典例子是Li等人提出的端到端（end-to-end）框架。它将用户的原始话语作为输入，并在交互中直接产生响应。他们建立了REDIAL数据集，找电影的人需要解释他们喜欢什么样的电影，然后向系统询问电影建议。系统则尝试了解用户的电影品味然后再推荐。所有的信息交互与推荐都使用自然语言，同时所有提及到的电影都要用“@”符号标记，表示这是一个实体，这样RESIAL数据中的对话就包含了所需要的语义信息。同时由于是监督学习，那么也要问三个问题以为数据打上标签：\n\n电影是否被用户提到，还是这是系统的推荐。\n用户是否看过这个电影（看过、没看过、没说）\n用户是否喜欢这部电影或是推荐（喜欢、不喜欢、没说）\n\n通过这种方式，虽然面值约束被删除，但包括上述项和属性、用户态度和用户兴趣都被保留并标记在原始话语中。基于深度神经网络的模型由四个部分组成：\n\n是一个层次循环编码器，用双向GRU实现，用以将原始话语转换为保留关键语义信息的潜在向量。\n在每次检测到一个电影实体时（使用“@“标记），将实例化一个RNN模型来分类搜索者关于该实体的情绪或意见。\n一个基于自动编码器的推荐模块，它将情绪预测作为输入，并产生一个推荐物品。\n生成响应的交换解码器，决定是否将推荐项推荐给用户，同时生成一个句子。\n\n但是深度神经网络经常被评论是不透明和难以解释的，所以现在还是不清楚深度语言模型如何了解用户的需求。因此有人提出使用BERT模型。\n 响应生成\n一个基于自然语言的CRS响应至少应该满足两个级别的标准。较低级别的标准要求生成的语言是正确的；较高级别的标准要求响应的推荐结果是有意义且有用的。\n 生成恰当的自然语言话语\n许多CRSs通过模板方法生成响应，然而模板方法会产生重复且不灵活的输出，此外模板响应会损害用户体验，因此在CRSs中自动生成适当和流畅的响应式很重要的。在文中介绍了两种方法。\n 基于检索的方法\n其基本思想是从大量的候选项中检索适当的响应，这个问题可以理解为是用户输入的查询与候选项之间的匹配问题。最直接的方式就是度量表示查询和响应的特征向量的内积。\n有两种策略，一种是用神经网络分别从用户的查询和候选响应中学习表示向量，然后定义匹配函数进行组合，输出匹配概率。另一种是先组合用户查询和候选响应的表示向量，然后在对组合后的表示对进行学习。第一种适合在线服务，第二种的匹配信息被充分的保存与挖掘。\n 基于生成的方法\n与基于检索的方法从模板响应数据库中选择现有的响应不同，基于生成的方法直接从模型中生成一个句子。基本的生成模型是一个循环的序列到序列的模型，以查询的每个单词作为输入，然后依次输出生成的单词。\n基于生成的方法存在一些挑战，首先是不能保证一定能够生成一个良好的自然语言句子，同时由于机器没有基本的常识与情感，让用户依然能够区分出人与机器。甚至有的机器为了说一个安全的答案，还会说”OK“、”我不知道你在说什么“，这很伤用户。因此，Ke等人提出要明确的控制生成的句子的功能，比如说机器可以用询问音来表示询问，命令音表示指示等等。其次是如何评估生成的响应，因为没有标准答案，这个在后文会讲。\n同时，一个句子的正确并不代表它完成了推荐任务，推荐的实体一定要在句子中被提及，同时也也要引入一些外部知识，使得推荐结果是有意义的。\n 合并面向推荐的信息\n利用端到端框架作为用户界面有一个主要的限制，那就是只有在训练语料库中提到的物品才能被推荐，这使其性能受人工推荐的训练数据影响。因此Chen等人提出引入知识图谱，（1）对话界面可以通过连接知识图谱中的相关实体来帮助推荐引擎，推荐模型基于R-GCN模型从知识图中提取信息，（2）推荐系统也可以帮助对话界面，通过采集高概率单词，对话可以将电影与一些有偏向的词汇连接起来，从而产生一致的、可解释的响应。\nZhou等人引入一种面向物品的知识图谱，但是系统存在不理解原始话语中的一些单词的情况，因此他们合并了两个知识图谱，一个是面向词的，一个是面向物的。同时还提出了一种基于自我注意的推荐模型，可以调整相应实体在图谱上的表示。\n 本章摘要\n一般来说，交互式会话、评论方式以及聚焦于多轮会话策略的CRSs更倾向于使用带有预处理的输入和基于规则或是基于模板的输出。对话系统与关注对话能力的CRSs系统才更有可能使用原始的自然语言。在未来，CRSs中的用户理解与响应仍然是一个关键的研究领域，只要它还是作为CRSs的用户界面并直接影响用户体验。\n Exploration-Exploitation Trade-offs\nCRSs面临的一个挑战是处理历史交互很少的冷启动用户。解决这一问题的办法就是用E&amp;E权衡。通过开发，系统可以利用已知的最佳选项。通过探究，虽然系统会承担一些收集未知选项信息的风险。但为了实现长期的优化，人们可以做出短期的牺牲。在E&amp;E的早期阶段，探究试验可能会失败，但它警告该模型在未来不要经常采取这种行动。\n在CRSs中引入了MAB算法来改进推荐值。\n 多臂老虎机（MAB）推荐\n MAB介绍\nMAB是一个经典问题，在一排老虎机面前，如果想要最大化预期收益，就要选择是继续玩当前的老虎机还是换下一台，每一台老虎机都是一个黑盒，同时每一台的属性（获胜概率）都只能通过之前的实验反馈来估计。\n同时，这问题可以理解为最小regret function，使最优预期积累与估计预期积累之间差异最小。\n\n其中 a∗a^*a∗ 是理论上在任何时候都能获得最大预期奖励的最佳手臂。\n常见MAB策略就是贪心，只利用当前估计最高的手臂。还有随机策略，只探究的策略，还有同时包括贪心与随机的策略。还有一些其他的如UCB与TS。\n 基于MAB的推荐方法\nMAB算法可以嵌入到在线推荐或者交互式推荐系统当中，在推荐最佳物品的过程中，每个手臂都对应一个物品。传统的老虎机思想只将物品视为独立的手臂，而忽视了它的属性，根据累积的奖励直接估计每个物品的概率，由于有大量物品，所以是低效的。\n在推荐中，用户和物品都有着一组丰富的特征，所以用户utu_tut​是否会选择物品ata_tat​可以通过它俩的特性进行预测。Li等人提出一个名为LinUCB的线性上下文的老虎机模型。这也是第一个在推荐系统中联系上下文（即用户/项目特性）的老虎机模型。\n也有研究指出，对于推荐的探究是重要的。比如推荐应该是多样化的，而不是收到类似物品的限制，例如Ding等人认为用户可能对物品的多样性有不同偏好，比如有特定兴趣爱好的用户可能更喜欢相关的物品集。而没有特定爱好的用户可能更喜欢不同的物品集来探究自己的兴趣爱好，因此他们提出了一种既考虑相关物品集又考虑多样物品集的框架，这是一种权衡准确性与多样性的方法。\n此外Yu等人认为，可以在视觉对话增强交互推荐系统中级联老虎机。其想法是，用户从第一个推荐一直看到最后一个，那么第一个选择的就是对他有吸引力的，那么之前看过的肯定是没有吸引力，因此可以作为可靠的负样本。\nMAB的主要优势就是在于它能够进行在线学习，能够使模型更好了解冷启动用户的偏好，并且可以在多次试验后快速调整策略的能力，从而能够追求全局最优。\n CRSs中的MAB\nCRSs与用户交互的能力使其能够直接使用基于MAB的方式帮助推荐。Chri等人基于MAB提出了一种经典的CRS。它使用几种简单的MAB方法来增强离线概率矩阵分解（PMF）模型。首先使用离线数据初始化模型参数，然后利用用户的实时反馈，通过常见的MAB模型来更新参数。一方面，由于在线更新，初始化模型的性能有所提高，另一方面，离线初始化有助于降低计算复杂度。\n原始的MAB方法忽略了项目的属性，Zhang等人提出一种ConUCB算法将LinUCB应用于CRS上下文。ConUCB不是询问项目，而是询问用户关于一个或多个属性。具体来说，他们假设用户对属性的偏好可以传播到物品，因此系统可以分析用户对查询属性的反馈，从而快速缩小候选项目的范围。此外，作者还使用了一个人工函数来确定询问属性或提出建议的时间，比如每 m 轮进行 k 次对话。\n但是上述策略是不灵活的，因为系统应该在认为可信度高的时候才给出推荐。因此，Li等人提出ConTS方法，来自动交替询问关于属性的问题，他们聚合同一个手臂中的所有物品与属性。因此从手臂聚合中选择的手臂既可以是推荐，也可以是属性问题。流程如下图：\n\n CRSs中的元学习\n除了MAB，还有一些工作在尝试保持E&amp;E的平衡。邹等人。[271]将交互式推荐表述为一个元学习问题，其目标是学习一个学习算法，它将用户的历史交互作为输入，并输出一个可以应用于新用户的模型（策略函数）。此外，Lee等人通过一个基于MAML模型的模型来解决推荐中的冷启动问题，学习到的推荐模型可以在微调阶段通过向冷用户询问一些关于某些物品的问题来快速适应冷用户偏好。 这项工作的一个缺点一个候选的证据只能被选择一次，并且询问过程只在冷用户刚开始使用时进行。 最好将此策略扩展到 CRS 设置并开发动态多轮询问策略以进一步增强推荐。\n 评估和用户模拟\n本节作者讨论如何评估CRSs，并尝试分为两类。\n\n轮级评估，它评估系统输出的单个回合，包括推荐任务和响应生成任务，这两者都是监督预测任务。\n对话级评价，评估多回合对话策略的性能，这是一个顺序决策任务。\n\n为了实现这一目标，用户模拟很重要的。\n 数据集和工具\n下表中列出了常用的CRS数据集的统计数据。一些研究通过要求真正的用户在特定规则下使用自然语言进行对话来收集人与人和人机对话数据。\n\n如前所述，许多关于CRS的研究都集中在交互策略和推荐策略上，而不是语言理解和生成。因此，所有这些研究所需要的都是多回合对话中的标记实体（包括用户、项目、属性等）。\n尽管在CRSs中似乎有许多数据集，但这些数据集没有能力开发可以在工业应用中可使用的CRSs。原因有两个方面：首先，这些数据集的规模不足以覆盖现实世界的实体和概念；其次，对话要么由非对话数据构建，要么在一定的严格约束下生成，因此很难推广到复杂而多样的真实世界对话。\nZhou等人已经实现了一个叫CRSLab的工具包用于构建和评估CRSs。将现有CRSs中的任务统一为三个子任务：推荐、对话和策略，这分别对应于图二中的三个组成模块：推荐引擎、用户界面和对话策略模块。\n 轮级评估\nCRSs的精准评估是对于每单轮的输出进行处理，包含两个任务：语言生成和推荐。\n 对语言生成能力的评估\n对于生成基于自然语言的响应以与用户交互的CRS模型，生成的响应的质量是至关重要的。因此，我们可以采用用于对话响应生成的指标来评估CRSs的输出。两个示例度量标准是BLEU和Rouge。然而，这些指标是否适用于评估语言生成还存在着广泛的争论。因为这些指标只对词汇变化敏感，所以它们不能适当地评估给定参考文献的语义或句法变化。因此，其他反映用户满意度的指标更适合用于评价，如流畅性、一致性、可读性等等。\n基于端到端的对话框架或深度语言模型的CRSs在实践中的可用性方面存在些局限性，当前面临三个关键问题。\n\n对于每个系统，大约三分之一的系统话语在给定的上下文中没有意义，并且可能会导致人工评估中的对话中断。\n不到三分之二的建议被认为在人工评估中有意义。\n这两个系统都没有“产生”话语，因为几乎所有的系统响应都已经出现在训练数据中。\n\n 推荐评估\n通过将预测结果与测试集中的记录进行比较，来评估推荐模型的性能。衡量推荐系统性能的指标有两种：\n\n\n基于评级的指标\n这些指标假设用户的反馈是一个明确的评分分数，例如，一个在1到5个范围内的整数。因此，我们可以测量模型的预测分数和测试集中用户给出的地面真实分数之间的差异。传统的基于评级的度量标准包括均方误差(MSE)和均方根误差(RMSE)，其中RMSE是MSE的平方根。\n\n\n基于排名的指标\n基于排名的指标要求预测项目的相对顺序应该与测试集中的项目的顺序相一致。因此，不需要从用户那里进行显式的评分，而隐式的交互（例如，点击、播放）可以用来评估模型。\n\n\n这些评估方法中最大的问题是，现实世界的用户交互非常少，很大一部分项目永远没有机会被用户使用。然而，这并不意味着用户不喜欢它们。\n 会话级评估\n会话级评估并不是一个有监督项目。因为每个发现都是一个连续过程的一部分，而且系统所做的每一个监视都可能影响未来的观测，因此交互过程不是i.i.d.的独立同分布。此外，会话在很大程度上依赖于用户的反馈。因此，对会话的评估需要进行在线用户测试或利用历史交互数据，这些数据可以通过非策略评估或使用用户模拟来进行。\n 在线用户测试\n通过利用真正的用户反馈来直接评估对话策略。为了进行评估，应设计适当的指标。\n平均回合（AT）是一种优化CRS的全局度量方式，因为模型应该引导用户意图并做出成功的建议，从而以尽可能少的完成对话。\n一个类似的指标是推荐成功率(SR@t)，它衡量的是第 t 回合成功推荐结束的多少对话。此外，尝试失败的比例，例如系统提出的多少问题被用户拒绝或忽略，也是衡量系统是否对用户满意的决策的可行方法。\n除了这些全局统计数据外，每个回合的累积性能也可以反映对话的整体质量。\n在线用户评估虽然有效，但仍存在严重问题：（1）人与CRS之间的在线互动速度较慢，通常需要数周的时间才能收集到足够的数据以使评估具有统计学意义。 （2）收集用户的反馈是昂贵的，并且可能会损害用户的体验，自然的解决方案是在模型开发阶段和评估阶段都拥有模拟用户。\n Off-Policy评估\n其目的是为了回答一个反事实的问题，即如果我们使用πθ\\pi_\\thetaπθ​代替πβ\\pi_\\betaπβ​，也就是说当我们像评估πθ\\pi_\\thetaπθ​的目标策略但只有πβ\\pi_\\betaπβ​的数据集，那仍然可以通过引入重要性抽样或者逆倾向评分来评估目标策略。类似于下面公式：\n\n另一种直观的方法是直接模拟用户行为。\n 用户模拟\n模拟用户的策略一般有四种类型。\n 直接使用用户的交互历史记录\n其基本思想类似于传统推荐系统的评估，其中保留了用户交互数据的一个子集作为测试集。如果CRS推荐的项目在用户的测试集中，那么这个建议将被认为是成功的。\n首先，Lei等人从oracle集中随机选择一个属性作为用户对会话的初始化。会话进入一个“模型行为-模拟器响应”过程的循环，在这个过程中，如果查询实体包含在Oracle集中，模拟用户将响应“是”，否则为“否”。因为其简单，大多数CRS研究采用这种模拟方法，然而，推荐系统中的稀疏性问题仍然存在：用户项矩阵中只有少数值是已知的，而大多数元素缺失，这阻碍了对这些项进行模拟。\n 估计用户在所有项目上的偏好\n使用直接的用户交互来模拟对话也具有与我们上面提到的相同的缺点，即大量没有被用户看到的项被视为不喜欢的项。为了克服评估过程中的这种偏差，一些研究建议提前获取用户对所有项目的偏好。给定一个项目及其辅助信息，模拟用户交互的关键是估计或综合对该项目的偏好。\n 从用户评论中提取信息\n除了用户行为历史外，许多电子商务平台都有文本审查数据。与消费历史不同，一个项目的审查数据通常明确地提到属性，这可以反映用户对该项目的个性化意见。\nZhang等人将Amazon数据集的每一部分的文本审查转换为问答序列，以模拟对话。例如，当一个用户在评论手机X时提到使用安卓系统的蓝色华为手机时，那么根据本文构建的对话序列是(类别：手机→系统：安卓→颜色：蓝色→推荐：X)。Zhang等然通过利用用户评论来构建模拟的交互。基于给定的用户档案及其历史观看记录，他们构建了一个主题线程，其中包括从这些已观看电影的评论中提取的主题（例如，“家庭”或“找工作”）。主题线程由一个规则组织起来，并最终导致目标影片。通过检索相应主题下最相关的评论，充实了合成的对话。\n一个值得注意的问题是，评论中提到的这些方面可能包含了产品的一些缺点，这并不有助于理解用户为什么选择了一个产品。因此，有必要对评审数据进行情绪分析，在选择项目时，只考虑具有积极情绪的属性。\n 模仿人类的会话语料库\n为了生成没有偏见的会话数据，一个可行的解决方案是使用真实世界的两方人的对话作为训练数据。通过使用这种类型的数据，CRS系统可以通过从大量真实的人与人之间的对话中学习来直接模拟人类的行为。\nLiu等人不仅通过记录人工交谈数据，还收集并构建一个知识图，并为每个寻求推荐的用户定义一个明确的配置文件。对话话题可以包含许多非推荐场景，如问答或社交聊天，这些在现实生活中更为常见。\n这种方法也有一些缺点。首先，在收集人类会话语料库时，两个工作者需要同时进入任务，这是一个严格的设置，因此限制了数据集的规模。其次，设计师通常有许多限制对话方向的要求。因此，生成的对话受到限制，不能完全覆盖真实世界的场景。\n最近，Zhang和Balog研究了使用用户模拟来评估CRSs。它们将模拟用户的动作序列组织为一个类似于堆栈的结构，称为用户议程。议程的动态更新被视为一系列的拉或推操作，其中对话行动被从顶部删除或添加到顶部。如下图所示：\n\n 未来的发展方向与机遇\n在描述了该领域在未来的关键进展和挑战之后，作者设想了一些有前途的未来方向。\n 共同优化三项任务\nCRSs中的推荐任务、语言理解和生成任务以及对话策略通常分别在图二的三个组件中进行研究。这三个组件彼此共享某些目标和数据。例如，用户界面将提取的方面值对提供给推荐引擎，然后将推荐引擎产生的实体集成到生成的响应中。然而，他们拥有的对彼此不利的独家数据。例如，用户界面可以在评论中使用丰富的语义信息，但不能与推荐引擎共享。此外，这两个组件可能在端到端框架中工作，在多轮对话中缺乏明确的对话策略，因此在人工评价中不满足性能。\n因此，这三个任务应该共同学习和以明确的对话策略来共同指导，例如，如果对话策略模块能够根据项目-项目关系(如互补性和可替代性来计划未来的对话行为）？\n 偏差和去偏差\n推荐系统不可避免地会遇到各种类型的偏差。通过引入用户与系统之间的交互作用，可以消除某些类型的偏差，如人气偏差和一致性偏差。比如说如果静态推荐系统不知道一个用户是不是随大流或者喜欢流行的，那就可以引入流行性偏差，因为流行性物品被推荐的可能性高。然而，这在CRSs中是可以避免的，因为CRS可以实时询问用户对流行物品的态度，并在用户给出负面反馈时避免推荐它们。\n仍然也会存在一些偏差。比如推荐系统推荐大量物品，而用户只能与少部分进行交互，这就是暴露偏差。或者是即使有些项目提供给了用户，可用户因为忽略或是不喜欢，依然只选择消费他们喜欢的，这就是选择偏差（积极偏差）。\n对CRS中的偏差问题的研究相对较少。利用 E&amp;E 方法可以缓解CRSs中某些类型的偏差。\n 复杂的多轮会话策略\n目前CRSs研究中考虑的多回合策略相对幼稚。比如仍在使用人工制作的函数来确定询问属性的或者给出推荐的时间。这些基于端到端对话系统或深度神经语言模型的研究更糟糕：它们甚至没有明确的策略来控制多回合对话。此外，一些策略在处理用户的负面反馈方面可能会出现问题。因为他们会把拒绝当作负样本。\n我们已经见证了一些研究通过确定模型行动，如是否询问或推荐，从而使用RL作为多轮会话策略。然而，在设计RL的状态、行动和回报方面，仍有很大的改进空间。\n 知识丰富\n改进CRSs的一个自然想法是引入额外的知识，在CRSs发展从早期的物品本身，再到引入物品属性，最近也考虑引入知识图谱。例如Zhou等人建议结合两个外部知识图谱（KGs），一个面向单词，提供单词之间的关系；一个面向物品，包含关于物品属性的结构事实。\n当然也可以基于视觉，Yu等人提出一个视觉对话框增强的CRS模型，这个模型将推荐物品的图片也一并放出，更好的给用户传递信息。\n 更好的评估和用户模拟\n模拟用户并不能完全取代人类。如何以最大的保真度模拟用户还有待进一步的研究。可行的方向包括设计系统的模拟议程，为可靠的模拟构建密集的用户交互，以及在板岩推荐上建模用户选择行为。\n此外，CRSs在不同的数据集上工作，它们有不同的假设和设置。因此，制定全面的评价指标和程序来评估crs的性能仍然是一个开放性的问题。\n","categories":["论文解读"],"tags":["论文","CRS"]},{"title":"关于","url":"/about/index.html","content":"Hi！你好！我是来自天津大学的博士研究生小泽，首先十分欢迎你来到我的博客。\n","categories":[],"tags":[]}]